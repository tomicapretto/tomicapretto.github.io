[
  {
    "objectID": "posts/2021-08-17_gsoc-2021-final-evaluation/index.html",
    "href": "posts/2021-08-17_gsoc-2021-final-evaluation/index.html",
    "title": "GSOC 2021: Final evaluation",
    "section": "",
    "text": "GSoC has been great. I’ve learnt so much during the past weeks. And I’m obviously eager to keep learning and doing stuff with Bambi in the future. The following is a summary of what we were able to achieve during these time of code:\n\nImplemented new default priors #360, #385.\nAdded new Student-T family #367.\nAdded new Beta family #368.\nImplemented predictions #372.\nImproved internal model specification by splitting it into smaller and composable classes #366.\nAdded the new Binomial family #386. This also implied some changes in its sibling project, formulae.\n\nThis, with many other smaller changes or improvements that you can find here were included in Bambi 0.6.0.\nOn the other hand, the items on my original proposal that left to do are multinomial regression and ordered categorical terms. I’ve started to do some work on the formulae side, but these features require a more involved work in Bambi, and thus it is left for future contributions.\nTo conclude, I want to thank Google for having such an amazing program and everyone who contributed or helped me to contribute to Bambi. Specially, I want to recognize the the work of my mentors Ravin Kumar and Thomas Wiecki, and my director Osvaldo Martin for all the support, feedback, and work during this program."
  },
  {
    "objectID": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html",
    "href": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html",
    "title": "Robust linear regression in Bambi",
    "section": "",
    "text": "The next thing in my TODO list for this Google Summer of Code season with NumFOCUS is to add new families of models to Bambi. This is still a WIP but I wanted to show you how to build a robust linear regression model using the Family class in Bambi."
  },
  {
    "objectID": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#what-do-we-mean-with-robust",
    "href": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#what-do-we-mean-with-robust",
    "title": "Robust linear regression in Bambi",
    "section": "What do we mean with robust?",
    "text": "What do we mean with robust?\nBefore showing how to build a robust regression with Bambi we need to be clear about what we mean when we say that a model is robust. Robust to what? How is linear regression non-robust?\nIn this post, we say a method is robust if its inferences aren’t (seriously) affected by the presence of outliers."
  },
  {
    "objectID": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#how-do-outliers-affect-linear-regression",
    "href": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#how-do-outliers-affect-linear-regression",
    "title": "Robust linear regression in Bambi",
    "section": "How do outliers affect linear regression?",
    "text": "How do outliers affect linear regression?\nI think it will be easier to understand how outliers affect linear regressions via an example based on the least squares method. This is not exactly how linear regression works in our Bayesian world, but outlier’s bad consequences are similar.\nIn classic statistics, linear regression models are usually fitted by ordinary least-squares method. This is equivalent to assuming the conditional distribution of the response given the predictors is normal (i.e. \\(y_i|\\boldsymbol{X}_i \\sim N(\\mu_i, \\sigma)\\)) and using the maximum likelihood estimator.\nLet’s get started by simulating some toy data.\n\nx = np.array([1., 2., 4., 5.])\ny = np.array([1.25, 1.45, 4.75, 4.8])\n\nThen, fit a linear regression between and visualize the result.\nThe next plot shows the data, the fitted line, and the contribution of each data point to the total (squared) error as a blue square (one way to see the least squares method is as the method that minimizes the sum of the areas of the squares associated to all the points).\n\nb, a = np.polyfit(x, y, 1)\ny_hat = a + b * x\nresidual = y_hat - y\n\n\narrowstyle = \"Simple, tail_width=0.3, head_width=4, head_length=4\"\nconnectiontyle = \"arc3, rad=0.4\"\narrowstyles = {\"color\": \"0.2\", \"arrowstyle\": arrowstyle, \"connectionstyle\": connectiontyle}\n\nfig, ax = plt.subplots(figsize=(6, 6), dpi=120)\nfig.set_facecolor(\"w\")\nax.set_xlim(0.25, 6)\nax.set_ylim(0.25, 6)\n\nax.scatter(x, y, s=50, ec=\"k\")\nax.plot(x, y_hat, color=\"0.2\", lw=2.5)\n\n# Add rectangles\nfor xy, r in zip(zip(x, y ), residual):\n    ax.add_patch(Rectangle(xy, abs(r), r, alpha=0.7, ec=\"k\", zorder=-1, lw=1))\n\n# Add arrows\nx_end = x + residual * np.array([0, 1, 0, 1])\nx_start = x_end + np.array([-0.3, 0.4, -0.4, 0.3])\ny_end = y + residual / 2\ny_start = y\ny_text = y_end + np.array([0.2, -0.45, 0.35, -0.3])\n\nfor xy0, xy1, r, yt in zip(zip(x_start, y_start), zip(x_end, y_end), residual, y_text):\n    ax.add_patch(FancyArrowPatch(xy0, xy1, **arrowstyles))\n    ax.text(xy0[0], yt, str(round(abs(r ** 2), 4)), ha=\"center\")\n\nax.text(\n    0, 1.01, f\"Sum of squares: {round(np.sum(residual ** 2), 4)}\", \n    size=12, transform=ax.transAxes, va=\"baseline\"\n);\n\n\n\n\nSo far so good! It looks like the fitted line is a good representation of the relationship between the variables.\nWhat happens if we introduce an outlier? In other words, what happens if there’s a new point that deviates too much from the pattern we’ve just seen above? Let’s see it!\n\nx = np.insert(x, 2, 2.25)\ny = np.insert(y, 2, 5.8)\n\n\nb, a = np.polyfit(x, y, 1)\ny_hat = a + b * x\nresidual = y_hat - y\n\n\nfig, ax = plt.subplots(figsize=(6, 6), dpi=120)\nfig.set_facecolor(\"w\")\nax.set_xlim(0, 6.5)\nax.set_ylim(0, 6.5)\n\nax.scatter(x, y, s=50, ec=\"k\")\nax.plot(x, y_hat, color=\"0.2\", lw=2.5)\n\n# Add rectangles\nfor xy, r in zip(zip(x, y ), residual):\n    ax.add_patch(Rectangle(xy, abs(r), r, alpha=0.7, ec=\"k\", zorder=-1, lw=1))\n\n# Add arrows\nx_end = x + np.abs(residual) * np.array([0, 1, 0, 1, 1])\nx_start = x_end + np.array([-0.4, 0.4, -0.5, 0.3, 0.3])\ny_end = y + residual / 2\ny_start = y + np.array([0.8, 0.4, -0.8, -0.4, 0])\ny_text = y_start + np.array([0.1, -0.1, 0.1, -0.1, -0.1])\n\nfor xy0, xy1, r, yt in zip(zip(x_start, y_start), zip(x_end, y_end), residual, y_text):\n    ax.add_patch(FancyArrowPatch(xy0, xy1, **arrowstyles))\n    ax.text(xy0[0], yt, str(round(abs(r ** 2), 4)), ha=\"center\", va=\"center\")\n\nax.text(\n    0, 1.01, f\"Sum of squares: {round(np.sum(residual ** 2), 4)}\", \n    size=12, transform=ax.transAxes, va=\"baseline\"\n);\n\n\n\n\nWhat a bummer! Why do we have such a huge error? It’s 10 times the previous error with only one extra data point! Why?!\nIt happens that each point’s contribution to the error grows quadratically as it moves away from the rest. Outliers not only contribute a lot to the total error, they also bias the estimation towards themselves, increasing the error associated with other points too. The final result? the fitted line is not a faithful representation of the relationship between the variables."
  },
  {
    "objectID": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#linear-regression-in-a-bayesian-way",
    "href": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#linear-regression-in-a-bayesian-way",
    "title": "Robust linear regression in Bambi",
    "section": "Linear regression in a Bayesian way",
    "text": "Linear regression in a Bayesian way\nNow that we’ve seen how bad outliers can be above, let’s see how one can robust a Bayesian linear regression. This part of the post is based on the Robust Linear Regression in PyMC3 docs.\nHere, we simulate data suitable for a normal linear regression and contaminate it with a few outliers.\n\nsize = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ntrue_regression_line = true_intercept + true_slope * x\ny = true_regression_line + np.random.normal(scale=0.5, size=size)\n\nx_out = np.append(x, [0.1, 0.15, 0.2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = pd.DataFrame(dict(x = x_out, y = y_out))\n\n\nfig, ax = plt.subplots(figsize=(10, 7), dpi=120)\nfig.set_facecolor(\"w\")\n\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7);\n\n\n\n\n\nNormal linear regression\nThe normal linear regression is as follows\n\\[\ny_i \\sim \\text{Normal}(\\mu_i, \\sigma)\n\\]\nwhere \\(\\mu_i = \\beta_0 + \\beta_1 x_i\\), and the priors are of the form\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal} \\\\\n\\beta_1 \\sim \\text{Normal}  \\\\\n\\sigma \\sim \\text{HalfStudentT}\n\\end{array}\n\\]\nwith their parameters automatically set by Bambi.\n\nmodel = bmb.Model(\"y ~ x\", data=data)\nidata = model.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_sigma, x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:01<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\n\n\nTo evaluate the fit, we use the posterior predictive regression lines. The line in black is the true regression line.\n\n# Prepare data\nx = np.linspace(0, 1, num=200)\nposterior_stacked = idata.posterior.stack(samples=(\"chain\", \"draw\"))\nintercepts = posterior_stacked[\"Intercept\"].values\nslopes = posterior_stacked[\"x\"].values\n\n# Create plot\nfig, ax = plt.subplots(figsize=(10, 7), dpi=120)\n\n# Data points\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7)\n\n# Posterior regression lines\nfor a, b in zip(intercepts, slopes):\n    ax.plot(x, a + b * x, color =\"0.5\", alpha=0.3, zorder=-1)\n\n# True regression line\nax.plot(x, true_intercept + true_slope * x, color=\"k\", lw=2);\n\n\n\n\nAs you can see, the posterior distribution fo the regression lines is not centered around the true regression line, which means the estimations are highly biased. This is the same phenomena we saw above with the least-squares toy example.\nWhy does it happen here? The reason is that the normal distribution does not have a lot of mass in the tails and consequently, an outlier will affect the fit strongly.\nSince the problem is the light tails of the Normal distribution we can instead assume that our data is not normally distributed but instead distributed according to the Student T distribution which has heavier tails as shown next.\n\n\nNormal and Student-T distributions\nHere we plot the pdf of a standard normal distribution and the pdf of a student-t distribution with 3 degrees of freedom.\n\nx = np.linspace(-8, 8, num=400)\ny_normal = stats.norm.pdf(x)\ny_t = stats.t.pdf(x, df = 3)\n\n\nfig, ax = plt.subplots(figsize=(10, 6), dpi=120)\nfig.set_facecolor(\"w\")\nax.set_ylim(0, 0.41)\n\nax.plot(x, y_normal, lw=2)\nax.plot(x, y_t, lw=2)\n\nax.add_patch(FancyArrowPatch((-3, 0.36), (x[180], y_normal[180]), **arrowstyles, zorder=1))\nax.add_patch(FancyArrowPatch((3, 0.31), (x[205], y_t[205]), **arrowstyles, zorder=1))\n\nax.text(-3, 0.37, \"Normal\", size=13, ha=\"center\", va=\"center\")\nax.text(3, 0.30, \"Student's T\", size=13, ha=\"center\", va=\"center\");\n\n\n\n\nAs you can see, the probability of values far away from the mean are much more likely under the Student-T distribution than under the Normal distribution.\n\n\nRobust linear regression\nThe difference with the model above is that this one uses a StudentT likelihood instead of a Normal one.\nBambi does not support yet to use the student-t distribution as the likelihood function for linear regression. However, we can construct our own custom family and Bambi will understand how to work with it.\nCustom families are represented by the Family class in Bambi. Let’s see what we need to create a custom family.\nFirst of all, we need a name. In this case the name is going to be just \"t\". Second, there is the likelihood function. This is represented by an object of class Likelihood in Bambi. To define a likelihood function we need the following:\n\nThe name of the distribution in PyMC3. In this case, it is \"StudentT\".\nThe name of the parent parameter (the mean). It is \"mu\".\nThe prior distributions for the auxiliary parameters in the distribution. These are nu and sigma in the StudentT distribution.\n\nFinally, we pass the link function. This can be a string or an object of class Link. In this case it’s simply the identity function, which can be passed as a string.\n\n# Construct likelihood\nnu = bmb.Prior(\"Gamma\", alpha=3, beta=1)\nsigma = bmb.Prior(\"HalfStudentT\", nu=4, sigma=1)\nlikelihood = bmb.Likelihood(name=\"StudentT\", parent=\"mu\", sigma=sigma, nu=nu)\n\n# Construct family\nt_family = bmb.Family(name = \"t\", likelihood = likelihood, link = \"identity\")\n\n# In addition, we pass our custom priors for the terms in the model.\npriors = {\n  \"Intercept\": bmb.Prior(\"Normal\", mu=2, sigma=5),\n  \"x\": bmb.Prior(\"Normal\", mu=0, sigma=10)\n}\n\n# Just add the `prior` and `family` arguments\nmodel = bmb.Model(\"y ~ x\", data, priors=priors, family=t_family)\nidata = model.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_nu, y_sigma, x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:02<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 2 seconds.\n\n\n\n# Prepare data\nx = np.linspace(0, 1, num=200)\nposterior_stacked = idata.posterior.stack(samples=(\"chain\", \"draw\"))\nintercepts = posterior_stacked[\"Intercept\"].values\nslopes = posterior_stacked[\"x\"].values\n\n# Create plot\nfig, ax = plt.subplots(figsize=(10, 7), dpi=120)\n\n# Data points\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7)\n\n# Posterior regression lines\nfor a, b in zip(intercepts, slopes):\n    ax.plot(x, a + b * x, color =\"0.5\", alpha=0.3, zorder=-1)\n\n# True regression line\nax.plot(x, true_intercept + true_slope * x, color=\"k\", lw=2);\n\n\n\n\nMuch better now! The posterior distribution of the regression lines is almost centered around the true regression line, and uncertainty has decreased, that’s great! The outliers are barely influencing our estimation because our likelihood function assumes that outliers are much more probable than under the Normal distribution."
  },
  {
    "objectID": "posts/2022-06-26_tidypolars/index.html",
    "href": "posts/2022-06-26_tidypolars/index.html",
    "title": "Let’s use tidypolars more",
    "section": "",
    "text": "In this blogpost I’m going to show how to perform the same task with pandas, the most popular library for data analysis in Python, and tidypolars, a new library to do data analysis with tabular data inspired on the tidyverse.\nThe task consists of computing a variable transformation that relies on grouped aggregations. In particular, we will be computing the standardized version of a numeric variable by group.\n\nimport palmerpenguins\n\nimport tidypolars as tp\n\nfrom tidypolars import col\n\nI’m going to work with the famous palmer penguins dataset. In Python this can be loaded very easily thanks to the palmerpenguins library.\n\ndata = palmerpenguins.load_penguins()\ndata\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      339\n      Chinstrap\n      Dream\n      55.8\n      19.8\n      207.0\n      4000.0\n      male\n      2009\n    \n    \n      340\n      Chinstrap\n      Dream\n      43.5\n      18.1\n      202.0\n      3400.0\n      female\n      2009\n    \n    \n      341\n      Chinstrap\n      Dream\n      49.6\n      18.2\n      193.0\n      3775.0\n      male\n      2009\n    \n    \n      342\n      Chinstrap\n      Dream\n      50.8\n      19.0\n      210.0\n      4100.0\n      male\n      2009\n    \n    \n      343\n      Chinstrap\n      Dream\n      50.2\n      18.7\n      198.0\n      3775.0\n      female\n      2009\n    \n  \n\n344 rows × 8 columns\n\n\n\n\n\nBefore seeing an example using tidypolars I’m going to perform some basic data wrangling with pandas. One of the first things one usually do with a data frame is exploring its first rows visually. The .head() method comes very handy.\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007\n    \n  \n\n\n\n\nI’m going to work with the species, sex, and body_mass_g columns only. To subset the dataframe we only need to pass the name of these columns as a list within brackets.\n\ndata = data[[\"species\", \"sex\", \"body_mass_g\"]].reset_index(drop=True)\ndata.head()\n\n\n\n\n\n  \n    \n      \n      species\n      sex\n      body_mass_g\n    \n  \n  \n    \n      0\n      Adelie\n      male\n      3750.0\n    \n    \n      1\n      Adelie\n      female\n      3800.0\n    \n    \n      2\n      Adelie\n      female\n      3250.0\n    \n    \n      3\n      Adelie\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      female\n      3450.0\n    \n  \n\n\n\n\nI add the .reset_index(drop=True) to avoid some SettingWithCopyWarnings later. I also want to drop any observations with missing values so I use .dropna() to do that.\n\ndata = data.dropna()\n\nSuppose now I want to standardize the variable body_mass_g. Pandas vectorized operations make it extremely easy. One can save the result in a new column in a very intuitive way as well.\n\ndata[\"body_mass_z\"] = (data[\"body_mass_g\"] - data[\"body_mass_g\"].mean()) / data[\"body_mass_g\"].std()\ndata.head()\n\n\n\n\n\n  \n    \n      \n      species\n      sex\n      body_mass_g\n      body_mass_z\n    \n  \n  \n    \n      0\n      Adelie\n      male\n      3750.0\n      -0.567621\n    \n    \n      1\n      Adelie\n      female\n      3800.0\n      -0.505525\n    \n    \n      2\n      Adelie\n      female\n      3250.0\n      -1.188572\n    \n    \n      4\n      Adelie\n      female\n      3450.0\n      -0.940192\n    \n    \n      5\n      Adelie\n      male\n      3650.0\n      -0.691811\n    \n  \n\n\n\n\nWe could consider it is more appropiate to standardize considering the species variable. We still perform the same operation than above, but we do it within each group.\nThe way to perform such operations in pandas is to use the .groupby() method. Then one can select the desired column and compute the aggregation.\nFor example, to compute the mean body mass by species we can do the following.\n\ndata.groupby(\"species\")[\"body_mass_g\"].mean()\n\nspecies\nAdelie       3706.164384\nChinstrap    3733.088235\nGentoo       5092.436975\nName: body_mass_g, dtype: float64\n\n\nWe obtained a pandas Series with three values, the mean for each species. If we want to obtain a Series of the same length than the original data, it is, for each row the mean of the species the observation belongs to, we can use .transform().\n\ndata.groupby(\"species\")[\"body_mass_g\"].transform(\"mean\")\n\n0      3706.164384\n1      3706.164384\n2      3706.164384\n4      3706.164384\n5      3706.164384\n          ...     \n339    3733.088235\n340    3733.088235\n341    3733.088235\n342    3733.088235\n343    3733.088235\nName: body_mass_g, Length: 333, dtype: float64\n\n\nThe same can be done with other transformations such as the standard deviation.\n\ndata.groupby(\"species\")[\"body_mass_g\"].transform(\"std\")\n\n0      458.620135\n1      458.620135\n2      458.620135\n4      458.620135\n5      458.620135\n          ...    \n339    384.335081\n340    384.335081\n341    384.335081\n342    384.335081\n343    384.335081\nName: body_mass_g, Length: 333, dtype: float64\n\n\nNow, putting all the pieces together, we can compute the standardized body mass by species.\n\ndata[\"body_mass_z\"] = (\n    (data[\"body_mass_g\"] - data.groupby(\"species\")[\"body_mass_g\"].transform(\"mean\")) \n    / data.groupby(\"species\")[\"body_mass_g\"].transform(\"std\")\n)\ndata.head()\n\n\n\n\n\n  \n    \n      \n      species\n      sex\n      body_mass_g\n      body_mass_z\n    \n  \n  \n    \n      0\n      Adelie\n      male\n      3750.0\n      0.095582\n    \n    \n      1\n      Adelie\n      female\n      3800.0\n      0.204604\n    \n    \n      2\n      Adelie\n      female\n      3250.0\n      -0.994645\n    \n    \n      4\n      Adelie\n      female\n      3450.0\n      -0.558555\n    \n    \n      5\n      Adelie\n      male\n      3650.0\n      -0.122464\n    \n  \n\n\n\n\nIt’s also possible to add more variables to the groups. For example, this is how we can perform the same compution considering groups given by species and sex.\n\ndata[\"body_mass_z\"] = (\n    (data[\"body_mass_g\"] - data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"mean\")) \n    / data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"std\")\n)\ndata.head()\n\n\n\n\n\n  \n    \n      \n      species\n      sex\n      body_mass_g\n      body_mass_z\n    \n  \n  \n    \n      0\n      Adelie\n      male\n      3750.0\n      -0.846261\n    \n    \n      1\n      Adelie\n      female\n      3800.0\n      1.600580\n    \n    \n      2\n      Adelie\n      female\n      3250.0\n      -0.441145\n    \n    \n      4\n      Adelie\n      female\n      3450.0\n      0.301301\n    \n    \n      5\n      Adelie\n      male\n      3650.0\n      -1.134602\n    \n  \n\n\n\n\nAnd finally, I can sort by the standardized body mass in ascending order.\n\ndata.sort_values(\"body_mass_z\")\n\n\n\n\n\n  \n    \n      \n      species\n      sex\n      body_mass_g\n      body_mass_z\n    \n  \n  \n    \n      314\n      Chinstrap\n      female\n      2700.0\n      -2.899080\n    \n    \n      192\n      Gentoo\n      female\n      3950.0\n      -2.591611\n    \n    \n      195\n      Gentoo\n      male\n      4750.0\n      -2.346530\n    \n    \n      298\n      Chinstrap\n      female\n      2900.0\n      -2.198147\n    \n    \n      119\n      Adelie\n      male\n      3325.0\n      -2.071711\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      114\n      Adelie\n      female\n      3900.0\n      1.971803\n    \n    \n      109\n      Adelie\n      male\n      4775.0\n      2.109234\n    \n    \n      284\n      Chinstrap\n      female\n      4150.0\n      2.182685\n    \n    \n      313\n      Chinstrap\n      male\n      4800.0\n      2.377631\n    \n    \n      169\n      Gentoo\n      male\n      6300.0\n      2.603039\n    \n  \n\n333 rows × 4 columns\n\n\n\n\n\n\nNow it’s time to see the same operations performed with tidypolars. Tidypolars is inspired on the tidyverse, a set of packages following a consistent design phillosophy that has revolutionated the way we do data science in R and other languages as well.\nIts description says > tidypolars is a data frame library built on top of the blazingly fast polars library that gives access to methods and functions familiar to R tidyverse users.\nTidypolars does not rely on any pandas data structure because it’s built on top of polars, not pandas. In addition, it works with a new data frame structure called Tibble, borrowing its name from the tibble in the R package of the same name.\nWe can convert a pandas DataFrame to a tibble with the .from_pandas() function.\n\ntibble = tp.from_pandas(palmerpenguins.load_penguins())\ntype(tibble)\n\ntidypolars.tibble.Tibble\n\n\nWe still have a .head() method that prints the first rows. The representation is very similar to a pandas data frame.\n\ntibble.head()\n\n\n\n\nshape: (5, 8)\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\nyear\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nstr\n\n\ni64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\nnull\n\n\nnull\n\n\nnull\n\n\nnull\n\n\nnull\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\n\n\nTo filter rows we can use the .filter() method. In the next chunk of code we use col(\"*\") to select all columns, tp.is_not_null() to flag observations with non-null values, and .filter() to use those booleans to actually perform the filtering.\nUpdate: See this issue where the main author of tidypolars lets me know that we can use the .drop_null() method instead.\n\ntibble.drop_null()\n\n\n\n\nshape: (333, 8)\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\nyear\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nstr\n\n\ni64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.3\n\n\n20.6\n\n\n190.0\n\n\n3650.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n38.9\n\n\n17.8\n\n\n181.0\n\n\n3625.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.2\n\n\n19.6\n\n\n195.0\n\n\n4675.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n41.1\n\n\n17.6\n\n\n182.0\n\n\n3200.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n38.6\n\n\n21.2\n\n\n191.0\n\n\n3800.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n34.6\n\n\n21.1\n\n\n198.0\n\n\n4400.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n36.6\n\n\n17.8\n\n\n185.0\n\n\n3700.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n38.7\n\n\n19.0\n\n\n195.0\n\n\n3450.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n45.2\n\n\n16.6\n\n\n191.0\n\n\n3250.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n49.3\n\n\n19.9\n\n\n203.0\n\n\n4050.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n50.2\n\n\n18.8\n\n\n202.0\n\n\n3800.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n45.6\n\n\n19.4\n\n\n194.0\n\n\n3525.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n51.9\n\n\n19.5\n\n\n206.0\n\n\n3950.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n46.8\n\n\n16.5\n\n\n189.0\n\n\n3650.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n45.7\n\n\n17.0\n\n\n195.0\n\n\n3650.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n55.8\n\n\n19.8\n\n\n207.0\n\n\n4000.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n43.5\n\n\n18.1\n\n\n202.0\n\n\n3400.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n49.6\n\n\n18.2\n\n\n193.0\n\n\n3775.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n50.8\n\n\n19.0\n\n\n210.0\n\n\n4100.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n50.2\n\n\n18.7\n\n\n198.0\n\n\n3775.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\n\n\nIt’s very easy to select columns with the .select() method.\n\ntibble.select(\"species\", \"sex\", \"body_mass_g\")\n\n\n\n\nshape: (344, 3)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3800.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\nnull\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3625.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n3475.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n4250.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n3300.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n3700.0\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4050.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3800.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3525.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3950.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4000.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3400.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3775.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4100.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3775.0\n\n\n\n\n\n\n\nWhat’s even better, we can chain these operations. This is where one can start seeing how powerful this approach is.\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n)\n\n\n\n\nshape: (333, 3)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3800.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3625.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3200.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3800.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4400.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3700.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4050.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3800.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3525.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3950.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4000.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3400.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3775.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4100.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3775.0\n\n\n\n\n\n\n\nWe then have the .summarise() method to compute summaries (or aggregations) by groups. Note we use functions available in the tidypolars namespace such as tp.mean().\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .summarise(\n        body_mass_mean=tp.mean(\"body_mass_g\"),\n        by=\"species\"\n    )\n)\n\n\n\n\nshape: (3, 2)\n\n\n\n\nspecies\n\n\nbody_mass_mean\n\n\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"Adelie\"\n\n\n3706.164384\n\n\n\n\n\"Gentoo\"\n\n\n5092.436975\n\n\n\n\n\"Chinstrap\"\n\n\n3733.088235\n\n\n\n\n\n\n\nIf we want to get a behavior similar to .groupby() and .transform() in pandas, we can use another verb, mutate().\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_mean=tp.mean(\"body_mass_g\"),\n        by=\"species\"\n    )\n)\n\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_mean\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4500.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4450.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5400.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4550.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4800.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5200.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4400.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5150.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4650.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5550.0\n\n\n5092.436975\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3400.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3475.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3050.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3725.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3000.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4250.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3475.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3700.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4000.0\n\n\n3706.164384\n\n\n\n\n\n\n\nThis verb is very powerful. We can compute more complex expressions, such as the one involved in the standardization of a variable.\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=(col(\"body_mass_g\") - tp.mean(\"body_mass_g\")) / tp.sd(\"body_mass_g\"),\n        by=\"species\"\n    )\n)\n\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4500.0\n\n\n-1.181386\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n1.211549\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4450.0\n\n\n-1.281092\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n1.211549\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5400.0\n\n\n0.613315\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4550.0\n\n\n-1.0816\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4800.0\n\n\n-0.583152\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5200.0\n\n\n0.214493\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4400.0\n\n\n-1.380797\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5150.0\n\n\n0.114787\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4650.0\n\n\n-0.882269\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5550.0\n\n\n0.912432\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3400.0\n\n\n-0.667577\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3475.0\n\n\n-0.504043\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3050.0\n\n\n-1.430736\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3725.0\n\n\n0.041\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3000.0\n\n\n-1.539759\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n-0.122464\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4250.0\n\n\n1.185808\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3475.0\n\n\n-0.504043\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n-0.558555\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n0.095582\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3700.0\n\n\n-0.013441\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4000.0\n\n\n0.640695\n\n\n\n\n\n\n\nAnd finally, we can use the .arrange() verb to sort observations by the standardized body mass in ascending order.\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=(col(\"body_mass_g\") - tp.mean(\"body_mass_g\")) / tp.sd(\"body_mass_g\"),\n        by=\"species\"\n    )\n    .arrange(\"body_mass_z\")\n)\n\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2700.0\n\n\n-2.687988\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n3950.0\n\n\n-2.278148\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-2.167609\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4150.0\n\n\n-1.879326\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6050.0\n\n\n1.909489\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4500.0\n\n\n1.995425\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4650.0\n\n\n2.0579\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n2.112501\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4550.0\n\n\n2.1255\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4700.0\n\n\n2.167013\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4725.0\n\n\n2.221524\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4775.0\n\n\n2.330547\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6300.0\n\n\n2.408017\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4800.0\n\n\n2.775994\n\n\n\n\n\n\n\nWe can make it even clearer if we wrap the standardization operation within a function.\n\ndef standardize(name):\n    return (col(name) - tp.mean(name)) / tp.sd(name)\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=standardize(\"body_mass_g\"),\n        by=\"species\"\n    )\n    .arrange(\"body_mass_z\")\n)\n\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2700.0\n\n\n-2.687988\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n3950.0\n\n\n-2.278148\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-2.167609\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4150.0\n\n\n-1.879326\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6050.0\n\n\n1.909489\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4500.0\n\n\n1.995425\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4650.0\n\n\n2.0579\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n2.112501\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4550.0\n\n\n2.1255\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4700.0\n\n\n2.167013\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4725.0\n\n\n2.221524\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4775.0\n\n\n2.330547\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6300.0\n\n\n2.408017\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4800.0\n\n\n2.775994\n\n\n\n\n\n\n\nIf we want to carry on the .mutate() operation grouped by more than one variable, we can simply pass the names in a list.\n\n\n\nLet’s see both approaches in action so we can better appreciate the differences.\n\n\n\ndata = palmerpenguins.load_penguins()\ndata = data[[\"species\", \"sex\", \"body_mass_g\"]].dropna().reset_index(drop=True)\ndata[\"body_mass_z\"] = (\n    (data[\"body_mass_g\"] - data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"mean\")) \n    / data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"std\")\n)\ndata.sort_values(\"body_mass_z\")\n\n\n\n\n\n  \n    \n      \n      species\n      sex\n      body_mass_g\n      body_mass_z\n    \n  \n  \n    \n      303\n      Chinstrap\n      female\n      2700.0\n      -2.899080\n    \n    \n      185\n      Gentoo\n      female\n      3950.0\n      -2.591611\n    \n    \n      188\n      Gentoo\n      male\n      4750.0\n      -2.346530\n    \n    \n      287\n      Chinstrap\n      female\n      2900.0\n      -2.198147\n    \n    \n      113\n      Adelie\n      male\n      3325.0\n      -2.071711\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      108\n      Adelie\n      female\n      3900.0\n      1.971803\n    \n    \n      103\n      Adelie\n      male\n      4775.0\n      2.109234\n    \n    \n      273\n      Chinstrap\n      female\n      4150.0\n      2.182685\n    \n    \n      302\n      Chinstrap\n      male\n      4800.0\n      2.377631\n    \n    \n      163\n      Gentoo\n      male\n      6300.0\n      2.603039\n    \n  \n\n333 rows × 4 columns\n\n\n\n\n\n\n\ndef standardize(name):\n    return (col(name) - tp.mean(name)) / tp.sd(name)\n\n\ntibble = tp.from_pandas(palmerpenguins.load_penguins())\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=standardize(\"body_mass_g\"),\n        by=[\"species\", \"sex\"]\n    )\n    .arrange(\"body_mass_z\")\n)\n\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2700.0\n\n\n-2.899\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n3950.0\n\n\n-2.591611\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n4750.0\n\n\n-2.3465\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-2.198147\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3325.0\n\n\n-2.071711\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.926035\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.926035\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3250.0\n\n\n-1.902511\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4150.0\n\n\n-1.881329\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n4925.0\n\n\n-1.787708\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3425.0\n\n\n-1.7833\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3300.0\n\n\n-1.764442\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4650.0\n\n\n1.748808\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6050.0\n\n\n1.804721\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n1.820893\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n5200.0\n\n\n1.847652\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n5200.0\n\n\n1.847652\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4700.0\n\n\n1.892979\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4725.0\n\n\n1.965064\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3900.0\n\n\n1.971803\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4775.0\n\n\n2.109234\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n4150.0\n\n\n2.182685\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4800.0\n\n\n2.377631\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6300.0\n\n\n2.603039\n\n\n\n\n\n\n\n\n\n\nPandas is great, so many people love it, and it will be the most used tool to work with tabular data in Python for a long time. In my case, it made me struggle a lot when I started to do data analysis in Python after working several years with R. Later, and fortunately, I began to understand how it worked and I became better at it.\nHowever, I never felt as comfortable with pandas as I do with dplyr syntax. Maybe it’s my R background? I don’t know. I just know that one of the things I enjoy the most in R is how easy and clear is to compose data manipulation operations with dplyr and friends. And having that expressiveness in Python is fantastic."
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html",
    "href": "posts/2022-06-12_lkj-prior/index.html",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "Throughout this blogpost, I will be working with the famous sleepstudy dataset. I’m going to estimate a hierarchical linear regression with both varying intercepts and varying slopes. The goal is to understand how to place non-independent priors for the group-specific effects in PyMC as efficiently as possible.\nThe sleepstudy dataset is derived from the study described in Belenky et al. (2003) and popularized in the lme4 R package. This dataset contains the average reaction time per day (in milliseconds) on a series of tests for the most sleep-deprived group in a sleep deprivation study. The first two days of the study are considered as adaptation and training, the third day is a baseline, and sleep deprivation started after day 3. The subjects in this group were restricted to 3 hours of sleep per night.\nWith that said, let’s get into the code!\n\nimport arviz as az\nimport aesara.tensor as at\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\n\n\n%matplotlib inline\naz.style.use(\"arviz-darkgrid\")\nmpl.rcParams[\"figure.facecolor\"] = \"white\"\n\nLet’s get started by downloading and exploring sleepstudy dataset.\n\nurl = \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/lme4/sleepstudy.csv\"\ndata = pd.read_csv(url, index_col = 0)\n\nThe following is a description of the variables we have in the dataset.\n\nReaction: Average of the reaction time measurements on a given subject for a given day.\nDays: Number of days of sleep deprivation.\nSubject: The subject ID.\n\n\nprint(f\"There are {len(data)} observations.\")\ndata.head()\n\nThere are 180 observations.\n\n\n\n\n\n\n  \n    \n      \n      Reaction\n      Days\n      Subject\n    \n  \n  \n    \n      1\n      249.5600\n      0\n      308\n    \n    \n      2\n      258.7047\n      1\n      308\n    \n    \n      3\n      250.8006\n      2\n      308\n    \n    \n      4\n      321.4398\n      3\n      308\n    \n    \n      5\n      356.8519\n      4\n      308\n    \n  \n\n\n\n\n\nprint(f\"Days range from {data['Days'].min()} to {data['Days'].max()}.\")\nprint(f\"There are J={data['Subject'].unique().size} subjects.\")\n\nDays range from 0 to 9.\nThere are J=18 subjects.\n\n\nLet’s explore the evolution of the reaction times through the days for every subject.\n\ndef plot_data(data, figsize=(16, 7.5)):\n    fig, axes = plt.subplots(2, 9, figsize=figsize, sharey=True, sharex=True)\n    fig.subplots_adjust(left=0.075, right=0.975, bottom=0.075, top=0.925, wspace=0.03)\n\n    for i, (subject, ax) in enumerate(zip(data[\"Subject\"].unique(), axes.ravel())):\n        idx = data.index[data[\"Subject\"] == subject].tolist()\n        days = data.loc[idx, \"Days\"].values\n        reaction = data.loc[idx, \"Reaction\"].values\n\n        # Plot observed data points\n        ax.scatter(days, reaction, color=\"C0\", ec=\"black\", alpha=0.7)\n\n        # Add a title\n        ax.set_title(f\"Subject: {subject}\", fontsize=14)\n\n    ax.xaxis.set_ticks([0, 2, 4, 6, 8])\n    fig.text(0.5, 0.02, \"Days\", fontsize=14)\n    fig.text(0.03, 0.5, \"Reaction time (ms)\", rotation=90, fontsize=14, va=\"center\")\n\n    return fig, axes\n\n\nplot_data(data);\n\n\n\n\nFor most of the subjects, there’s a clear positive association between Days and Reaction time. Reaction times increase as people accumulate more days of sleep deprivation. Participants differ in the initial reaction times as well as in the association between sleep deprivation and reaction time. Reaction times increase faster for some subjects and slower for others. Finally, the relationship between Days and Reaction time presents some deviations from linearity within the panels, but these are neither substantial nor systematic.\n\n\nThe model we’re going to build today is a hierarchical linear regression, with a Gaussian likelihood. In the following description, I use the greek letter \\(\\beta\\) to refer to common effects and the roman letter \\(u\\) to refer to group-specific (or varying) effects.\n\\[\ny_{ij} = \\beta_0 + u_{0j} + \\left( \\beta_1 + u_{1j} \\right) \\cdot {\\text{Days}} + \\epsilon_i\n\\]\nwhere\n\\[\n\\begin{aligned}\ny_{ij} &= \\text{Reaction time for the subject } j \\text{ on day } i \\\\\n\\beta_0 &= \\text{Intercept common to all subjects} \\\\\n\\beta_1 &= \\text{Days slope common to all subjects} \\\\\nu_{0j} &= \\text{Intercept deviation of the subject } j \\\\\nu_{1j} &= \\text{Days slope deviation of the subject } j \\\\\n\\epsilon_i &= \\text{Residual random error}\n\\end{aligned}\n\\]\nwe also have\n\\[\n\\begin{aligned}\ni = 1, \\cdots, 10 \\\\\nj = 1, \\cdots, 18\n\\end{aligned}\n\\]\nwhere \\(i\\) indexes Days and \\(j\\) indexes subjects.\nFrom the mathematical description we notice both the intercept and the slope are made of two components. The intercept is made of a common or global intercept \\(\\beta_0\\) and subject-specific deviations \\(u_{0j}\\). The same logic applies for the slope with both \\(\\beta_1\\) and \\(u_{1j}\\).\n\n\n\n\n\nFor the common effects, we Guassian independent priors.\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal}\\left(\\bar{y}, \\sigma_{\\beta_0}\\right) \\\\\n\\beta_1 \\sim \\text{Normal}\\left(0, \\sigma_{\\beta_1}\\right)\n\\end{array}\n\\]\nBambi centers the prior for the intercept at \\(\\bar{y}\\), so do we. For \\(\\sigma_{\\beta_0}\\) and \\(\\sigma_{\\beta_1}\\) I’m going to use 50 and 10 respectively. We’ll use these same priors for all the variants of the model above.\n\n\n\n\\[\n\\begin{aligned}\n\\epsilon_i &\\sim \\text{Normal}(0, \\sigma) \\\\\n\\sigma &\\sim \\text{HalfStudentT}(\\nu, \\sigma_\\epsilon)\n\\end{aligned}\n\\]\nWhere \\(\\nu\\) and \\(\\sigma_\\epsilon\\), both positive constants, represent the degrees of freedom and the scale parameter, respectively.\n\n\n\nThroughout this post we’ll propose the following variants for the priors of the group-specific effects.\n\nIndependent priors.\nCorrelated priors.\n\nUsing LKJCholeskyCov.\nUsing LKJCorr.\nUsign LKJCorr with non-trivial standard deviation.\n\n\nEach of them will be described in more detail in its own section.\nThen we create subjects and subjects_idx. These represent the subject IDs and their indexes. These are used with the distribution of the group-specific coefficients. We also have the coords that we pass to the model and the mean of the prior for the intercept\n\n# Subjects and subjects index\nsubjects, subjects_idx = np.unique(data[\"Subject\"], return_inverse=True)\n\n# Coordinates to handle dimensions of PyMC distributions and use better labels\ncoords = {\"subject\": subjects}\n\n# Response mean -- Used in the prior for the intercept\ny_mean = data[\"Reaction\"].mean()\n\n# Days variable\ndays = data[\"Days\"].values\n\n\n\n\n\n\n\n\\[\n\\begin{array}{lr}\nu_{0j} \\sim \\text{Normal} \\left(0, \\sigma_{u_0}\\right) & \\text{for all } j:1,..., 18 \\\\\nu_{1j} \\sim \\text{Normal} \\left(0, \\sigma_{u_1}\\right) & \\text{for all } j:1,..., 18\n\\end{array}\n\\]\nwhere the hyperpriors are\n\\[\n\\begin{array}{c}\n\\sigma_{u_0} \\sim \\text{HalfNormal} \\left(\\tau_0\\right) \\\\\n\\sigma_{u_1} \\sim \\text{HalfNormal} \\left(\\tau_1\\right)\n\\end{array}\n\\]\nwhere \\(\\tau_0\\) and \\(\\tau_1\\) represent the standard deviations of the hyperpriors. These are fixed positive constants. We set them to the same values than \\(\\sigma_{\\beta_0}\\) and \\(\\sigma_{\\beta_1}\\) respectively.\n\nwith pm.Model(coords=coords) as model_independent:\n    # Common effects\n    β0 = pm.Normal(\"β0\", mu=y_mean, sigma=50)\n    β1 = pm.Normal(\"β1\", mu=0, sigma=10)\n    \n    # Group-specific effects\n    # Intercept\n    σ_u0 = pm.HalfNormal(\"σ_u0\", sigma=50)\n    u0 = pm.Normal(\"u0\", mu=0, sigma=σ_u0, dims=\"subject\")\n    \n    # Slope\n    σ_u1 = pm.HalfNormal(\"σ_u1\", sigma=10)   \n    u1 = pm.Normal(\"u1\", mu=0, sigma=σ_u1, dims=\"subject\")\n   \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", β0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (β1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    μ = pm.Deterministic(\"μ\", intercept + slope)\n    \n    # Residual standard deviation\n    σ = pm.HalfStudentT(\"σ\", nu=4, sigma=50)\n    \n    # Response\n    y = pm.Normal(\"y\", mu=μ, sigma=σ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_independent)\n\n\n\n\n\nwith model_independent:\n    idata_independent = pm.sample(draws=1000, chains=4, random_seed=1234)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [β0, β1, σ_u0, u0, σ_u1, u1, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:31<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 33 seconds.\n\n\n\naz.plot_trace(\n    idata_independent, \n    var_names=[\"β0\", \"β1\", \"u0\", \"u1\", \"σ\", \"σ_u0\", \"σ_u1\"],\n    combined=True, \n    chain_prop={\"ls\": \"-\"}\n);\n\n\n\n\n\ndef plot_predictions(data, idata, figsize=(16, 7.5)):\n    # Plot the data\n    fig, axes = plot_data(data, figsize=figsize)\n    \n    # Extract predicted mean\n    reaction_mean = idata.posterior[\"μ\"]\n\n    for subject, ax in zip(subjects, axes.ravel()):\n        idx = (data[\"Subject\"]== subject).values\n        days = data.loc[idx, \"Days\"].values\n\n        # Plot highest density interval / credibility interval\n        az.plot_hdi(days, reaction_mean[..., idx], color=\"C0\", ax=ax)\n\n        # Plot mean regression line\n        ax.plot(days, reaction_mean[..., idx].mean((\"chain\", \"draw\")), color=\"C0\")\n    \n    return fig ,axes\n\n\nplot_predictions(data, idata_independent);\n\n\n\n\n\n\n\n\nInstead of placing independent priors on \\(u_{0j}\\) and \\(u_{1j}\\), we place a multivariate normal prior on \\([u_{0j}, u_{1j}]^T\\), which allows for correlated group-specific effects.\n\\[\n\\begin{array}{lr}\n    \\left[\n        \\begin{array}{c}\n            u_{0j} \\\\\n            u_{0j}\n        \\end{array}\n    \\right]\n    \\sim \\text{MvNormal}\n    \\left(\n        \\left[\n            \\begin{array}{c}\n                0 \\\\\n                0\n            \\end{array}\n        \\right],\n        \\Sigma =\n        \\left[\n            \\begin{array}{cc}\n                \\sigma_{u_0}^2 & \\text{cov}(u_0, u_1) \\\\\n                \\text{cov}(u_0, u_1) & \\sigma_{u_1}^2\n            \\end{array}\n        \\right]\n    \\right)\n    &\n    \\text{for all } j:1,..., 18\n\\end{array}\n\\]\nand we use the same hyperpriors\n\\[\n\\begin{array}{c}\n\\sigma_{u_0} \\sim \\text{HalfNormal} \\left(\\tau_0\\right) \\\\\n\\sigma_{u_1} \\sim \\text{HalfNormal} \\left(\\tau_1\\right)\n\\end{array}\n\\]\n\n\nIn practice, we do not set a prior for the covariance matrix \\(\\Sigma\\). Instead, we set a prior on the correlation matrix \\(\\Omega\\) and use the following decomposition to recover the covariance matrix\n\\[\n\\begin{split}\n    \\Sigma & =\n    \\begin{pmatrix}\n        \\sigma_{u_0} & 0 \\\\\n        0 & \\sigma_{u_1}\n    \\end{pmatrix}\n    \\begin{pmatrix} 1 & \\rho_{u_0, u_1} \\\\ \\rho_{u_0, u_1} & 1 \\end{pmatrix}\n    \\begin{pmatrix}\n        \\sigma_{u_0} & 0 \\\\\n        0 & \\sigma_{u_1}\n    \\end{pmatrix} \\\\\n      & = \\text{diag}(\\sigma_u) \\ \\Omega \\ \\text{diag}(\\sigma_u)\n\\end{split}\n\\]\nA very popular and flexible alternative is to place an LKJ prior on the correlation matrix.\n\\[\n\\Omega \\sim \\text{LKJ}(\\eta), \\ \\  \\eta > 0\n\\]\nLKJ stands for the Lewandowski-Kurowicka-Joe distribution. If \\(\\eta = 1\\) (our default choice), the prior is jointly uniform over all correlation matrices of the same dimension as \\(\\Omega\\). If \\(\\eta > 1\\), then the mode of the distribution is the identity matrix. The larger the value of \\(\\eta\\) the more sharply peaked the density is at the identity matrix\n\n\n\nFor efficiency and numerical stability, the correlation matrix \\(\\Omega\\) can be parametrized by its (lower-triangular) Cholesky factor \\(L\\), which can be seen as the square root of \\(\\Omega\\)\n\\[\n\\boldsymbol{L}\\boldsymbol{L^T} = \\Omega = \\begin{pmatrix} 1 & \\rho_{u_0, u_1} \\\\ \\rho_{u_0, u_1} & 1 \\end{pmatrix}\n\\]\nIf \\(\\boldsymbol{Z}_{\\text{uncorr}}\\) is a \\(2\\times n\\) matrix where the rows are 2 samples from uncorrelated random variables, then\n\\[\n\\begin{split}\n    \\boldsymbol{Z}_{\\text{corr}} & =\n    \\begin{pmatrix} \\sigma_{u_0} & 0 \\\\ 0 & \\sigma_{u_1} \\end{pmatrix}\n    \\boldsymbol{L}\n    \\boldsymbol{Z}_{\\text{uncorr}} \\\\\n    & = \\text{diag}(\\sigma_u) \\boldsymbol{L} \\boldsymbol{Z}_{\\text{uncorr}}     \n\\end{split}\n\\]\nThen \\(\\boldsymbol{Z}_{\\text{corr}}\\), of shape \\((2, n)\\), contains a sample of size \\(n\\) of two correlated variables with the desired variances \\(\\sigma_{u_0}^2\\), \\(\\sigma_{u_1}^2\\), and correlation \\(\\rho_{u_0, u_1}\\).\n\n\n\n\nPyMC conveniently implements a distribution called LKJCholeskyCov. Here, n represents the dimension of the correlation matrix. eta is the parameter of the LKJ distribution. sd_dist is the prior distribution for the standard deviations of the group-specific effects. compute_corr=True means we want it to also return the correlation between the group-specific parameters and their standard deviations. store_in_trace=False means we don’t want to store the correlation and the standard deviations in the trace.\nBefore seeing the code, we note that sd_dist is not a random variable, but a stateless distribution (i.e. the result of pm.Distribution.dist()).\n\ncoords.update({\"effect\": [\"intercept\", \"slope\"]})\n\nwith pm.Model(coords=coords) as model_lkj_cov:\n    ## Common effects\n    β0 = pm.Normal(\"β0\", mu=y_mean, sigma=50)\n    β1 = pm.Normal(\"β1\", mu=0, sigma=10)\n   \n    ## Group-specific effects\n    # Hyper prior for the standard deviations\n    u_σ = pm.HalfNormal.dist(sigma=np.array([50, 10]), shape=2)\n    \n    # Obtain Cholesky factor for the covariance\n    L, ρ_u, σ_u = pm.LKJCholeskyCov(\n        \"L\", n=2, eta=1, sd_dist=u_σ, compute_corr=True, store_in_trace=False\n    )\n    \n    # Parameters\n    u_raw = pm.Normal(\"u_raw\", mu=0, sigma=1, dims=(\"effect\", \"subject\")) \n    u = pm.Deterministic(\"u\", at.dot(L, u_raw).T, dims=(\"subject\", \"effect\"))\n   \n    ## Separate group-specific terms \n    # Intercept\n    u0 = pm.Deterministic(\"u0\", u[:, 0], dims=\"subject\")\n    σ_u0 = pm.Deterministic(\"σ_u0\", σ_u[0])\n    \n    # Slope\n    u1 = pm.Deterministic(\"u1\", u[:, 1], dims=\"subject\")\n    σ_u1 = pm.Deterministic(\"σ_u1\", σ_u[1])\n\n    # Correlation\n    ρ_u = pm.Deterministic(\"ρ_u\", ρ_u[0, 1])\n    \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", β0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (β1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    μ = pm.Deterministic(\"μ\", intercept + slope)\n    \n    ## Residual standard deviation\n    σ = pm.HalfStudentT(\"σ\", nu=4, sigma=56)\n    \n    ## Response\n    y = pm.Normal(\"y\", mu=μ, sigma=σ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_lkj_cov)\n\n\n\n\n\nwith model_lkj_cov:\n    idata_lkj_cov = pm.sample(draws=1000, chains=4, random_seed=1234, target_accept=0.9)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [β0, β1, L, u_raw, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 01:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 66 seconds.\n\n\n\naz.plot_trace(\n    idata_lkj_cov, \n    var_names=[\"β0\", \"β1\", \"u0\", \"u1\", \"σ\", \"σ_u0\", \"σ_u1\", \"ρ_u\"],\n    combined=True, \n    chain_prop={\"ls\": \"-\"}\n);\n\n\n\n\nFrom the traceplot of the correlation coefficient ρ_u it looks like the group-specific intercept and slope are not related since the distribution is centered around zero.\nBut there’s another question we haven’t answered yet: Are the initial reaction times associated with how much the sleep deprivation affects the evolution of reaction times? Let’s create a scatterplot to visualize the joint posterior of the subject-specific intercepts and slopes. This chart uses different colors for the individuals.\n\nposterior_u0 = idata_lkj_cov.posterior[\"u0\"].values\nposterior_u1 = idata_lkj_cov.posterior[\"u1\"].values\n\nfig, ax = plt.subplots()\nfor subject in range(18):\n    # Not all the samples are drawn\n    x = posterior_u0[::10, :, subject]\n    y = posterior_u1[::10, :, subject]\n    ax.scatter(x, y, alpha=0.5)\n    \nax.axhline(c=\"k\", ls=\"--\", alpha=0.5)\nax.axvline(c=\"k\", ls=\"--\", alpha=0.5)\nax.set(xlabel=\"Subject-specific intercept\", ylabel=\"Subject-specific slope\");\n\n\n\n\nIf we look at the bigger picture, i.e omitting the groups, we can conclude there’s no association between the intercept and slope. In other words, having lower or higher intial reaction times does not say anything about how much sleep deprivation affects the average reaction time on a given subject.\nOn the other hand, if we look at the joint posterior for a given individual, we can see a negative correlation between the intercept and the slope. This is telling that, conditional on a given subject, the intercept and slope posteriors are not independent. However, it doesn’t imply anything about the overall relationship between the intercept and the slope, which is what we need if we want to know whether the initial time is associated with how much sleep deprivation affects the reaction time.\nLet’s check predictions\n\nplot_predictions(data, idata_lkj_cov);\n\n\n\n\n\n\n\nOne problem with LKJCholeskyCov is that its sd_dist argument must be a stateless distribution and thus we cannot use a customized distribution for the standard deviations of the group-specific effects.\nIf we want to use a random variable instead of a stateless distribution for the standard deviation of the group-specific effects, then we need to perform many steps manually. Let’s see how we can implement it!\n\nwith pm.Model(coords=coords) as model_lkj_corr:\n    # Common part\n    β0 = pm.Normal(\"β0\", mu=y_mean, sigma=50)\n    β1 = pm.Normal(\"β1\", mu=0, sigma=10)\n    \n    # Group-specific part   \n    σ_u = pm.HalfNormal(\"u_σ\", sigma=np.array([50, 10]), dims=\"effect\")\n    \n    # Triangular upper part of the correlation matrix\n    Ω_triu = pm.LKJCorr(\"Ω_triu\", eta=1, n=2)\n\n    # Construct correlation matrix\n    Ω = pm.Deterministic(\n        \"Ω\", \n        at.fill_diagonal(Ω_triu[np.zeros((2, 2), dtype=np.int64)], 1)\n    )\n    \n    # Construct diagonal matrix of standard deviation\n    σ_diagonal = pm.Deterministic(\"σ_diagonal\", at.eye(2) * σ_u)\n    \n    # Compute covariance matrix\n    Σ = at.nlinalg.matrix_dot(σ_diagonal, Ω, σ_diagonal)\n    \n    # Cholesky decomposition of covariance matrix\n    L = pm.Deterministic(\"L\", at.slinalg.cholesky(Σ))\n    \n    # And finally get group-specific coefficients\n    u_raw = pm.Normal(\"u_raw\", mu=0, sigma=1, dims=(\"effect\", \"subject\")) \n    u = pm.Deterministic(\"u\", at.dot(L, u_raw).T, dims=(\"subject\", \"effect\"))\n\n    u0 = pm.Deterministic(\"u0\", u[:, 0], dims=\"subject\")\n    σ_u0 = pm.Deterministic(\"σ_u0\", σ_u[0])\n    \n    u1 = pm.Deterministic(\"u1\", u[:, 1], dims=\"subject\")\n    σ_u1 = pm.Deterministic(\"σ_u1\", σ_u[1])\n    \n    # Correlation\n    ρ_u = pm.Deterministic(\"ρ_u\", Ω[0, 1])\n         \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", β0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (β1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    μ = pm.Deterministic(\"μ\", intercept + slope)\n    \n    σ = pm.HalfStudentT(\"σ\", nu=4, sigma=50)\n    y = pm.Normal(\"y\", mu=μ, sigma=σ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_lkj_corr)\n\n\n\n\n\nwith model_lkj_corr:\n    idata_lkj_corr = pm.sample(draws=1000, chains=2, random_seed=1234, target_accept=0.9)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [β0, β1, u_σ, Ω_triu, u_raw, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 01:15<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 76 seconds.\n\n\n\naz.plot_trace(\n    idata_lkj_corr, \n    var_names=[\"β0\", \"β1\", \"u0\", \"u1\", \"σ\", \"σ_u0\", \"σ_u1\", \"ρ_u\"],\n    combined=True, \n    chain_prop={\"ls\": \"-\"}\n);\n\n\n\n\n\nplot_predictions(data, idata_lkj_corr);\n\n\n\n\n\n\n\nThis model is like the previous one, but σ_u is the result of multiplying several random variables. Rstanarm prior is introduced here\nNOTE: σ_u is what I would like to be able to pass to the sd_dist argument in pm.LKJCholeskyCov. Since I can only pass a stateless distribution, I need to perform all the steps manually.\n\nThe vector of variances is set equal to the product of a simplex vector \\(\\pi\\) — which is non-negative and sums to 1 — and the scalar trace: \\(J\\tau^2\\pi\\).\n[…]\nFor the simplex vector \\(\\pi\\) we use a symmetric Dirichlet prior which has a single concentration parameter \\(\\gamma > 0\\).\n\nOn top of that, the \\(J\\tau^2\\pi\\) is scaled by the residual standard deviation as explained in this comment.\n\nJ = 2 # Order of covariance matrix\n\nwith pm.Model(coords=coords) as model_lkj_corr_2:\n    # Common part\n    β0 = pm.Normal(\"β0\", mu=y_mean, sigma=50)\n    β1 = pm.Normal(\"β1\", mu=0, sigma=10)\n    \n    # Residual SD \n    σ = pm.HalfStudentT(\"σ\", nu=4, sigma=50)\n    \n    # Group-specific part\n    # Begin of rstanarm approach ----------------------------------\n    τ = pm.Gamma(\"τ\", alpha=1, beta=1)\n    Σ_trace = J * τ ** 2\n    π = pm.Dirichlet(\"π\", a=np.ones(J), dims=\"effect\")\n    σ_u = pm.Deterministic(\"b_σ\", σ * π * (Σ_trace) ** 0.5)\n    # End of rstanarm approach ------------------------------------\n    \n    # Triangular upper part of the correlation matrix\n    Ω_triu = pm.LKJCorr(\"Ω_triu\", eta=1, n=2)\n     \n    # Correlation matrix\n    Ω = pm.Deterministic(\n        \"Ω\", at.fill_diagonal(Ω_triu[np.zeros((2, 2), dtype=np.int64)], 1.)\n    )\n\n    # Construct diagonal matrix of standard deviation\n    σ_u_diagonal = pm.Deterministic(\"σ_u_diagonal\", at.eye(2) * σ_u)\n    \n    # Covariance matrix\n    Σ = at.nlinalg.matrix_dot(σ_u_diagonal, Ω, σ_u_diagonal)\n    \n    # Cholesky decomposition, lower triangular matrix.\n    L = pm.Deterministic(\"L\", at.slinalg.cholesky(Σ))\n    u_raw = pm.Normal(\"u_raw\", mu=0, sigma=1, dims=(\"effect\", \"subject\")) \n    \n    u = pm.Deterministic(\"u\", at.dot(L, u_raw).T, dims=(\"subject\", \"effect\"))\n                         \n    u0 = pm.Deterministic(\"u0\", u[:, 0], dims=\"subject\")\n    σ_u0 = pm.Deterministic(\"σ_u0\", σ_u[0])\n    \n    u1 = pm.Deterministic(\"u1\", u[:, 1], dims=\"subject\")\n    σ_u1 = pm.Deterministic(\"σ_u1\", σ_u[1])\n    \n    # Correlation\n    ρ_u = pm.Deterministic(\"ρ_u\", Ω[0, 1])\n         \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", β0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (β1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    μ = pm.Deterministic(\"μ\", intercept + slope)\n       \n    y = pm.Normal(\"y\", mu=μ, sigma=σ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_lkj_corr_2)\n\n\n\n\n\nwith model_lkj_corr_2:\n    idata_lkj_corr_2 = pm.sample(draws=1000, chains=4, random_seed=1234, target_accept=0.9)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [β0, β1, σ, τ, π, Ω_triu, u_raw]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 03:00<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 181 seconds.\n\n\n\naz.plot_trace(\n    idata_lkj_corr_2,\n    var_names=[\"β0\", \"β1\", \"u0\", \"u1\", \"σ\", \"σ_u0\", \"σ_u1\", \"ρ_u\", \"τ\", \"π\"],\n);\n\n\n\n\n\nplot_predictions(data, idata_lkj_corr_2);\n\n\n\n\n\n\n\n\ngroups = [\n    [\"β0\", \"β1\", \"σ\"],\n    [\"u0\", \"σ_u0\"],\n    [\"u1\", \"σ_u1\"],\n]\n\nmodel_names = [\"Indepentent\", \"LKJCholeskyCov\", \"LKJCorr\", \"LKJCorr + rstanarm\"]\nfig, ax = plt.subplots(1, 3, figsize = (20, 10))\n\nfor idx, group in enumerate(groups):\n    az.plot_forest(\n        [idata_independent, idata_lkj_cov, idata_lkj_corr, idata_lkj_corr_2],\n        model_names=model_names,\n        var_names=group,\n        combined=True,\n        ax=ax[idx],\n    )\n\n\n\n\n\naz.plot_forest(\n    [idata_lkj_cov, idata_lkj_corr, idata_lkj_corr_2],\n    model_names=model_names[1:],\n    var_names=[\"ρ_u\"],\n    combined=True\n);\n\n\n\n\n\n\n\n\n\n\nWe showed how to use correlated priors for group-specific coefficients.\nThe posteriors resulted to be the same for the in all the cases.\nThe correlated priors didn’t imply any benefit to our sampling process. However, this could be beneficial for more complex hierarchical models.\nWhat’s more, the model with the correlated priors took more time to sample than the one with independent priors.\nAttempting to replicate rstanarm approach takes even longer because we are forced to compute many things manually.\n\n\n\n\n\nSometimes, the models with correlated priors based on pm.LKJCorr resulted in divergences. We needed to increase target_accept.\nIt would be good to be able to pass a random variable to sd_dist in pm.LKJCholeskyCov, and not just a stateless distribution. This forced me to use pm.LKJCorr and perform many manipulations manually, which was more error-prone and inefficient.\nIt would be good to check if there’s something in the LKJCorr/LKJCholeskyCov that could be improved. I plan to use LKJCholeskyCov within Bambi in the future and I would like it to work as better as possible.\n\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Sun Jun 12 2022\n\nPython implementation: CPython\nPython version       : 3.10.4\nIPython version      : 8.3.0\n\nnumpy     : 1.21.6\npandas    : 1.4.2\nmatplotlib: 3.5.2\narviz     : 0.12.1\naesara    : 2.6.6\npymc      : 4.0.0\nsys       : 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html",
    "title": "Binomial family in Bambi",
    "section": "",
    "text": "Although GSoC 2021 is close to come to an end, there’s still a lot of exciting things going on around Bambi. Today I’m going to talk about another new family that’s about to be merged into the main branch, the Binomial family.\nLet’s get started by trying to see why we need to have another new family for modeling binary data in Bambi."
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#aggregated-vs-disaggregated-data",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#aggregated-vs-disaggregated-data",
    "title": "Binomial family in Bambi",
    "section": "Aggregated vs disaggregated data",
    "text": "Aggregated vs disaggregated data\nBambi already has the Bernoulli family to model binary data. This family fits very well when you have a data set where each row represents a single observation and there’s a column that represents the binary outcome ( i.e the result of the Bernoulli trial) as well as other columns with the predictor variables.\nLet’s say we want to study the lethality of a certain drug and we have a group of mice to experiment with. An approach could be to divide the mice into smaller groups, assign a certain dose to all the mice in each group, and then finally count the number of units that died after a fixed amount of time. Under the Bernoulli family paradigm, each row has to represent a single observation, looking like this:\n\n\n\n\nObs\nDose\nDied\n\n\n\n\n1\n1.3\n0\n\n\n2\n1.8\n1\n\n\n3\n2.2\n1\n\n\n\n\nwhere each row represents a single mouse (i.e. a single Bernoulli trial). The 0 is used to represent a failure/survival, and 1 is used to represent a successes/death.\nWhat if our data is aggregated? The nature of the experiment makes it natural to have rows representing groups, a column representing the number of deaths, and another column representing the number of mice in the group.\n\n\n\n\nGroup\nDose\nDead\nTotal\n\n\n\n\n1\n1.3\n12\n20\n\n\n2\n1.8\n18\n25\n\n\n3\n2.2\n24\n34\n\n\n\n\nwhere each row represents a group of mice. Dose is the dose applied to all the units in the group, Dead is the number of mice that died, and Total is the number of mice in the group. If we focus on the Dead and Total columns we can easily see they resemble data coming from a Binomial distribution (i.e. number of successes out of a series of \\(n\\) independent Bernoulli trials). In other words, for a given row, we can think there’s a Binomial distribution where Dead represents the number of successes out of Total number of trials (each mouse is a trial).\nBefore continuing, it’s important to note that if the data is originally aggregated as in the lower table, it can always be disaggregated to resemble the one in the upper table. So what’s the problem?\nThe answer is that there’s actually nothing wrong with having the data in such a granular form! But, if the data already comes aggregated, why doing extra work when we now have the Binomial family? Let’s have a look at the examples below!\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\naz.style.use(\"arviz-darkgrid\")\nBLUE = \"#003f5c\"\nPURPLE = \"#7a5195\"\nPINK = \"#ef5675\"\n\nWe’re going to use real data in this example1. This data consists of the numbers of beetles dead after five hours of exposure to gaseous carbon disulphide at various concentrations:\n\n\n\n\n\n\n\n\n\nDose, \\(x_i\\) (\\(\\log_{10}\\text{CS}_2\\text{mgl}^{-1}\\))\nNumber of beetles, \\(n_i\\)\nNumber killed, \\(y_i\\)\n\n\n\n\n1.6907\n59\n6\n\n\n1.7242\n60\n13\n\n\n1.7552\n62\n18\n\n\n1.7842\n56\n28\n\n\n1.8113\n63\n52\n\n\n1.8369\n59\n53\n\n\n1.8610\n62\n61\n\n\n1.8839\n60\n60\n\n\n\n\n\nx = np.array([1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839])\nn = np.array([59, 60, 62, 56, 63, 59, 62, 60])\ny = np.array([6, 13, 18, 28, 52, 53, 61, 60])\n\ndata = pd.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"n\": n\n})\n\nQuite simple, right? Can we use it as it is with the Bernoulli family? Let’s have a look below."
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#bernoulli-family",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#bernoulli-family",
    "title": "Binomial family in Bambi",
    "section": "Bernoulli family",
    "text": "Bernoulli family\nNope, no surprises today. To use the Bernoulli family, we first need to transform the data into the dissagregated or long format. One approach is the following\n\ndata_bernoulli = pd.DataFrame({\n    \"x\": np.concatenate([np.repeat(x, n) for x, n in zip(x, n)]),\n    \"killed\": np.concatenate([np.repeat([1, 0], [y, n - y]) for y, n in zip(y, n)])\n})\n\nDo you realize how bothering it can be to do that if we have many more variables? Nevermind, let’s keep going.\nNow let’s initialize a Bambi model and sample from the posterior:\n\nmodel_brn = bmb.Model(\"killed ~ x\", data_bernoulli, family=\"bernoulli\")\nidata_brn = model_brn.fit()\n\nModeling the probability that killed==1\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:03<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds.\n\n\nand explore the marginal posteriors\n\naz.summary(idata_brn, kind=\"stats\")\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      -61.021\n      5.250\n      -70.606\n      -51.311\n    \n    \n      x\n      34.443\n      2.954\n      28.933\n      39.745\n    \n  \n\n\n\n\nWe can predict the the probability of dying for out-of-sample data to see how it evolves with the different concentration levels.\n\nnew_data = pd.DataFrame({\"x\": np.linspace(1.6, 2, num=200)})\nmodel_brn.predict(idata_brn, data=new_data)\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\n\n# Plot HDI for the mean of the probability of dying\naz.plot_hdi(\n  new_data[\"x\"], \n  idata_brn.posterior[\"killed_mean\"].values, \n  color=BLUE,\n  ax=ax\n)\n\nax.plot(\n  new_data[\"x\"], \n  idata_brn.posterior[\"killed_mean\"].values.mean((0, 1)), \n  color=BLUE\n)\n\nax.scatter(x, y / n, s=50, color=PURPLE, edgecolors=\"black\", zorder=10)\nax.set_ylabel(\"Probability of death\")\nax.set_xlabel(r\"Dose $\\log_{10}CS_2mgl^{-1}$\")\nax.set_title(\"family='bernoulli'\")\n\nplt.show()"
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#binomial-family",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#binomial-family",
    "title": "Binomial family in Bambi",
    "section": "Binomial family",
    "text": "Binomial family\nBefore writing down the model with the Binomial family, let’s take a moment to review new notation that was added specifically for this purpose.\nThe model formula syntax only allows us to pass one variable on its LHS. Then, how do we tell Bambi that what we want to model is the proportion that results from dividing y over n?\nThanks to recent developments, it’s as easy as writing proportion(y, n), or any of its aliases prop(y, n) and p(y, n). To keep it shorter, let’s use the last one.\n\nmodel_bnml = bmb.Model(\"p(y, n) ~ x\", data, family=\"binomial\")\nidata_bnml = model_bnml.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:03<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\n\n\nQuite simple, right? The code here is very similar to the one for the model with the Bernoulli family. However, the new Binomial family allows us to use the data in its original form.\nLet’s finish this section by getting the marginal posteriors as well as a figure as the one displayed above.\n\naz.summary(idata_bnml, kind=\"stats\")\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      Intercept\n      -61.045\n      4.969\n      -69.905\n      -51.495\n    \n    \n      x\n      34.452\n      2.793\n      29.185\n      39.552\n    \n  \n\n\n\n\n\nmodel_bnml.predict(idata_bnml, data=new_data)\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\n\naz.plot_hdi(\n  new_data[\"x\"],\n  idata_bnml.posterior[\"p(y, n)_mean\"].values,\n  color=BLUE,\n  ax=ax\n)\n\nax.plot(\n  new_data[\"x\"], \n  idata_bnml.posterior[\"p(y, n)_mean\"].values.mean((0, 1)), \n  color=BLUE\n)\n\nax.scatter(x, y / n, s=50, color=PURPLE, edgecolors=\"black\", zorder=10)\nax.set_ylabel(\"Probability of death\")\nax.set_xlabel(r\"Dose $\\log_{10}CS_2mgl^{-1}$\")\nax.set_title(\"family='binomial'\")\nfig.set_facecolor(\"w\")\nfig.savefig(\"imgs/plot.png\")\nplt.show()"
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#conclusions",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#conclusions",
    "title": "Binomial family in Bambi",
    "section": "Conclusions",
    "text": "Conclusions\nThis blog post introduced the new Binomial family. This new family saves us from having to manipulate aggregated data prior to modeling, making it more pleasant and simpler to specify and fit models for binary data in Bambi."
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "",
    "text": "A linear mixed model can be written as\n\\[\n\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} +\n                 \\boldsymbol{Z}\\boldsymbol{u} + \\boldsymbol{\\epsilon}\n\\]\nwhere \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) are the two design matrices we need to somehow construct when dealing with this type of model. \\(\\boldsymbol{X}\\) is the design matrix for the common (a.k.a. fixed) effects, and \\(\\boldsymbol{Z}\\) is the design matrix for the group-specific (a.k.a. random or varying) effects.\nIt is quite easy to obtain the design matrix \\(\\boldsymbol{X}\\) in R using its popular formula interface. In Python, patsy provides equivalent functionality. Unfortunately, there aren’t as many alternatives to compute the matrix \\(\\boldsymbol{Z}\\).\nIn R, there’s lme4, the statistical package par excellence for mixed models. It extends the base formula interface to include group-specific effects via the pipe operator (|) and internally computes both \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) without the user noticing. That’s great!\nIn Python, we are working on formulae, a library we use to handle mixed model formulas in Bambi. In this process, I’ve found Fitting Linear Mixed-Effects Models Using lme4 vignette extremely useful when figuring out how to compute the design matrix for the group-specific effects.\nToday, I was adding tests to make sure we are constructing \\(\\boldsymbol{Z}\\) appropriately and found myself comparing the matrices obtained with formulae with matrices obtained with lme4. Then I was like … why not making this a blog post? 🤔\n… and so here we are! But before we get started, just note this post mixes both R and Python code. I will try to be explicit when I’m using one language or the other. But if you’re reading a chunk and it looks like Python, it’s Python. And if it looks like R… you guessed! It’s R."
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#setup",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#setup",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Setup",
    "text": "Setup\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lme4)\nlibrary(patchwork)\n\n\n\n\n\nfrom formulae import design_matrices"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#problem",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#problem",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Problem",
    "text": "Problem\nHere we will be comparing design matrices for the group-specific terms in a mixed-effects model obtained with both lme4 and formulae. We’re using the dataset Pixel that comes with the R package nlme.\n\ndata(\"Pixel\", package = \"nlme\")\nhead(Pixel)\n\nGrouped Data: pixel ~ day | Dog/Side\n  Dog Side day  pixel\n1   1    R   0 1045.8\n2   1    R   1 1044.5\n3   1    R   2 1042.9\n4   1    R   4 1050.4\n5   1    R   6 1045.2\n6   1    R  10 1038.9\n\n\nWe’re not interested in how to fit a certain model here. We’re interested in constructing the design matrix for group-specific effects with different characteristics. We use the following formula\n\nf1 = ~ (0 + day | Dog) + (1 | Side / Dog)\n\nwhere each part can be interpreted as follows\n\n(0 + day | Dog) means that day has a group-specific slope for each Dog. This is usually known as a random slope. The 0 indicates not to add the default group-specific intercept (because it’s added next).\n(1 | Side / Dog) is equivalent to (1 | Side) + (1 | Dog:Side). This means there’s a varying intercept for each Side and a varying intercept for each combination of Dog and Side. In other words, we have a nested group-specific intercept, where Dog is nested within Side.\n\n\nlme4_terms = mkReTrms(findbars(f1), model.frame(subbars(f1), data = Pixel))\n\nlme4_terms contains much more information than what we need for this post. We mostly use lme4_terms$Ztlist, which is a list that contains the transpose of the group-specific effects model matrix, separated by term. These matrices are stored as sparse matrices of dgCMatrix class. If we want to have the sub-matrix for a given group-specific term as a base R matrix, we have to do as.matrix(t(lme4_terms$Ztlist$[[\"term\"]])).\n\nnames(lme4_terms$Ztlist)\n\n[1] \"1 | Dog:Side\"  \"0 + day | Dog\" \"1 | Side\"     \n\n\nWe have three group-specific terms. The first and the last ones are the group-specific intercepts we mentioned. These are the result of the nested group-specific intercept (1 | Side / Dog). Dog is nested within Side and consequently there’s an intercept varying among Side and another varying among Dog within Side. The second term, 0 + day | Dog, represents varying slope of day for each level of Dog.\nWe finally store the sub-matrix for each term in different objects that we’ll later use when comparing results with those obtained with formulae.\n\nday_by_dog = as.matrix(t(lme4_terms$Ztlist$`0 + day | Dog`))\nintercept_by_side = as.matrix(t(lme4_terms$Ztlist$`1 | Side`))\nintercept_by_side_dog = as.matrix(t(lme4_terms$Ztlist$`1 | Dog:Side`))\n\nOn the other hand, in Python, we use design_matrices() from the formulae library to obtain a DesignMatrices object. All the information associated with the group-specific terms is contained in the .group attribute and the sub-matrix corresponding to a particular term is accessed with .group[term_name].\n\ndm = design_matrices(\"(0 + day | Dog) + (1 | Side / Dog)\", r.Pixel)\n\nThere’s a dictionary called terms_info within dm.group. To see the names of the group-specific effects we just retrieve the keys.\n\ndm.group.terms.keys()\n\ndict_keys(['day|Dog', '1|Side', '1|Side:Dog'])\n\n\nNames differ a little with the ones from lme4, but they represent the same thing.\n\nday_by_dog = dm.group['day|Dog']\nintercept_by_side = dm.group['1|Side']\nintercept_by_side_dog = dm.group['1|Side:Dog']\n\nNow let’s compare those matrices!"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#design-matrices-for-daydog",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#design-matrices-for-daydog",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Design matrices for (day|Dog)",
    "text": "Design matrices for (day|Dog)\nRectangles in the following plot correspond to the cells in the matrix. The lowest value for day is 0, represented by violet, and the highest value is 21, represented by yellow. The 10 columns represent the 10 groups in Dog, and the rows represent the observations in Pixel. Here, and also in the other cases, the left panel contains the matrix obtained with lme4 and the right panel the one produced with formulae.\n\n\n\n\n\n\n\n\n\nIn this first case, both panels are representing the same data so we can happily conclude the result obtained with formulae matches the one from lme4. Yay!!\nBut we’re humans and our eyes can fail so it’s better to always check appropiately with\n\nall(py$day_by_dog == day_by_dog)\n\n[1] TRUE\n\n\n\nDesign matrices for (1|Side)\nHere the first column represents Side == \"L\" and the second column represents Side == \"R\". Since we’re dealing with an intercept, violet means 0 and yellow means 1. In this case it is much easier to see both results match.\n\n\n\n\n\n\n\n\n\n\nall(py$intercept_by_side == intercept_by_side)\n\n[1] TRUE\n\n\n\n\nDesign matrices for (1|Side:Dog)\nBut things are not always as one wishes. It’s clear from the following plot that both matrices aren’t equal here.\n\n\n\n\n\n\n\n\n\nBut don’t worry. We’re not giving up. We still have things to do1. We can check what are the groups being represented in the columns of the matrices we’re plotting.\n\ncolnames(intercept_by_side_dog)\n\n [1] \"1:L\"  \"1:R\"  \"10:L\" \"10:R\" \"2:L\"  \"2:R\"  \"3:L\"  \"3:R\"  \"4:L\"  \"4:R\" \n[11] \"5:L\"  \"5:R\"  \"6:L\"  \"6:R\"  \"7:L\"  \"7:R\"  \"8:L\"  \"8:R\"  \"9:L\"  \"9:R\" \n\n\n\ndm.group.terms[\"1|Side:Dog\"].labels\n\n['1|Side[L]:Dog[1]', '1|Side[L]:Dog[10]', '1|Side[L]:Dog[2]', '1|Side[L]:Dog[3]', '1|Side[L]:Dog[4]', '1|Side[L]:Dog[5]', '1|Side[L]:Dog[6]', '1|Side[L]:Dog[7]', '1|Side[L]:Dog[8]', '1|Side[L]:Dog[9]', '1|Side[R]:Dog[1]', '1|Side[R]:Dog[10]', '1|Side[R]:Dog[2]', '1|Side[R]:Dog[3]', '1|Side[R]:Dog[4]', '1|Side[R]:Dog[5]', '1|Side[R]:Dog[6]', '1|Side[R]:Dog[7]', '1|Side[R]:Dog[8]', '1|Side[R]:Dog[9]']\n\n\nAnd there it is! Matrices differ because columns are representing different groups. In lme4, groups are looping first along Dog and then along Side, while in formulae it is the other way around.\nWe can simply re-order the columns of one of the matrices and generate and check whether they match or not.\n\nintercept_by_side_dog_f = as.data.frame(py$intercept_by_side_dog)\ncolnames(intercept_by_side_dog_f) = py$dm$group$terms[[\"1|Side:Dog\"]]$groups\nnames_lme4_order = paste(\n  rep(c(\"L\", \"R\"), 10),\n  rep(c(1, 10, 2, 3, 4, 5, 6, 7, 8, 9), each = 2),\n  sep = \":\"\n)\nintercept_by_side_dog_f = intercept_by_side_dog_f[names_lme4_order] %>%\n  as.matrix() %>%\n  unname()\n\n\n\n\n\n\n\n\n\n\n\nall(intercept_by_side_dog_f == intercept_by_side_dog)\n\n[1] TRUE\n\n\nAnd there it is! Results match 🤩"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#another-formula",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#another-formula",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Another formula",
    "text": "Another formula\nThis other formula contains an interaction between categorical variables as the expression of the group-specific term, which is something we’re not covering above. In this case, we are going to subset the data so the design matrices are smaller and we can understand what’s going on with more ease.\n\n# Subset data\nPixel2 = Pixel %>%\n  filter(Dog %in% c(1, 2, 3), day %in% c(2, 4, 6)) %>%\n  mutate(Dog = forcats::fct_drop(Dog))\n# Create terms with lme4\nf2 = ~ day +  (0 + Dog:Side | day)\nlme4_terms = mkReTrms(findbars(f2), model.frame(subbars(f2), data = Pixel2))\ndog_and_side_by_day = as.matrix(t(lme4_terms$Ztlist$`0 + Dog:Side | day`))\n\nAnd now with design_matrices() in Python.\n\n# Create terms with\ndm = design_matrices(\"(0 + Dog:Side|day)\", r.Pixel2)\ndog_and_side_by_day = dm.group[\"Dog:Side|day\"]\n\n\nDesign matrix for (Dog:Side|day)\nAlthough this term is called slope, it is not actually a slope like the one for (day|Dog). Since both Dog and Side are categorical, the entries of this matrix consist of zeros and ones.\n\n\n\n\n\n\n\n\n\nWe have the same problem than above, matrices don’t match. So we know what to do: look at the groups represented in the columns.\n\ncolnames(dog_and_side_by_day)\n\n [1] \"2\" \"2\" \"2\" \"2\" \"2\" \"2\" \"4\" \"4\" \"4\" \"4\" \"4\" \"4\" \"6\" \"6\" \"6\" \"6\" \"6\" \"6\"\n\n\n\ndm.group.terms[\"Dog:Side|day\"].labels\n\n['Dog[1]:Side[L]|day[2.0]', 'Dog[1]:Side[R]|day[2.0]', 'Dog[2]:Side[L]|day[2.0]', 'Dog[2]:Side[R]|day[2.0]', 'Dog[3]:Side[L]|day[2.0]', 'Dog[3]:Side[R]|day[2.0]', 'Dog[1]:Side[L]|day[4.0]', 'Dog[1]:Side[R]|day[4.0]', 'Dog[2]:Side[L]|day[4.0]', 'Dog[2]:Side[R]|day[4.0]', 'Dog[3]:Side[L]|day[4.0]', 'Dog[3]:Side[R]|day[4.0]', 'Dog[1]:Side[L]|day[6.0]', 'Dog[1]:Side[R]|day[6.0]', 'Dog[2]:Side[L]|day[6.0]', 'Dog[2]:Side[R]|day[6.0]', 'Dog[3]:Side[L]|day[6.0]', 'Dog[3]:Side[R]|day[6.0]']\n\n\nBut this they represent the same groups2. We can look if there’s a difference in how the interactions are ordered within each group.\n\nlme4_terms$cnms\n\n$day\n[1] \"Dog1:SideL\" \"Dog2:SideL\" \"Dog3:SideL\" \"Dog1:SideR\" \"Dog2:SideR\"\n[6] \"Dog3:SideR\"\n\n\nAnd again, thankfully, we see there’s a difference in how columns are being ordered. Let’s see if matrices match after we reorder the one obtained with formulae.\n\ndog_and_side_by_day_f = as.data.frame(py$dog_and_side_by_day)\ncolnames(dog_and_side_by_day_f) = py$dm$group$terms[[\"Dog:Side|day\"]]$labels\nside = rep(rep(c(\"L\", \"R\"), each = 3), 3)\ndog = rep(1:3, 6)\nday = rep(c(\"2.0\", \"4.0\", \"6.0\"), each = 6)\nnames_lme4_order = glue::glue(\"Dog[{dog}]:Side[{side}]|day[{day}]\")\ndog_and_side_by_day_f = dog_and_side_by_day_f[names_lme4_order] %>%\n  as.matrix() %>%\n  unname()\n\n\n\n\n\n\n\n\n\n\n\nall(dog_and_side_by_day_f == dog_and_side_by_day)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#conclusion",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#conclusion",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough formulae works differently than lme4, and has different goals, we showed that formulae produces the same design matrices as lme4 for the variety of examples we covered. While case-based comparisons like these are not what one should rely on when writing software, the examples here were really helpful when working on the implementation in formulae and writing the corresponding tests. And if this post helps someone to better understand what’s going on when working with design matrices associated with group-specific effects, it will have been even more worth it!"
  },
  {
    "objectID": "posts/2021-07-14_new-families-in-Bambi/index.html",
    "href": "posts/2021-07-14_new-families-in-Bambi/index.html",
    "title": "New families in Bambi",
    "section": "",
    "text": "I’m very happy I could contribute with many exciting changes to Bambi. Some changes, such as the reorganization of the default priors and built-in families, are not visible to the user but make the codebase more modular and easier to read. Other changes, such as the ones I’m going to describe here, have a direct impact on what you can do with Bambi.\nToday I’ll describe two new built-in families that have been added to Bambi. The first one, already described in my previous post, is the \"t\" family. This can be used to make linear regressions more robust to outliers. The second one the \"beta\" family which can be used to model ratings and proportions.\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/2021-07-14_new-families-in-Bambi/index.html#robust-linear-regression-with-the-t-family.",
    "href": "posts/2021-07-14_new-families-in-Bambi/index.html#robust-linear-regression-with-the-t-family.",
    "title": "New families in Bambi",
    "section": "Robust linear regression with the t family.",
    "text": "Robust linear regression with the t family.\nA Bayesian robust linear regression looks as follows\n\\[\ny_i \\sim \\text{StudentT}(\\mu_i, \\lambda, \\nu)\n\\]\nwhere \\(\\mu_i = \\beta_0 + \\beta_1 x_{1, i} + \\cdots + \\beta_p x_{p, i}\\), \\(\\lambda\\) is the precision parameter and \\(\\nu\\) is the degrees of freedom.\nThis wouldn’t be a Bayesian model without priors. Bambi uses the following priors by default:\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal}(\\mu_{\\beta_0}, \\sigma_{\\beta_0}) \\\\\n\\beta_j \\sim \\text{Normal}(\\mu_{\\beta_j}, \\sigma_{\\beta_j})  \\\\\n\\lambda \\sim \\text{HalfCauchy(1)}\n\\end{array}\n\\]\nwhere the \\(\\mu_{\\beta_j}\\) and \\(\\sigma_{\\beta_j}\\) are estimated from the data. By default, \\(\\nu=2\\), but it is also possible to assign it a probability distribution (as we’re going to see below).\nBefore seeing how this new family works, let’s simulate some data. On this opportunity, we’re using the same dataset than in the previous post. This is a toy dataset with one predictor x, one response y, and some outliers contaminating the beautiful linear relationship between the variables.\n\nsize = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ny = true_intercept + true_slope * x + np.random.normal(scale=0.5, size=size)\n\nx_out = np.append(x, [0.1, 0.15, 0.2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = pd.DataFrame(dict(x = x_out, y = y_out))\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\n\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7);\n\n\n\n\n\nModel specification and fit\nUsing this new family is extremely easy. It is almost as simple as running a default normal linear regression. The only difference is that we need to add the family=\"t\" argument to the Model() instantiation.\n\nmodel = bmb.Model(\"y ~ x\", data, family=\"t\")\nmodel\n\nFormula: y ~ x\nFamily name: T\nLink: identity\nObservations: 103\nPriors:\n  Common-level effects\n    Intercept ~ Normal(mu: 2.1234, sigma: 5.9491)\n    x ~ Normal(mu: 0.0, sigma: 10.4201)\n\n  Auxiliary parameters\n    sigma ~ HalfStudentT(nu: 4, sigma: 1.2227)\n    nu ~ Gamma(alpha: 2, beta: 0.1)\n\n\nThe output above shows information about the family being used and the parameters for the default priors. Next, we just do model.fit() to run the sampler.\n\nidata = model.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_nu, y_sigma, x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:02<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\n\n\n\n\nUse custom priors\nLet’s say we are not happy with having a fixed value for the degrees of freedom and we want to assign it a prior distribution. Is that a problem? Of course not!\n\n# Use a Gamma prior for the degrees of freedom\nmodel = bmb.Model(\"y ~ x\", data, family=\"t\")\nmodel.set_priors({\"nu\": bmb.Prior(\"Gamma\", alpha=3, beta=1)})\nmodel\n\nFormula: y ~ x\nFamily name: T\nLink: identity\nObservations: 103\nPriors:\n  Common-level effects\n    Intercept ~ Normal(mu: 2.1234, sigma: 5.9491)\n    x ~ Normal(mu: 0.0, sigma: 10.4201)\n\n  Auxiliary parameters\n    sigma ~ HalfStudentT(nu: 4, sigma: 1.2227)\n    nu ~ Gamma(alpha: 3, beta: 1)\n\n\nAnd hit the inference button\n\nidata = model.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_nu, y_sigma, x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:04<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds.\n\n\n\n\nExplore results\nFirst of all we can see the marginal posteriors for the parameters in the model and their respective traces\n\naz.plot_trace(idata)\nplt.tight_layout()\n\n\n\n\nAnd it is also good to explore the posterior distribution of regression lines\n\n# Prepare data\nx = np.linspace(0, 1, num=200)\nposterior_stacked = idata.posterior.stack(samples=(\"chain\", \"draw\"))\nintercepts = posterior_stacked[\"Intercept\"].values\nslopes = posterior_stacked[\"x\"].values\n\n# Create plot\nfig, ax = plt.subplots(figsize=(10, 7))\n\n# Data points\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7)\n\n# Posterior regression lines\nfor a, b in zip(intercepts, slopes):\n    ax.plot(x, a + b * x, color =\"0.5\", alpha=0.3, zorder=-1)\n\n# True regression line\nax.plot(x, true_intercept + true_slope * x, color=\"k\", lw=2);\n\n\n\n\nwhere the line in black is the true regression line."
  },
  {
    "objectID": "posts/2021-07-14_new-families-in-Bambi/index.html#beta-regression-with-the-beta-family.",
    "href": "posts/2021-07-14_new-families-in-Bambi/index.html#beta-regression-with-the-beta-family.",
    "title": "New families in Bambi",
    "section": "Beta regression with the beta family.",
    "text": "Beta regression with the beta family.\nBeta regression is useful to model response variables that have values within the \\((0, 1)\\) interval. This type of regression is based on the assumption that the conditional distribution of the response variable follows a Beta distribution with its mean related to a set of regressors through a linear predictor with unknown coefficients and a link function.\nThe beta regression model is based on an alternative parameterization of the beta density in terms of the mean \\(\\mu\\) and a precision parameter \\(\\kappa\\).\n\\[\n\\begin{array}{lr}\n\\displaystyle f(y | \\mu, \\kappa) =\n  \\frac{\\Gamma(\\kappa)}{\\Gamma(\\mu\\kappa)\\Gamma((1-\\mu)\\kappa)}\n  y^{\\mu\\kappa -1}\n  y^{(1 - \\mu)\\kappa -1}, & 0 < y < 1\n\\end{array}\n\\]\nwith \\(0 < \\mu < 1\\) and \\(\\kappa > 0\\).\nIf we use the same notation than for the robust linear regression, the beta regression model is defined as\n\\[\ny_i \\sim \\text{Beta}(g^{-1}(\\mu_i), \\kappa)\n\\]\nwhere \\(\\mu_i = \\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_p x_{p,i}\\), \\(\\kappa\\) is the precision parameter and \\(g\\) is a twice differentiable, strictly increasing, link function.\nBambi uses again the following priors by default:\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal}(\\mu_{\\beta_0}, \\sigma_{\\beta_0}) \\\\\n\\beta_j \\sim \\text{Normal}(\\mu_{\\beta_j}, \\sigma_{\\beta_j})  \\\\\n\\kappa \\sim \\text{HalfCauchy(1)}\n\\end{array}\n\\]\nwhere the \\(\\mu_{\\beta_j}\\) and \\(\\sigma_{\\beta_j}\\) are estimated from the data. By default, \\(g\\) is the logit function. Other options available are the identity, the probit, and the cloglog link functions.\nIt’s possible to resume all of this in a very simplistic way by seeing that the beta regression as a very close relative of the GLM family. This model presents all the characteristics of GLMs, with the exception that the beta distribution doesn’t belong to the exponential family.\n\nModel specification and fit\nHere we are going to use the GasolineYield dataset from the betareg R package. This dataset is about the proportion of crude oil converted to gasoline. The response variable is the proportion of crude oil after distillation and fractionation. In this example, we use the temperature at which gasoline has vaporized in Fahrenheit degrees (\"temp\") and a factor that indicates ten unique combinations of gravity, pressure and temperature (\"batch\").\nThe following is just a re-ordering of the categories in the \"batch\" variable so it matches the original contrasts used in the betareg package.\n\ndata = pd.read_csv(\"data/gasoline.csv\")\ndata[\"batch\"] = pd.Categorical(\n  data[\"batch\"], \n  [10, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n  ordered=True\n)\n\nNext, we define the model. The only difference is that we indicate family=\"beta\". Bambi handles all the rest for us.\n\n# Note this model does not include an intercept\nmodel = bmb.Model(\"yield ~ 0 + temp + batch\", data, family=\"beta\")\nmodel\n\nFormula: yield ~ 0 + temp + batch\nFamily name: Beta\nLink: logit\nObservations: 32\nPriors:\n  Common-level effects\n    temp ~ Normal(mu: 0.0, sigma: 0.0364)\n    batch ~ Normal(mu: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], sigma: [ 8.5769  7.5593  8.5769  8.5769  7.5593  8.5769  8.5769  7.5593  8.5769\n 10.328 ])\n\n  Auxiliary parameters\n    kappa ~ HalfCauchy(beta: 1)\n\n\nAnd model.fit() is all we need to ask the sampler to start running.\n\nidata = model.fit(draws=2000, target_accept=0.95)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [yield_kappa, batch, temp]\n\n\n\n\n\n\n\n    \n      \n      100.00% [6000/6000 00:19<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 20 seconds.\nThe number of effective samples is smaller than 10% for some parameters.\n\n\n\n\nExplore results\nOnce we got the posterior, we explore it. This time we’re going to plot highest density intervals for the marginal posteriors corresponding to the parameters in the model.\n\nsummary = az.summary(idata, kind=\"stats\")\nsummary\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      temp\n      0.011\n      0.001\n      0.010\n      0.012\n    \n    \n      batch[10]\n      -6.111\n      0.244\n      -6.586\n      -5.656\n    \n    \n      batch[1]\n      -4.388\n      0.201\n      -4.766\n      -4.010\n    \n    \n      batch[2]\n      -4.799\n      0.191\n      -5.167\n      -4.453\n    \n    \n      batch[3]\n      -4.551\n      0.185\n      -4.892\n      -4.207\n    \n    \n      batch[4]\n      -5.055\n      0.211\n      -5.442\n      -4.635\n    \n    \n      batch[5]\n      -4.980\n      0.213\n      -5.415\n      -4.591\n    \n    \n      batch[6]\n      -5.073\n      0.215\n      -5.468\n      -4.668\n    \n    \n      batch[7]\n      -5.571\n      0.212\n      -5.973\n      -5.170\n    \n    \n      batch[8]\n      -5.617\n      0.233\n      -6.030\n      -5.149\n    \n    \n      batch[9]\n      -5.729\n      0.250\n      -6.201\n      -5.264\n    \n    \n      yield_kappa\n      263.233\n      86.097\n      115.416\n      425.489\n    \n  \n\n\n\n\n\nsummary[\"row\"] = list(range(12))\nsummary[\"panel\"] = [\"1-Temperature\"] + [\"2-Batch\"] * 10 + [\"3-Precision\"]\n\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 5.33), sharey=True, dpi=200)\nfig.subplots_adjust(left=0.12, right=0.975, wspace=0.1, bottom=0.12, top=0.925)\nfig.set_facecolor(\"w\")\n\nfor i, (ax, panel) in enumerate(zip(axes, [\"1-Temperature\", \"2-Batch\", \"3-Precision\"])):\n    plt_data = summary[summary[\"panel\"] == panel]\n    ax.scatter(plt_data[\"mean\"], plt_data[\"row\"], s=80)\n    ax.hlines(plt_data[\"row\"], plt_data[\"hdi_3%\"], plt_data[\"hdi_97%\"], lw=3)\n    ax.set_title(panel)\n    ax.tick_params(\"y\", length=0)\n\nax.set_yticks(range(len(summary.index)))\nax.set_yticklabels(list(summary.index))\n\nfig.text(0.5, 0.025, \"Marginal posterior\", size=12, ha=\"center\")\nfig.text(0.02, 0.5, \"Parameter\", size=12, va=\"center\", rotation=90)\n\nfig.savefig(\"imgs/plot.png\", dpi=200)"
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html",
    "title": "How to generate bingo cards in R",
    "section": "",
    "text": "Hello wor… Well, my first hello world post appeared about a year ago, but this site had the same fate as many of my othe side-projects… abandonment.\nUntil now."
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html#introduction",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html#introduction",
    "title": "How to generate bingo cards in R",
    "section": "Introduction",
    "text": "Introduction\nToday I’m going to show you how I came up with “an algorithm” to generate random bingo cards and some utility functions to print them on a nice looking (?) .pdf file.\nFirst of all, what type of bingo card I’m referring to? As an Argentine, the only bingo cards I’ve ever heard of are bingo cards like this one\n\n\n\n\nExample bingo card from bingo.es\n\n\n\nIt contains fifteen numbers from 1 to 90 that are divided in three rows and nine columns. The first column contains numbers between 1 and 9, the second column numbers between 10 and 20, and so on until the last column that contains numbers between 80 and 90. The type of bingo that you play with this bingo card is known as the 90-ball bingo game or British bingo. As I said, this is the only version I knew before this project 1 and I think it is the only bingo version you’ll find here in Argentina (I also bet you’ll find some fellow Argentine confirming this a national invention).\nSo, if you entered this post thinking you’ll find how to print those bingo cards that are popular in places like United States, I’m sorry, this is not for you 2. Fortunately, other people have invented a tool for you even before I wondered how to generate bingo cards. If you are interested, have a look at this package and the Shiny app introduced there.\nNow, let’s go back to our business.\nAnyone who has gone to one of those events where people gather to play bingo 3 knows that bingo cards don’t usually come separated in individual pieces of paper. Sellers usually have strips of six bingo cards in their hands. In some events, you can buy bingo cards directly. In others, you have to buy the entire strip.\nSince this is a 90-ball bingo game and each card contains fifteen numbers, six bingo cards with no repeated numbers is all we need to have all the numbers of the game in a single strip. You see where it is going?. Yes, we won’t generate isolated cards, we’ll generate entire strips. This is how a bingo strip looks like (just imagine them vertically stacked on a single strip)\n\n\n\n\nExample bingo strip from bingo.es"
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html#valid-cards-and-valid-strips",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html#valid-cards-and-valid-strips",
    "title": "How to generate bingo cards in R",
    "section": "Valid cards and valid strips",
    "text": "Valid cards and valid strips\nBingo cards are not just a bunch of numbers thrown at a piece of paper. All valid strips are composed of six valid cards each made of three valid rows. But not any combinations of three valid rows make up a valid card nor any combinations of six valid cards make up a valid strip. What a shame!\nBut what is a valid row, a valid card, a va… whatever. Let’s just get to the point and list the rules that will govern how we generate bingo cards.\n\nValid row\nWe’re going to think that a row is a numeric vector of length nine where some elements are empty and some are filled with numbers.\n\nExactly five elements are numbers, and four are empty.\nThere can’t be more than two consecutive empty elements, which is equivalent to having at most three consecutive numbers.\n\nExample valid rows\n\n \n\nExample invalid rows\n\n \n\n\n\nValid card\nWe can think that a bingo card is a matrix of three rows and nine columns. Each row must be a valid row as specified in the previous point, plus\n\nNo column can be completely empty.\nNo column can be completely filled with numbers.\nNumbers are sorted in ascending order within columns.\n\nExample valid card\n\n\n\n\n\nValid strip\nA valid strip contains six valid cards that satisfy the following conditions\n\nThe first column must have nine numbers and nine empty slots.\nColumns 2 to 8 must have ten numbers and eight empty slots.\nColumn 9 must have eleven numbers and seven empty slots.\n\nIn total, we have \\(6\\times3\\times9 = 162\\) slots in a strip. 90 of them are filled with numbers, 72 are not."
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html#sample-this-sample-that-ive-got-no-need-to-compute-them-all",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html#sample-this-sample-that-ive-got-no-need-to-compute-them-all",
    "title": "How to generate bingo cards in R",
    "section": "Sample this, sample that, I’ve got no need to compute them all4",
    "text": "Sample this, sample that, I’ve got no need to compute them all4\nOne approach to generate bingo cards would be to get all possible combinations of row layouts, bingo layouts, number arrangements, etc. But the number of cards you could generate is huge and the task wouldn’t be easy at all.\nThe approach used here is one that mixes some simple combinatorics and random sampling. We use permutations to compute all the possible row layouts. Then, we sample rows to create cards and sample cards to create strips5.\nFirst of all, we are going to find valid layouts (i.e. the skeleton of our bingo strips). Once we have them, we are going to fill them with numbers.\n\nFinding valid rows\nIf we represent empty slots with a 0 and filled slots with a 1, getting all permutations between four 0s and five 1s is as simple as calling combinat::permn(c(rep(0, 4), rep(1, 5))). However, this is not what we want because not all the returned layouts are valid rows. We need to select only those row layouts that are valid in a bingo card.\nThe following function, find_window(), receives a numeric vector x and looks for find windows of length width where all the elements are equal to what. If such a window is found, the function returns TRUE, otherwise it returns FALSE.\n\nfind_window <- function(x, width, what) {\n    for (i in 1:(length(x) - width)) {\n        if (all(x[i:(i + width)] == what)) {\n            return(TRUE)\n        }\n    }\n    return(FALSE)\n}\n\nThen we write a function called get_rows() that generates all the possible row layouts and uses find_window() to select the layouts that satisfy our conditions.\n\nget_rows <- function() {\n    # Get all row layouts\n    rows <- combinat::permn(c(rep(0, 4), rep(1, 5)))\n    # Keep rows with at most two consecutive empty slots\n    rows <- rows[!vapply(rows, find_window, logical(1), 2, 0)]\n    # Keep rows with at most three consecutive filled slots\n    rows <- rows[!vapply(rows, find_window, logical(1), 3, 1)]\n    return(rows)\n}\n\n\n\nSampling valid cards\nWe noted that a valid card is made of three valid rows, but not all combinations of three valid rows make up a valid card. What if we sample three row layouts and keep/discard the combination based on whether they make up a valid card or not? We can repeat this until we have some desired number of card layours. The process is as follows\n\nLet \\(N\\) be the number of cards we want to generate.\nWhile the number of cards generated is smaller than \\(N\\), do:\n\nSample three rows and make up the card.\nCount the number of filled slots per column.\nIf all the counts are between 1 and 3, keep the card, else discard it.\n\n\nOnce we’re done, we end up with \\(N\\) bingo card layouts that are valid in terms of our requirements above.\nThis idea is implemented in a function called get_cards(). It receives the rows we generate with get_rows() and the number of card layouts we want to generate. Finally it returns a list whose elements are vectors of length 3 with the row indexes6.\n\nget_cards <- function(rows, cards_n = 2000) {\n    rows_n <- length(rows)\n    cards <- vector(\"list\", cards_n)\n\n    attempts <- 0\n    card_idx <- 0\n\n    while (card_idx < cards_n) {\n        attempts <- attempts + 1\n        # Sample three rows\n        row_idxs <- sample(rows_n, 3)\n        mm <- matrix(unlist(rows[row_idxs]), ncol = 9, byrow = TRUE)\n        col_sums <- colSums(mm)\n\n        # Select valid cards.\n        # These have between 1 and 3 numbers per column.\n        if (all(col_sums != 0) && all(col_sums != 3)) {\n            card_idx <- card_idx + 1\n            cards[[card_idx]] <- list(row_idxs, col_sums)\n        }\n        # Print message every 1000 attempts\n        if (attempts %% 1000 == 0) {\n            message(\"Attempt \", attempts, \" | Cards built:\", card_idx, \"\\n\")\n        }\n    }\n    # Check duplicates\n    dups <- duplicated(lapply(cards, `[[`, 1))\n    message(\"There are \", sum(dups), \" duplicated cards.\")\n    return(cards)\n}\n\n\n\nSampling valid strips\nThis is the much like what we did above, with two differences. Instead of sampling three row layouts, we sample six card layouts. Instead of checking if the number of filled slots per column are between 1 and 3, we check if they match a number between 9 and 11 specific to each of them.\nThen, we have get_strips(). It receives a list called cards where each element contains the three row indexes corresponding to each card layout. rows is a list of row layouts and strips_n controls how many strip layouts we want to generate.\n\nget_strips <- function(cards, rows, strips_n = 100) {\n    valid_counts <- c(9, rep(10, 7), 11)\n    cards_n <- length(cards)\n    strips <- vector(\"list\", strips_n)\n\n    attempts <- 0\n    strip_idx <- 0\n\n    while (strip_idx < strips_n) {\n        attempts <- attempts + 1\n\n        # Sample 6 cards\n        cards_idxs <- sample(cards_n, 6)\n        strip <- cards[cards_idxs]\n\n        # Contains column counts by card\n        card_counts <- matrix(\n            unlist(lapply(strip, `[[`, 2)),\n            ncol = 9, byrow = TRUE\n        )\n\n        # Check if strip column counts are valid\n        if (all(colSums(card_counts) == valid_counts)) {\n            strip_idx <- strip_idx + 1\n            # Get row indexes contained in the selected card indexes\n            rows_idxs <- unlist(lapply(cards[cards_idxs], `[[`, 1))\n            strips[[strip_idx]] <- matrix(\n                unlist(rows[rows_idxs]),\n                ncol = 9, byrow = TRUE\n            )\n        }\n        # Print message every 1000 attempts\n        if (attempts %% 1000 == 0) {\n            message(\"Attempt \", attempts, \" | Strips built:\", strip_idx, \"\\n\")\n        }\n    }\n    dups <- duplicated(strips)\n    message(\"There are \", sum(dups), \" duplicatd layouts.\\n\")\n    return(strips)\n}\n\n\n\nA last but not least step\nI’ve never seen a bingo game where you are given empty layouts and are asked to put numbers yourself. So let’s wrap this up and fill our empty cards!\nfill_strips() receives the strip layouts we generated, randomly selects n of them, and, also randomly, fills the slots the cards with numbers. Of course, the first column contains numbers from 1 to 9, the second column contains numbers from 10 to 19… and so on until the last column, that has numbers from 80 to 90.\n\nfill_strips <- function(strips, n = 100) {\n    # Numbers that go in each column\n    numbers <- list(1:9, 10:19, 20:29, 30:39, 40:49, 50:59, 60:69, 70:79, 80:90)\n    # Row indexes corresponding to each card in the strip\n    card_rows <- list(1:3, 4:6, 7:9, 10:12, 13:15, 16:18)\n\n    fill_strip <- function(strip) {\n        # Put numbers in the slots with a 1 (meaning they must contain a number)\n        strip[strip == 1] <- unlist(\n            # This `sample()` reorders the numbers in each column randomly\n            mapply(sample, numbers, sapply(numbers, length))\n        )\n\n        for (i in seq_along(card_rows)) {\n            strip_ <- strip[card_rows[[i]], ]\n            # Numbers in a given column are sorted in ascending order within cards\n            x <- sort(strip_)\n            strip_[strip_ != 0] <- x[x != 0]\n            strip[card_rows[[i]], ] <- strip_\n        }\n        return(strip)\n    }\n    # Strip layouts can be repeated\n    strips <- lapply(sample(strips, n, replace = TRUE), fill_strip)\n    message(\"There are \", sum(duplicated(strips)), \" duplicated strips.\\n\")\n    return(strips)\n}\n\nAnd we finally get our bingo strips :)\n\nset.seed(0303456)\nrows <- get_rows()\ncards <- get_cards(rows, 1000)\nstrips <- get_strips(cards, rows, 20)\nstrips <- fill_strips(strips, 50)\n# Output messages have been suppressed\n\nLet’s check some of them\n\nstrips[[1]]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n [1,]    0   11   20    0   48    0    0   74   80\n [2,]    8    0    0   31    0   51   60   78    0\n [3,]    0   19   27   39    0   54   62    0    0\n [4,]    1    0   26    0   42   55    0    0   84\n [5,]    2   14    0   34    0    0   65   77    0\n [6,]    0   17   29    0   43   59    0    0   89\n [7,]    0    0   22   33    0    0   64   75   88\n [8,]    0   15    0   35   45    0    0   79   90\n [9,]    9    0   25    0   49   50   66    0    0\n[10,]    3    0   28   30    0    0   61   71    0\n[11,]    7    0    0   36   40   58    0    0   81\n[12,]    0   10    0    0   44    0   63   76   87\n[13,]    0    0   21   37    0   52   68   70    0\n[14,]    5   16    0    0   41    0    0   72   82\n[15,]    0   18    0   38   47   57    0    0   86\n[16,]    0    0   23    0   46   53    0   73   83\n[17,]    4   12    0   32    0    0   67    0   85\n[18,]    6   13   24    0    0   56   69    0    0\n\n\n\nstrips[[30]]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n [1,]    0    0   25    0   43   50    0   74   80\n [2,]    0   16   26   34    0    0   65   79    0\n [3,]    6   17    0   38    0   58    0    0   86\n [4,]    3    0   27    0   40   51   61    0    0\n [5,]    4    0    0   32   49   59    0    0   81\n [6,]    0   19   29   35    0    0   68   71    0\n [7,]    1   14    0    0   47    0   60   75    0\n [8,]    2    0   20   31    0    0   66    0   83\n [9,]    0    0   24    0   48   55    0   77   89\n[10,]    0    0   28   33   42    0   64   76    0\n[11,]    5   12    0   39    0    0   67    0   84\n[12,]    9   15    0    0   45   54    0    0   87\n[13,]    0   13   21    0    0   52    0   73   85\n[14,]    0   18   22    0   44    0   63   78    0\n[15,]    8    0    0   37   46   56    0    0   90\n[16,]    0    0   23   30    0   53   62    0   82\n[17,]    7   10    0   36    0    0   69   70    0\n[18,]    0   11    0    0   41   57    0   72   88"
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html#are-we-going-to-play-on-r-consoles",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html#are-we-going-to-play-on-r-consoles",
    "title": "How to generate bingo cards in R",
    "section": "Are we going to play on R consoles?",
    "text": "Are we going to play on R consoles?\nAll we got so far are matrices that look like a bingo strip. But honestly, without any given context, they just look like a bunch of matrices of the same dimension filled with 0s and other integer numbers. Our last task is to generate a .pdf output where these matrices really look like bingo cards.\nIn this last part of the post we make use of the grid package. For those who haven’t heard of it, it is the low level plotting library behind ggplot2, for example.\nHere we have a little function, make_grid(), that given a number of rows and columns returns the natural parent coordinates of the borders the grid that defines the rectangles within each card.\n\nmake_grid <- function(rows, cols) {\n    lines_rows <- grid::unit((0:rows) / rows, \"npc\")\n    lines_cols <- grid::unit((0:cols) / cols, \"npc\")\n    return(list(\"row\" = lines_rows, \"col\" = lines_cols))\n}\n\nAnd now we have the main function used to plot the bingo strips. Since the function is quite large, I prefer to explain how it works with comments in the body.\n\nplot_strips <- function(strips, col = \"#8e44ad\", width_row = 0.925,\n                        width_col = 0.975) {\n\n    # `rows` and `cols` are the dimensions of each card\n    rows <- 3\n    cols <- 9\n    g <- make_grid(rows, cols)\n    # Compute the center of each square in the card grid\n    centers_rows <- g$row[-1] - grid::unit(1 / (rows * 2), \"npc\")\n    centers_cols <- g$col[-1] - grid::unit(1 / (cols * 2), \"npc\")\n    # Sort the centers appropiately\n    # This is required because of how we loop over the values in each card\n    x_coords <- rep(centers_cols, each = rows)\n    y_coords <- rep(rev(centers_rows), cols)\n\n    # Create unique identifiers for the cards\n    cards_n <- paste(\n        paste0(\"CARD N\", intToUtf8(176)),\n        seq_len(length(strips) * 6)\n    )\n    # Compute the number of sheets we're going to need.\n    # Each sheet contains two strips\n    sheets_n <- ceiling(length(strips) / 2)\n\n    # Initial numbers\n    card_idx <- 0\n    strip_idx <- 0\n\n    # Loop over sheets\n    for (sheet_idx in seq_len(sheets_n)) {\n        # Each sheet is a grid of 6 rows and 3 columns.\n        # Columns 1 and 3 are where we place the strips.\n        # Column 2 just gives vertical separation.\n        l <- grid::grid.layout(\n            nrow = 6, ncol = 3,\n            widths = c(48.75, 2.5 + 3.75, 48.75)\n        )\n        # Start a new page filled with white\n        grid::grid.newpage()\n        grid::grid.rect(gp = grid::gpar(col = NULL, fill = \"white\"))\n\n        vp_mid <- grid::viewport(0.5, 0.5, width_row, width_col, layout = l)\n        grid::pushViewport(vp_mid)\n\n        # Loop over columns 1 and 3\n        for (j in c(1, 3)) {\n            # Select strip\n            strip_idx <- strip_idx + 1\n            if (strip_idx > length(strips)) break\n            strip <- strips[[strip_idx]]\n\n            # Loop over rows (these rows represent the 6 rows assigned to cards)\n            for (i in 1L:l$nrow) {\n                card_idx <- card_idx + 1\n                vp_inner <- grid::viewport(layout.pos.row = i, layout.pos.col = j)\n                grid::pushViewport(vp_inner)\n\n                # Add card identification number on top-left\n                grid::grid.text(\n                    label = cards_n[card_idx],\n                    x = 0,\n                    y = 0.96,\n                    just = \"left\",\n                    gp = grid::gpar(fontsize = 9)\n                )\n\n                # Draw a grill that separates the slots in the card\n                vp_mid_inner <- grid::viewport(0.5, 0.5, 1, 0.80)\n                grid::pushViewport(vp_mid_inner)\n                grid::grid.grill(h = g$row, v = g$col, gp = grid::gpar(col = col))\n\n                # Select the numbers that correspond to this card\n                numbers <- as.vector(strip[(3 * i - 2):(3 * i), ])\n                # Logical vector that indicates which rectangles are filled\n                # with nunumbers and which rectangles are empty\n                lgl <- ifelse(numbers == 0, FALSE, TRUE)\n\n                # Draw the numbers in positions given by the rectangle centers\n                grid::grid.text(\n                    label = numbers[lgl],\n                    x = x_coords[lgl],\n                    y = y_coords[lgl],\n                    gp = grid::gpar(fontsize = 18)\n                )\n\n                # Fill empty slots with color\n                grid::grid.rect(\n                    x = x_coords[!lgl],\n                    y = y_coords[!lgl],\n                    height = grid::unit(1 / rows, \"npc\"),\n                    width = grid::unit(1 / cols, \"npc\"),\n                    gp = grid::gpar(\n                        col = NA,\n                        fill = farver::encode_colour(farver::decode_colour(col), 0.7)\n                    )\n                )\n                # End\n                grid::popViewport()\n                grid::popViewport()\n            }\n        }\n        grid::popViewport()\n    }\n}\n\nNow, all we need is to pass the strips generated above to plot_strips() and wrap that call within grDevices::pdf() and grDevices::dev.off().\n\n# Height and width are in inches and here they correspond to legal paper size\ngrDevices::pdf(\"imgs/strips.pdf\", height = 14, width = 8.5)\nplot_strips(strips)\ngrDevices::dev.off()\n\nIf it works, you’ll have a 25 pages pdf with bingo cards that look like this one\n\n\n\n\nFirst card in the output\n\n\n\nIf you can’t (or just don’t want to) run the code, here you have the generated pdf."
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html",
    "href": "posts/2021-05-24_why-bambi/index.html",
    "title": "Why Bambi?",
    "section": "",
    "text": "I’ve been thinking about writing a new blog post for a while now but honestly, there was nothing coming to my mind that made me think “Oh, yeah, this is interesting, it can be useful for someone else”. And it was just a few hours ago that I realized I could write about something quite curious that happened to me while trying to replicate a Bambi model with PyMC3.\nPyMC3 is a Python package for Bayesian statistical modeling that implements advanced Markov chain Monte Carlo algorithms, such as the No-U-Turn sampler (NUTS). Bambi is a high-level Bayesian model-building interface in Python. It is built on top of PyMC3 and allows users to specify and fit Generalized Linear Models (GLMs) and Generalized Linear Mixed Models (GLMMs) very easily using a model formula much similar to the popular model formulas in R.\nA couple of weeks ago Agustina Arroyuelo told me she was trying to replicate a model in one of the example notebooks we have in Bambi and wanted my opinion on what she was doing. After many attempts, neither of us could replicate the model successfully. It turned out to be we were messing up with the shapes of the priors and also had some troubles with the design matrix.\nThe point of this post is not about good practices when doing Bayesian modeling neither about modeling techniques. This post aims to show how Bambi can save you effort, code, and prevent us from making some mistakes when fitting not-so-trivial GLMs in Python.\nWell, I think this is quite enough for an introduction. Let’s better have a look at the problem at hand.\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport theano.tensor as tt"
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#the-problem",
    "href": "posts/2021-05-24_why-bambi/index.html#the-problem",
    "title": "Why Bambi?",
    "section": "The problem",
    "text": "The problem\nIn this problem we use a data set consisting of 67856 insurance policies and 4624 (6.8%) claims in Australia between 2004 and 2005. The original source of this dataset is the book Generalized Linear Models for Insurance Data by Piet de Jong and Gillian Z. Heller.\n\nurl = \"https://courses.ms.ut.ee/2020/glm/spring/uploads/Main/carclaims.csv\"\ndata = pd.read_csv(url)\ndata = data[data[\"claimcst0\"] > 0]\n\nThe age (binned), the gender, and the area of residence are used to predict the amount of the claim, conditional on the existence of the claim because we are only working with observations where there is a claim.\nWe use a Wald regression model. This is a GLM where the random component follows a Wald distribution. The link function we choose is the natural logarithm."
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#pymc3-model",
    "href": "posts/2021-05-24_why-bambi/index.html#pymc3-model",
    "title": "Why Bambi?",
    "section": "PyMC3 model",
    "text": "PyMC3 model\n\nData preparation\nTo fit the model with PyMC3 we first need to create the model matrix. We need to represent age, area, and gender with dummy variables because they are categorical. We can think of the following objects as sub-matrices of the design matrix in the model.\n\nintercept = np.ones((len(data), 1))\nage = pd.get_dummies(data[\"agecat\"], drop_first=True).to_numpy()\narea = pd.get_dummies(data[\"area\"], drop_first=True).to_numpy()\ngender = pd.get_dummies(data[\"gender\"], drop_first=True).to_numpy()\n\nNote we have used drop_first=True. This means that we use n_levels - 1 dummies to represent each categorical variable, and the first level is taken as reference. This ensures the resulting design matrix is of full rank.\nNext, we stack these sub-matrices horizontally and convert the result to a Theano tensor variable so we can compute the dot product between this matrix and the vector of coefficients when writing our model in PyMC3.\n\nX = np.hstack([intercept, age, gender, area])\nX = tt.as_tensor_variable(X)\n\n\n\nFit\nWe start declaring the priors for each of the predictors in the model. They are all independent Gaussian distributions. You may wonder where I took the values for the parameters of these distributions. I’ve just copied Bambi’s default values for this particular problem.\nAt this stage, it is very important to give appropriate shapes to all the objects we create in the model. For example, β_age is a random variable that represents the coefficients for the age variable. Since 5 dummy variables are used to represent the age, both β_age and the values passed to mu and sigma must have shape=(5, 1). I’ve failed here many times when trying to replicate the model, so, unfortunately, I know what I’m talking about 😅\n\n# Create model and sample posterior\nwith pm.Model() as model_pymc3:\n    # Build predictors\n    β_0 = pm.Normal(\"β_0\", mu=0, sigma=5, shape=1)\n    β_gender = pm.Normal(\"β_gender\", mu=0, sigma=5, shape=1)\n    β_age = pm.Normal(\n        \"β_age\",\n        mu=np.array([0] * 5),\n        sigma=np.array([0.32, 6.94, 1.13, 5.44, 9.01]),\n        shape=5\n    )\n    β_area = pm.Normal(\n      \"β_area\",\n      mu=np.array([0] * 5),\n      sigma=np.array([0.86, 0.25, 1.3, 0.76, 5.33]),\n      shape=5\n    )\n    \n    # Concatenate the vectors for the coefficients into a single vector\n    β = tt.concatenate([β_0, β_age, β_gender, β_area], axis=0)\n    \n    # Compute and transform linear predictor\n    mu = tt.exp(X.dot(β))\n      \n    lam = pm.HalfCauchy(\"claim_lam\", beta=1)\n    pm.Wald(\"claim\", mu=mu, lam=lam, observed=data[\"claimcst0\"])\n    \n    idata_pymc = pm.sample( \n      draws=2000, \n      target_accept=0.9, \n      random_seed=1234,\n      return_inferencedata=True\n    )\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [claim_lam, β_area, β_gender, β_age, β_0]\n\n\n\n\n\n\n\n    \n      \n      100.00% [6000/6000 00:25<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 26 seconds."
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#bambi-model",
    "href": "posts/2021-05-24_why-bambi/index.html#bambi-model",
    "title": "Why Bambi?",
    "section": "Bambi model",
    "text": "Bambi model\nAs you can see below, we don’t need to do any data preparation, or even specify priors by hand. Bambi automatically obtains sensible default priors when they are not specified, and also knows how to handle each variable type very well.\nThe model is specified using a model formula, quite similar to model formulas in R. The left-hand side of ~ is the response variable, and the rest are the predictors. Here C(agecat) tells Bambi that agecat should be interpreted as categorical. The family argument indicates the conditional distribution for the response, and the link tells Bambi which function of the mean is being modeled by the linear predictor. More information about how they work can be found here.\nThen we have the .fit() method, where you can pass arguments to the pm.sample() function that’s running in the background.\n\nmodel_bambi = bmb.Model(\n  \"claimcst0 ~ C(agecat) + gender + area\", \n  data, \n  family = \"wald\", \n  link = \"log\"\n)\nidata_bambi = model_bambi.fit(draws=2000, target_accept=0.9, random_seed=1234)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [claimcst0_lam, area, gender, C(agecat), Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [6000/6000 00:18<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 19 seconds.\n\n\nAnd that’s it! A model that took several lines of codes to specify in PyMC3 only took a few lines of code in Bambi. Quite an advantage, right?"
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#check-results",
    "href": "posts/2021-05-24_why-bambi/index.html#check-results",
    "title": "Why Bambi?",
    "section": "Check results",
    "text": "Check results\nThe simplicity we gain with Bambi would be worthless if the results turned out to be different. We want an interface that makes our job easier, without affecting the quality of the inference. The following is a forest plot where the point gives the posterior mean and the bars indicate a 94% HDI.\n\nsummary_pymc = az.summary(idata_pymc)\nsummary_bambi = az.summary(idata_bambi)\n\n\nsummary_pymc[\"row\"] = list(range(13))\nsummary_pymc[\"panel\"] = [\"1-Intercept\"] + [\"2-Effects\"] * 11 + [\"3-Dispersion\"]\n\nsummary_bambi[\"row\"] = list(range(13))\nsummary_bambi[\"panel\"] = [\"1-Intercept\"] + [\"2-Effects\"] * 11 + [\"3-Dispersion\"]\n\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 5.33), sharey=True, dpi=120)\nfig.subplots_adjust(left=0.13, right=0.975, wspace=0.1, bottom=0.12, top=0.925)\nfig.set_facecolor(\"w\")\n\nfor i, (ax, panel) in enumerate(zip(axes, [\"1-Intercept\", \"2-Effects\", \"3-Dispersion\"])):\n    plt_data = summary_bambi[summary_bambi[\"panel\"] == panel]\n    ax.scatter(plt_data[\"mean\"], plt_data[\"row\"] - 0.25, s=40, label=\"Bambi\")\n    ax.hlines(plt_data[\"row\"] - 0.25, plt_data[\"hdi_3%\"], plt_data[\"hdi_97%\"], lw=2)\n    \n    plt_data = summary_pymc[summary_pymc[\"panel\"] == panel]\n    ax.scatter(plt_data[\"mean\"], plt_data[\"row\"] + 0.25, s=40, label=\"PyMC3\")\n    ax.hlines(plt_data[\"row\"] + 0.25, plt_data[\"hdi_3%\"], plt_data[\"hdi_97%\"], lw=2, color=\"C1\")\n    \n    ax.set_title(panel)\n    ax.tick_params(\"y\", length=0)\n\naxes[0].legend()\nax.set_yticks(range(len(summary_bambi.index)))\nax.set_yticklabels(list(summary_bambi.index))\n\nfig.text(0.5, 0.025, \"Marginal posterior\", size=12, ha=\"center\")\nfig.text(0.02, 0.5, \"Parameter\", size=12, va=\"center\", rotation=90)\n\nfig.savefig(\"imgs/plot.png\", dpi=120)\n\n\n\n\nWhile most of the marginal posteriors match very well, we can clearly see the ones for β_area[3] and β_area[4] don’t overlap as much as the others. One of the possible explanations for this difference is related to the MCMC algorithm. While we know both models are indeed the same model, their internal representation is not exactly the same. For example, the model we wrote in pure PyMC3 computes a unique dot product between a matrix of shape (n, p) a vector of shape (p, 1), while the model in Bambi is computing the sum of many smaller dot products. As the internal representations are not exactly the same, the sampling spaces differ and the sampling algorithm obtained slightly different results."
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#conclusion",
    "href": "posts/2021-05-24_why-bambi/index.html#conclusion",
    "title": "Why Bambi?",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we saw how the same GLM can be expressed in both PyMC3 and Bambi. PyMC3 allowed us to control every fine-grained detail of the model specification, while Bambi allowed us to express the same model in a much more concise manner.\nBambi’s advantages in these types of scenarios aren’t only related to the amount of code one has to write. Bambi also prevents us from making mistakes when writing the PyMC3 model, such as the mistakes I was making when specifying the shapes for the distributions. Or one could just simply don’t know how correctly prepare the data that should go in the design matrix, such as the conversion of the categorical data to numeric matrices in such a way that the information is retained without introducing structural redundancies.\nNevertheless, this doesn’t mean we should always favor Bambi over PyMC3. Whether Bambi or PyMC3 is appropriate for you actually depends on your use case. If you’re someone who mainly needs to fit GLMs or GLMMs, Bambi is the way to go and it would be nice you give it a chance. There are a bunch of examples showing how to specify and fit different GLMs with Bambi. On the other hand, if you’re someone who writes a lot of custom models, PyMC3 will be your best friend when it comes to working with Bayesian models in Python.\nBambi is a community project and welcomes contributions such as bug fixes, examples, issues related to bugs or desired enhancements, etc. Want to know more? Visit the official docs or explore the Github repo. Also, if you have any doubts about whether the feature you want is available or going to be developed, feel free to reach out to us! You can always open a new issue to request a feature or leave feedback about the library, and we welcome them a lot 😁."
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#acknowledgments",
    "href": "posts/2021-05-24_why-bambi/index.html#acknowledgments",
    "title": "Why Bambi?",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI want to thank Agustina, Ravin, and Osvaldo for very useful comments and feedback on an earlier version of this post. They helped me to make this post much nicer than what it was originally."
  },
  {
    "objectID": "posts/2021-06-28_first-weeks-of-gsoc/index.html",
    "href": "posts/2021-06-28_first-weeks-of-gsoc/index.html",
    "title": "First weeks of GSoC",
    "section": "",
    "text": "I am really happy to participate in this Google Summer of Code season with NumFOCUS to contribute to the Bambi library. The coding period ranges from June 7 to August 16, with an intermediate evaluation taking place between July 12 and July 16."
  },
  {
    "objectID": "posts/2021-06-28_first-weeks-of-gsoc/index.html#overview",
    "href": "posts/2021-06-28_first-weeks-of-gsoc/index.html#overview",
    "title": "First weeks of GSoC",
    "section": "Overview",
    "text": "Overview\nMy project is called Extend available models and default priors in Bambi. The main goal of this project is to add new families of generalized linear models, such as beta regression, robust linear regression (i.e. linear model with error following a T-Student distribution)1 as well as multinomial regression. However, this raises a second problem, which is about default priors distributions.\nDefault priors in Bambi are limited to the families implemented in the GLM module instatsmodels, which does not include the families mentioned above. For this reason, it is first necessary to incorporate alternative automatic priors so new families work without requiring the user to manually specify priors.\nTherefore, these first weeks of the coding period were centered around understanding how default priors work on other high-level modeling packages such as brms and rstanarm, how to translate their ideas into PyMC3 code, and finally how to implement everything within Bambi."
  },
  {
    "objectID": "posts/2021-06-28_first-weeks-of-gsoc/index.html#alternative-default-priors",
    "href": "posts/2021-06-28_first-weeks-of-gsoc/index.html#alternative-default-priors",
    "title": "First weeks of GSoC",
    "section": "Alternative default priors",
    "text": "Alternative default priors\nCurrently, Bambi uses maximum likelihood estimates in the construction of its default priors. There are two limitations associated with this approach. First, current default priors don’t exist whenever uniquely identifiable maximum likelihood estimates don’t exist (e.g. \\(p > n\\) or complete separation scenarios). Secondly, these estimates are obtained via the GLM module in statsmodels, which means default priors can only be obtained for families made available in statsmodels.\nBased on the available documentation and simulations I’ve done, I decided to implement alternative default priors that are much like the default priors in rstanarm. These priors aim to be weakly-informative in most scenarios and do not depend on maximum likelihood estimates. Their documentation is excellent and it was a great guide for my implementation.\nThis is the PR where I implement alternative default priors inspired on rstanarm default priors. In addition, I also implement LKJ prior for the correlation matrices of group-specific effects.\n\nHow to invoke alternative default priors\nThe Model() class has gained one new argument, automatic_priors, that can be equal to \"default\" to use Bambi’s default method, or \"rstanarm\" to use the alternative implementation2.\nmodel = bmb.Model(\"y ~ x + z\", data, automatic_priors=\"rstanarm\")\n\n\nHow to use LKJ priors for correlation matrices of group-specific effects\nGroup-specific effects can now have non-independent priors. Instead of using independent normal distributions, we can use a multivariate normal distribution whose correlation matrix has an LKJ prior distribution. This distribution depends on a parameter \\(\\eta > 0\\). If \\(\\eta=1\\), the LJK prior is jointly uniform over all correlation matrices of the same dimension. If \\(\\eta >1\\) increases, the mode of the distribution is the identity matrix. The larger the value of \\(\\eta\\) the more sharply peaked the density is at the identity matrix.\nModel has an argument priors_cor where we can pass a dictionary to indicate which groups are going to have a LKJ prior. The keys of the dictionary are the names of the groups, and the values are the values for \\(\\eta\\).\nIn the following model, we have a varying intercept and varying slope for the groups given by group. These varying effects have a multivariate normal prior whose covariance matrix depends on a correlation matrix that has a LKJ hyperprior with \\(\\eta=1\\).\nmodel = bmb.Model(\"y ~ x + (x|group)\", data, priors_cor={\"group\": 1})"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I’m Full Stack Data Scientist and a part-time Statistics PhD student.  I speak mainly Python and R, but I’m also experienced with other languages and tools as well.  On top of that, I’m an Open Source Software developer, mostly focusing on Bambi."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tomás Capretto",
    "section": "",
    "text": "Jun 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI describe how to use the LKJCholeskyCov and LKJCorr distributions to include correlated priors in Bayesian hierarchical modeling using PyMC.\n\n\n\n\n\n\n\n\n\nJun 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal post about Google Summer of Code 2021. This post sums up my contributions to the Bambi library during the ten weeks of this program.\n\n\n\n\n\n\n\n\n\nAug 17, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\nMy fourth post describing work done during GSoC 2021. On this occasion, I’m introducing the Binomial family. This new family is very useful to build models for binary data when each row in the data set contains the number of successes and the number of trials instead of the results of Bernoulli trials.\n\n\n\n\n\n\n\n\n\nAug 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\nIn this third post about my work during this Google Summer of Code I describe two families of models recently added. The first one, is the Student T family, used to make linear regressions more robust. The second, is the Beta family which can be used to model ratings and proportions.\n\n\n\n\n\n\n\n\n\nJul 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\nSecond post about this Google Summer of Code season. Today I show some of the problems associated with outliers in linear regression and demonstrate how one can implement a robust linear regression in Bambi.\n\n\n\n\n\n\n\n\n\nJul 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst post of a series about my contributions to Bambi in this Google Summer of Code season. This post highlights new features related to default priors and priors for group-specific effects.\n\n\n\n\n\n\n\n\n\nJun 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBambi uses the library formulae to automatically construct design matrices for both common and group-specific effects. This post compares design matrices for group-specific effects obtained with formulae for a variety of scenarios involving categorical variables with the ones obtained with the R package lme4.\n\n\n\n\n\n\n\n\n\nJun 8, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\nAn example comparing how to fit a GLM with Bambi and PyMC3. Here I attempt to highlight how Bambi can help us to write a Bayesian GLM in a concise manner, saving us from having to realize error-prone tasks that are sometimes necessary when directly working with PyMC3.\n\n\n\n\n\n\n\n\n\nMay 24, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\nA walkthrough the process of understanding how bingo cards are composed and a set of R functions that let us generate random bingo cards and print them in a nice looking .pdf output.\n\n\n\n\n\n\n\n\n\nNov 3, 2020\n\n\n\n\n\n\nNo matching items"
  }
]