[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I‚Äôm a Data Scientist at PyMC Labs.\nI create tools to make statistics easier for everybody.\nI write educational content and I teach.\nI enjoy sharing what I do on this website.\n\n    \n    \n  \n\n\n\nWhat I‚Äôm working on\n\n\n\n\n\n\n\nStatistical Consulting\n\n\nWe use statistics to help companies solve challenging business problems.\n\n\n Learn more about our work \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntuitive Bayes\n\n\nI‚Äôm co-developing interactive courses to teach Statistics intuitively.\n\n\n Sign up for the Advanced Regression course \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBambi: Bayesian Model Building Interface\n\n\nA Python library to make Bayesian modeling easier and more flexible.\n\n\n Check the repository"
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html",
    "href": "posts/2021-05-24_why-bambi/index.html",
    "title": "Why Bambi?",
    "section": "",
    "text": "I‚Äôve been thinking about writing a new blog post for a while now but honestly, there was nothing coming to my mind that made me think ‚ÄúOh, yeah, this is interesting, it can be useful for someone else‚Äù. And it was just a few hours ago that I realized I could write about something quite curious that happened to me while trying to replicate a Bambi model with PyMC3.\nPyMC3 is a Python package for Bayesian statistical modeling that implements advanced Markov chain Monte Carlo algorithms, such as the No-U-Turn sampler (NUTS). Bambi is a high-level Bayesian model-building interface in Python. It is built on top of PyMC3 and allows users to specify and fit Generalized Linear Models (GLMs) and Generalized Linear Mixed Models (GLMMs) very easily using a model formula much similar to the popular model formulas in R.\nA couple of weeks ago Agustina Arroyuelo told me she was trying to replicate a model in one of the example notebooks we have in Bambi and wanted my opinion on what she was doing. After many attempts, neither of us could replicate the model successfully. It turned out to be we were messing up with the shapes of the priors and also had some troubles with the design matrix.\nThe point of this post is not about good practices when doing Bayesian modeling neither about modeling techniques. This post aims to show how Bambi can save you effort, code, and prevent us from making some mistakes when fitting not-so-trivial GLMs in Python.\nWell, I think this is quite enough for an introduction. Let‚Äôs better have a look at the problem at hand.\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport theano.tensor as tt"
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#introduction",
    "href": "posts/2021-05-24_why-bambi/index.html#introduction",
    "title": "Why Bambi?",
    "section": "",
    "text": "I‚Äôve been thinking about writing a new blog post for a while now but honestly, there was nothing coming to my mind that made me think ‚ÄúOh, yeah, this is interesting, it can be useful for someone else‚Äù. And it was just a few hours ago that I realized I could write about something quite curious that happened to me while trying to replicate a Bambi model with PyMC3.\nPyMC3 is a Python package for Bayesian statistical modeling that implements advanced Markov chain Monte Carlo algorithms, such as the No-U-Turn sampler (NUTS). Bambi is a high-level Bayesian model-building interface in Python. It is built on top of PyMC3 and allows users to specify and fit Generalized Linear Models (GLMs) and Generalized Linear Mixed Models (GLMMs) very easily using a model formula much similar to the popular model formulas in R.\nA couple of weeks ago Agustina Arroyuelo told me she was trying to replicate a model in one of the example notebooks we have in Bambi and wanted my opinion on what she was doing. After many attempts, neither of us could replicate the model successfully. It turned out to be we were messing up with the shapes of the priors and also had some troubles with the design matrix.\nThe point of this post is not about good practices when doing Bayesian modeling neither about modeling techniques. This post aims to show how Bambi can save you effort, code, and prevent us from making some mistakes when fitting not-so-trivial GLMs in Python.\nWell, I think this is quite enough for an introduction. Let‚Äôs better have a look at the problem at hand.\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport theano.tensor as tt"
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#the-problem",
    "href": "posts/2021-05-24_why-bambi/index.html#the-problem",
    "title": "Why Bambi?",
    "section": "The problem",
    "text": "The problem\nIn this problem we use a data set consisting of 67856 insurance policies and 4624 (6.8%) claims in Australia between 2004 and 2005. The original source of this dataset is the book Generalized Linear Models for Insurance Data by Piet de Jong and Gillian Z. Heller.\n\nurl = \"https://courses.ms.ut.ee/2020/glm/spring/uploads/Main/carclaims.csv\"\ndata = pd.read_csv(url)\ndata = data[data[\"claimcst0\"] &gt; 0]\n\nThe age (binned), the gender, and the area of residence are used to predict the amount of the claim, conditional on the existence of the claim because we are only working with observations where there is a claim.\nWe use a Wald regression model. This is a GLM where the random component follows a Wald distribution. The link function we choose is the natural logarithm."
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#pymc3-model",
    "href": "posts/2021-05-24_why-bambi/index.html#pymc3-model",
    "title": "Why Bambi?",
    "section": "PyMC3 model",
    "text": "PyMC3 model\n\nData preparation\nTo fit the model with PyMC3 we first need to create the model matrix. We need to represent age, area, and gender with dummy variables because they are categorical. We can think of the following objects as sub-matrices of the design matrix in the model.\n\nintercept = np.ones((len(data), 1))\nage = pd.get_dummies(data[\"agecat\"], drop_first=True).to_numpy()\narea = pd.get_dummies(data[\"area\"], drop_first=True).to_numpy()\ngender = pd.get_dummies(data[\"gender\"], drop_first=True).to_numpy()\n\nNote we have used drop_first=True. This means that we use n_levels - 1 dummies to represent each categorical variable, and the first level is taken as reference. This ensures the resulting design matrix is of full rank.\nNext, we stack these sub-matrices horizontally and convert the result to a Theano tensor variable so we can compute the dot product between this matrix and the vector of coefficients when writing our model in PyMC3.\n\nX = np.hstack([intercept, age, gender, area])\nX = tt.as_tensor_variable(X)\n\n\n\nFit\nWe start declaring the priors for each of the predictors in the model. They are all independent Gaussian distributions. You may wonder where I took the values for the parameters of these distributions. I‚Äôve just copied Bambi‚Äôs default values for this particular problem.\nAt this stage, it is very important to give appropriate shapes to all the objects we create in the model. For example, Œ≤_age is a random variable that represents the coefficients for the age variable. Since 5 dummy variables are used to represent the age, both Œ≤_age and the values passed to mu and sigma must have shape=(5, 1). I‚Äôve failed here many times when trying to replicate the model, so, unfortunately, I know what I‚Äôm talking about üòÖ\n\n# Create model and sample posterior\nwith pm.Model() as model_pymc3:\n    # Build predictors\n    Œ≤_0 = pm.Normal(\"Œ≤_0\", mu=0, sigma=5, shape=1)\n    Œ≤_gender = pm.Normal(\"Œ≤_gender\", mu=0, sigma=5, shape=1)\n    Œ≤_age = pm.Normal(\n        \"Œ≤_age\",\n        mu=np.array([0] * 5),\n        sigma=np.array([0.32, 6.94, 1.13, 5.44, 9.01]),\n        shape=5\n    )\n    Œ≤_area = pm.Normal(\n      \"Œ≤_area\",\n      mu=np.array([0] * 5),\n      sigma=np.array([0.86, 0.25, 1.3, 0.76, 5.33]),\n      shape=5\n    )\n    \n    # Concatenate the vectors for the coefficients into a single vector\n    Œ≤ = tt.concatenate([Œ≤_0, Œ≤_age, Œ≤_gender, Œ≤_area], axis=0)\n    \n    # Compute and transform linear predictor\n    mu = tt.exp(X.dot(Œ≤))\n      \n    lam = pm.HalfCauchy(\"claim_lam\", beta=1)\n    pm.Wald(\"claim\", mu=mu, lam=lam, observed=data[\"claimcst0\"])\n    \n    idata_pymc = pm.sample( \n      draws=2000, \n      target_accept=0.9, \n      random_seed=1234,\n      return_inferencedata=True\n    )\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [claim_lam, Œ≤_area, Œ≤_gender, Œ≤_age, Œ≤_0]\n\n\n\n\n\n\n\n    \n      \n      100.00% [6000/6000 00:25&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 26 seconds."
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#bambi-model",
    "href": "posts/2021-05-24_why-bambi/index.html#bambi-model",
    "title": "Why Bambi?",
    "section": "Bambi model",
    "text": "Bambi model\nAs you can see below, we don‚Äôt need to do any data preparation, or even specify priors by hand. Bambi automatically obtains sensible default priors when they are not specified, and also knows how to handle each variable type very well.\nThe model is specified using a model formula, quite similar to model formulas in R. The left-hand side of ~ is the response variable, and the rest are the predictors. Here C(agecat) tells Bambi that agecat should be interpreted as categorical. The family argument indicates the conditional distribution for the response, and the link tells Bambi which function of the mean is being modeled by the linear predictor. More information about how they work can be found here.\nThen we have the .fit() method, where you can pass arguments to the pm.sample() function that‚Äôs running in the background.\n\nmodel_bambi = bmb.Model(\n  \"claimcst0 ~ C(agecat) + gender + area\", \n  data, \n  family = \"wald\", \n  link = \"log\"\n)\nidata_bambi = model_bambi.fit(draws=2000, target_accept=0.9, random_seed=1234)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [claimcst0_lam, area, gender, C(agecat), Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [6000/6000 00:18&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 19 seconds.\n\n\nAnd that‚Äôs it! A model that took several lines of codes to specify in PyMC3 only took a few lines of code in Bambi. Quite an advantage, right?"
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#check-results",
    "href": "posts/2021-05-24_why-bambi/index.html#check-results",
    "title": "Why Bambi?",
    "section": "Check results",
    "text": "Check results\nThe simplicity we gain with Bambi would be worthless if the results turned out to be different. We want an interface that makes our job easier, without affecting the quality of the inference. The following is a forest plot where the point gives the posterior mean and the bars indicate a 94% HDI.\n\nsummary_pymc = az.summary(idata_pymc)\nsummary_bambi = az.summary(idata_bambi)\n\n\nsummary_pymc[\"row\"] = list(range(13))\nsummary_pymc[\"panel\"] = [\"1-Intercept\"] + [\"2-Effects\"] * 11 + [\"3-Dispersion\"]\n\nsummary_bambi[\"row\"] = list(range(13))\nsummary_bambi[\"panel\"] = [\"1-Intercept\"] + [\"2-Effects\"] * 11 + [\"3-Dispersion\"]\n\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 5.33), sharey=True, dpi=120)\nfig.subplots_adjust(left=0.13, right=0.975, wspace=0.1, bottom=0.12, top=0.925)\nfig.set_facecolor(\"w\")\n\nfor i, (ax, panel) in enumerate(zip(axes, [\"1-Intercept\", \"2-Effects\", \"3-Dispersion\"])):\n    plt_data = summary_bambi[summary_bambi[\"panel\"] == panel]\n    ax.scatter(plt_data[\"mean\"], plt_data[\"row\"] - 0.25, s=40, label=\"Bambi\")\n    ax.hlines(plt_data[\"row\"] - 0.25, plt_data[\"hdi_3%\"], plt_data[\"hdi_97%\"], lw=2)\n    \n    plt_data = summary_pymc[summary_pymc[\"panel\"] == panel]\n    ax.scatter(plt_data[\"mean\"], plt_data[\"row\"] + 0.25, s=40, label=\"PyMC3\")\n    ax.hlines(plt_data[\"row\"] + 0.25, plt_data[\"hdi_3%\"], plt_data[\"hdi_97%\"], lw=2, color=\"C1\")\n    \n    ax.set_title(panel)\n    ax.tick_params(\"y\", length=0)\n\naxes[0].legend()\nax.set_yticks(range(len(summary_bambi.index)))\nax.set_yticklabels(list(summary_bambi.index))\n\nfig.text(0.5, 0.025, \"Marginal posterior\", size=12, ha=\"center\")\nfig.text(0.02, 0.5, \"Parameter\", size=12, va=\"center\", rotation=90)\n\nfig.savefig(\"imgs/plot.png\", dpi=120)\n\n\n\n\n\n\n\n\nWhile most of the marginal posteriors match very well, we can clearly see the ones for Œ≤_area[3] and Œ≤_area[4] don‚Äôt overlap as much as the others. One of the possible explanations for this difference is related to the MCMC algorithm. While we know both models are indeed the same model, their internal representation is not exactly the same. For example, the model we wrote in pure PyMC3 computes a unique dot product between a matrix of shape (n, p) a vector of shape (p, 1), while the model in Bambi is computing the sum of many smaller dot products. As the internal representations are not exactly the same, the sampling spaces differ and the sampling algorithm obtained slightly different results."
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#conclusion",
    "href": "posts/2021-05-24_why-bambi/index.html#conclusion",
    "title": "Why Bambi?",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we saw how the same GLM can be expressed in both PyMC3 and Bambi. PyMC3 allowed us to control every fine-grained detail of the model specification, while Bambi allowed us to express the same model in a much more concise manner.\nBambi‚Äôs advantages in these types of scenarios aren‚Äôt only related to the amount of code one has to write. Bambi also prevents us from making mistakes when writing the PyMC3 model, such as the mistakes I was making when specifying the shapes for the distributions. Or one could just simply don‚Äôt know how correctly prepare the data that should go in the design matrix, such as the conversion of the categorical data to numeric matrices in such a way that the information is retained without introducing structural redundancies.\nNevertheless, this doesn‚Äôt mean we should always favor Bambi over PyMC3. Whether Bambi or PyMC3 is appropriate for you actually depends on your use case. If you‚Äôre someone who mainly needs to fit GLMs or GLMMs, Bambi is the way to go and it would be nice you give it a chance. There are a bunch of examples showing how to specify and fit different GLMs with Bambi. On the other hand, if you‚Äôre someone who writes a lot of custom models, PyMC3 will be your best friend when it comes to working with Bayesian models in Python.\nBambi is a community project and welcomes contributions such as bug fixes, examples, issues related to bugs or desired enhancements, etc. Want to know more? Visit the official docs or explore the Github repo. Also, if you have any doubts about whether the feature you want is available or going to be developed, feel free to reach out to us! You can always open a new issue to request a feature or leave feedback about the library, and we welcome them a lot üòÅ."
  },
  {
    "objectID": "posts/2021-05-24_why-bambi/index.html#acknowledgments",
    "href": "posts/2021-05-24_why-bambi/index.html#acknowledgments",
    "title": "Why Bambi?",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI want to thank Agustina, Ravin, and Osvaldo for very useful comments and feedback on an earlier version of this post. They helped me to make this post much nicer than what it was originally."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html",
    "href": "posts/2025-08-14_temperatura-mate/index.html",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "",
    "text": "NOTE: This blogpost was automatically translated from this versi√≥n en espa√±ol.\nCode\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport preliz as pz\nimport pymc as pm\nimport xarray as xr\n\nrandom_seed = sum(map(ord, \"cooling\"))\n\n\ndef compute_temperature(time, r, T_0, T_env):\n    return T_env + (T_0 - T_env) * np.exp(-r * time)\n\n\ndef plot_estimated_curves(idata, x, y, T_env, color=\"C0\", kind=\"mean\", axes=None):\n    if axes is None:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    else:\n        fig = axes[0].figure\n\n    t_grid = np.linspace(0, 9, num=100)\n    t_grid_xr = xr.DataArray(t_grid, coords={\"__idx__\": list(range(100))}, dims=\"__idx__\")\n\n    mu_transformed = idata.posterior[\"alpha\"] - idata.posterior[\"beta\"] * t_grid_xr\n\n    if kind == \"outcome\":\n        coords = mu_transformed.coords\n        dims = mu_transformed.dims\n        draws = np.random.normal(\n            loc=mu_transformed, scale=idata.posterior[\"sigma\"].to_numpy()[..., np.newaxis]\n        )\n\n        mu_transformed = xr.DataArray(\n            draws,\n            coords=coords,\n            dims=dims\n        )\n\n    mu_original = np.exp(mu_transformed) + T_env\n    mu_transformed_mean = mu_transformed.mean((\"chain\", \"draw\")).to_numpy()\n    mu_transformed_ci50 = mu_transformed.quantile((0.25, 0.75), (\"chain\", \"draw\")).to_numpy()\n    mu_transformed_ci90 = mu_transformed.quantile((0.05, 0.95), (\"chain\", \"draw\")).to_numpy()\n\n    mu_original_mean = mu_original.mean((\"chain\", \"draw\")).to_numpy()\n    mu_original_ci50 = mu_original.quantile((0.25, 0.75), (\"chain\", \"draw\")).to_numpy()\n    mu_original_ci90 = mu_original.quantile((0.05, 0.95), (\"chain\", \"draw\")).to_numpy()\n\n\n    axes[0].plot(t_grid, mu_original_mean, color=color)\n    axes[0].fill_between(\n        x=t_grid, y1=mu_original_ci50[0], y2=mu_original_ci50[1], alpha=0.5, color=color\n    )\n    axes[0].fill_between(\n        x=t_grid, y1=mu_original_ci90[0], y2=mu_original_ci90[1], alpha=0.5, color=color\n    )\n\n    axes[1].plot(t_grid, mu_transformed_mean, color=color)\n    axes[1].fill_between(\n        x=t_grid, y1=mu_transformed_ci50[0], y2=mu_transformed_ci50[1], alpha=0.5, color=color\n    )\n    axes[1].fill_between(\n        x=t_grid, y1=mu_transformed_ci90[0], y2=mu_transformed_ci90[1], alpha=0.5, color=color\n    )\n\n    axes[0].scatter(x, y, color=\"0.33\");\n    axes[1].scatter(x, np.log(y - T_env), color=\"0.33\");\n\n    axes[0].set(xlabel=\"Tiempo (horas)\", ylabel=\"Temperatura (¬∞C)\");\n    axes[1].set(xlabel=\"Tiempo (horas)\", ylabel=\"$\\\\log(T - T_\\\\text{ambiente})$\");\n\n    axes[0].grid(ls=\"--\")\n    axes[1].grid(ls=\"--\")\n    axes[0].set_axisbelow(True)\n    axes[1].set_axisbelow(True)\n\n    return fig, axes"
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html#introduction",
    "href": "posts/2025-08-14_temperatura-mate/index.html#introduction",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "Introduction",
    "text": "Introduction\nA few weeks ago, while browsing the internet, I came across this post on the social network X. There, a farmer described an experiment he was about to conduct to measure the heat retention capacity of several thermoses he had on hand.\n\nThe experiment involved pouring water at 80 ¬∞C into each thermos and recording its temperature several times throughout the day. With the collected data, it would be possible to determine which thermos had the best‚Äîand which had the worst‚Äîheat retention capacity.\nAfter several measurements, the author of the experiment shared the results obtained:\n\nFrom these results, it can be concluded that the ‚Äúnn tapa rosa‚Äù thermos was the worst performer, as the water inside lost heat considerably more quickly than in any other thermos.\nNow, the inevitable question is: which thermos offers the best heat retention?\nTo answer this question, we will use a Bayesian model based on Newton‚Äôs Law of Cooling."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html#newtons-law-of-cooling",
    "href": "posts/2025-08-14_temperatura-mate/index.html#newtons-law-of-cooling",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "Newton‚Äôs Law of Cooling",
    "text": "Newton‚Äôs Law of Cooling\nNewton‚Äôs law of cooling states that the temperature of an object changes at a rate proportional to the difference between its temperature and the ambient temperature:\n\\[\n\\frac{dT(t)}{dt} = r \\, (T_\\text{env} - T(t))\n\\tag{1}\\]\nwhere \\(r\\) is a cooling rate.\nOne solution to Equation¬†1 is:\n\\[\nT(t) = T_\\text{env} + (T(0)- T_\\text{env}) \\, e^{-rt}\n\\tag{2}\\]\nThis means that the temperature decays exponentially toward the ambient temperature as time passes.\nIt can also be noted that the logarithm of the difference between the temperature at time \\(t\\) and the ambient temperature is a linear function:\n\\[\n\\log\\left(T(t) - T_\\text{env}\\right) = \\log\\left(T(0) - T_\\text{env}\\right) - rt\n\\]\nwhich can be written more concisely as:\n\\[\n\\log\\left(T(t) - T_\\text{env}\\right) = \\alpha - \\beta t\n\\]\nwhere \\(\\alpha = \\log\\left(T(0) - T_\\text{env}\\right)\\) and \\(\\beta = r\\).\nThe figure below shows the shape taken by Equation¬†2 for different cooling rates \\(r\\) alongside its corresponding transformed version.\n\n\nCode\nT_0, T_env = 80, 24\ntime_grid = np.linspace(0, 24, num=200)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nfor r in (0.02, 0.05, 0.1, 0.2, 0.5, 1):\n    axes[0].plot(\n        time_grid,\n        compute_temperature(time=time_grid, r=r, T_0=T_0, T_env=T_env),\n        label=f\"$r={r}$\"\n    );\n    axes[1].plot(\n        time_grid,\n        np.log(compute_temperature(time=time_grid, r=r, T_0=T_0, T_env=T_env) - T_env),\n        label=f\"$r={r}$\"\n    );\n\naxes[0].axhline(y=T_env, color=\"0.2\", lw=1.5, ls=\"--\")\naxes[0].grid(zorder=-99, ls=\"--\")\naxes[1].grid(zorder=-99, ls=\"--\")\naxes[1].set(ylim=(-5, 4.4))\n\naxes[0].legend()\naxes[0].set(xlabel=\"Tiempo\", ylabel=\"Temperatura\");\naxes[1].set(xlabel=\"Tiempo\", ylabel=\"$\\\\log(T(t) - T_\\\\text{ambiente})$\");\n\n\n\n\n\n\n\n\nFigure¬†1\n\n\n\n\n\nThe higher the value of \\(r\\), the worse the thermos‚Äô ability to retain temperature. In other words, the best thermos will be the one with the lowest value of \\(r\\) (assuming that \\(r &gt; 0\\))."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html#prior-elicitation",
    "href": "posts/2025-08-14_temperatura-mate/index.html#prior-elicitation",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "Prior elicitation",
    "text": "Prior elicitation\nIn this article, we will work with models in the following way:\n\\[\n\\begin{aligned}\n\\log(T(t_i) - T_\\text{env}) \\mid t_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\alpha - \\beta t_i\n\\end{aligned}\n\\]\nThat is, for a given time \\(t_i\\), we assume that the logarithm of the difference between the water temperature and the ambient temperature follows a normal distribution.\nThe parameter of greatest interest is \\(\\beta\\), which represents the cooling rate of the water in the thermos. First, we know that its value must be positive, since the initial water temperature drops to the ambient temperature. Furthermore, based on the curves shown in Figure¬†1, we can assume that a reasonable range for this parameter is in the interval \\((0, 1)\\). This range implies that the water in the thermos would reach room temperature, at worst, about 5 hours after filling it.\nUsing PreliZ, we can obtain the parameters of a gamma distribution that satisfy our requirements.\n\npz.maxent(pz.Gamma(), lower=0.001, upper=1, mass=0.99);\n\n\n\n\n\n\n\n\nAnother unknown parameter in our model is \\(\\sigma\\), the conditional standard deviation. It is important to note that this deviation is not expressed in degrees Celsius, as it describes the variability of \\(\\log(T(t_i) - T_\\text{env})\\), and not the variability of \\(T(t_i)\\).\nLooking at the right panel of Figure¬†1, we can see that the range of variation in the response covers only a few units. Therefore, in this case, we will opt for a moderately informative gamma distribution, which concentrates a high probability in the interval \\((0.05, 0.3)\\).\n\npz.maxent(pz.Gamma(), lower=0.05, upper=0.3, mass=0.95);\n\n\n\n\n\n\n\n\nFinally, we could elicitate a prior distribution for \\(\\alpha\\). However, it is not a parameter with an intuitive interpretation.\nWhat we can do is establish a prior for the initial temperature, which implicitly determines a prior for \\(\\alpha\\).\nSince in our case we know the water temperature at \\(t = 0\\), we will consider two approaches:\n\nFixed value for \\(T(0)\\): in this case, \\(\\alpha\\) is fixed to \\(\\log(T(0) - T_\\text{env})\\).\nInformative distribution for \\(T(0)\\): centered on the observed value. We use a normal distribution with mean \\(T(0)\\) and standard deviation of \\(0.3\\) ¬∞C, which is equivalent to saying that the initial temperature differs by at most one degree from the measured temperature."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html#data",
    "href": "posts/2025-08-14_temperatura-mate/index.html#data",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "Data",
    "text": "Data\nFrom the photo shared in X‚Äôs post, the following time and temperature values can be obtained:\n\nT_env = 24\ntime = np.array([0, 180, 320, 500]) / 60 # en horas\ndata = {\n    \"stanley\": np.array([80.0, 70.0, 65.0, 60.0]),\n    \"aluminio\": np.array([80.0, 62.5, 57.5, 50.0]),\n    \"lumilagro\": np.array([75.0, 65.0, 60.0, 55.0]),\n    \"nn-rosa\": np.array([80.0, 47.5, 37.5, 30.0])\n}\n\nIt should be noted that the value of the ambient temperature (T_env) is an assumption, as it does not appear in any of the posts.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nfig.subplots_adjust(bottom=0.2)\n\nfor i, (brand, temperatures) in enumerate(data.items()):\n    axes[0].plot(time, temperatures, color=f\"C{i}\", lw=1)\n    axes[1].plot(time, np.log(temperatures - T_env), color=f\"C{i}\", lw=1)\n    axes[0].scatter(time, temperatures, color=f\"C{i}\", label=brand)\n    axes[1].scatter(time, np.log(temperatures - T_env), color=f\"C{i}\", label=brand)\n\naxes[0].set(xlabel=\"Tiempo (horas)\", ylabel=\"Temperatura (¬∞C)\");\naxes[1].set(xlabel=\"Tiempo (horas)\", ylabel=\"$\\\\log(T(t) - T_\\\\text{ambiente})$\");\naxes[0].grid(ls=\"--\")\naxes[1].grid(ls=\"--\")\n\naxes[0].axhline(y=T_env, color=\"0.2\", lw=1.5, ls=\"--\")\n\naxes[1].legend(\n    loc=\"lower center\",\n    ncol=4,\n    bbox_to_anchor=(0.5, 0.025),\n    bbox_transform=fig.transFigure,\n    handletextpad=0.1,\n    columnspacing=1\n);\n\n\n\n\n\n\n\n\nFigure¬†2\n\n\n\n\n\nAt first glance, the ‚Äústanley‚Äù thermos maintains the highest temperatures at all times, while the one with the ‚Äúnn tapa rosa‚Äù stands out for its poor performance. The ‚Äúlumilagro‚Äù, on the other hand, performs better than the ‚Äúaluminio‚Äù one: although it started with a lower initial temperature, it cooled down more slowly. Finally, it cannot be said with certainty whether the ‚Äústanley‚Äù thermos really outperforms the ‚Äúlumilagro‚Äù, since, although its measurements were always higher, it also started with a higher temperature.\nOn the other hand, the right panel of Figure¬†2 shows a linear trend for each thermos, which is consistent with the use of a linear model on \\(\\log(T(t) - T_\\text{env})\\)."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html#modelos",
    "href": "posts/2025-08-14_temperatura-mate/index.html#modelos",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "Modelos",
    "text": "Modelos\n\nModel 1: A thermos + known intercept\nBefore we start working with a model that considers all brands together, let‚Äôs work with a model for the brand ‚Äústanley‚Äù only.\n\nwith pm.Model() as model_1:\n1    T_0 = pm.Data(\"T_0\", data[\"stanley\"].item(0))\n2    alpha = pm.Deterministic(\"alpha\", np.log(T_0 - T_env))\n    beta = pm.Gamma(\"beta\", alpha=3.3, beta=9)\n    sigma = pm.Gamma(\"sigma\", alpha=6.2, beta=37)\n    mu = alpha - beta * time\n    pm.Normal(\"log(T - T_env)\", mu=mu, sigma=sigma, observed=np.log(data[\"stanley\"] - T_env))\n\ndisplay(model_1.to_graphviz())\n\nwith model_1:\n3    idata_1 = pm.sample(random_seed=random_seed, target_accept=0.95)\n\n\n1\n\nIt registers the initial temperature as data within the model so that it displays the corresponding node in the graph.\n\n2\n\nIt registers alpha with a Deterministic so that its values are stored in the posterior group of InferenceData.\n\n3\n\nThe same random_seed and a high target_accept value are always used to reduce the chance of divergence.\n\n\n\n\n\n\n\n\n\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [beta, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\nNext, we use az.summary() to obtain a summary of the marginal posteriors:\n\naz.summary(idata_1, var_names=[\"beta\", \"sigma\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nbeta\n0.059\n0.010\n0.041\n0.081\n0.000\n0.000\n1421.0\n1350.0\n1.0\n\n\nsigma\n0.095\n0.047\n0.024\n0.179\n0.001\n0.001\n921.0\n899.0\n1.0\n\n\n\n\n\n\n\nFor both parameters, the effective sample size is large enough, and the chains converge and mix correctly. 1\n\nfig, axes = plot_estimated_curves(idata=idata_1, x=time, y=data[\"stanley\"], T_env=T_env);\n\n\n\n\n\n\n\nFigure¬†3\n\n\n\n\n\nIn the right panel of Figure¬†3, the fitted regression line is shown along with the 50% and 95% credible intervals on the transformed data scale, while the left panel shows the results on the original scale. The regression line fits the points well, although the uncertainty associated with its estimate increases over time. On the other hand, the absence of uncertainty at $t=0$ is explained by the fact that the intercept has a fixed value.\n\n\nModel 2: One thermos + unknown intercept\nIn this second model, we still work with a single thermos. The difference is that instead of fixing the initial temperature to the observed value, we assign it a highly informative prior distribution. This way, we continue incorporating the information we already have, but we don‚Äôt force the regression line to pass through a fixed point.\n\nwith pm.Model() as model_2:\n    T_0 = pm.Normal(\"T_0\", mu=80, sigma=0.5)\n    alpha = pm.Deterministic(\"alpha\", np.log(T_0 - T_env))\n    beta = pm.Gamma(\"beta\", alpha=3.34, beta=12.8)\n    sigma = pm.Gamma(\"sigma\", alpha=6.2, beta=37)\n    mu = pm.Deterministic(\"mu\", alpha - beta * time)\n    pm.Normal(\"log(T - T_env)\", mu=mu, sigma=sigma, observed=np.log(data[\"stanley\"] - T_env))\n\ndisplay(model_2.to_graphviz())\n\nwith model_2:\n    idata_2 = pm.sample(random_seed=random_seed, target_accept=0.95)\n\n\n\n\n\n\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [T_0, beta, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\naz.summary(idata_2, var_names=[\"T_0\", \"alpha\", \"beta\", \"sigma\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nT_0\n79.993\n0.508\n79.056\n80.967\n0.010\n0.009\n2720.0\n2448.0\n1.0\n\n\nalpha\n4.025\n0.009\n4.008\n4.042\n0.000\n0.000\n2720.0\n2448.0\n1.0\n\n\nbeta\n0.059\n0.010\n0.040\n0.079\n0.000\n0.000\n1891.0\n1432.0\n1.0\n\n\nsigma\n0.096\n0.049\n0.022\n0.187\n0.001\n0.001\n1399.0\n1500.0\n1.0\n\n\n\n\n\n\n\n\nfig, axes = plot_estimated_curves(idata=idata_2, x=time, y=data[\"stanley\"], T_env=T_env);\n\n\n\n\n\n\n\n\n\naz.plot_forest([idata_1, idata_2], model_names=[\"Modelo 1\", \"Modelo 2\"], var_names=[\"beta\", \"sigma\"], combined=True, figsize=(6, 4));\n\n\n\n\n\n\n\n\nThe marginal posteriors for beta and sigma are practically identical to those from the first model. In other words, the conclusions we can draw about beta and sigma are similar to those obtained when using a fixed initial temperature. In addition, the effective sample sizes are consistently higher than those obtained previously.\n\n\nModel 3: All brands\nNow that we are familiar with the model for a single brand, we can extend it to work with all brands.\nInstead of having a single T_0, alpha, and beta, we will have one for each brand. In PyMC, this is achieved using dims, which allows us to work with vectors of random variables instead of scalars.\n\n1y = np.log(np.concatenate([temps for temps in data.values()]) - T_env)\ntimes = np.tile(time, 4)\nbrand_idx = np.repeat(np.arange(4), 4)\ncoords = {\n    \"brand\": list(data)\n}\n\nwith pm.Model(coords=coords) as model_3:\n    T_0 = pm.Normal(\"T_0\", mu=[v.item(0) for v in data.values()], sigma=0.5, dims=\"brand\")\n    alpha = pm.Deterministic(\"alpha\", np.log(T_0 - T_env), dims=\"brand\")\n    beta = pm.Gamma(\"beta\", alpha=3.3, beta=12.8, dims=\"brand\")\n    sigma = pm.Gamma(\"sigma\", alpha=6.2, beta=37)\n    mu = pm.Deterministic(\"mu\", alpha[brand_idx] - beta[brand_idx] * times)\n    pm.Normal(\"log(T - T_env)\", mu=mu, sigma=sigma, observed=y)\n\ndisplay(model_3.to_graphviz())\n\nwith model_3:\n    idata_3 = pm.sample(random_seed=random_seed, target_accept=0.95)\n\n\n1\n\nOne-dimensional arrays are created with the temperatures, times, and brand index for all brands. In addition, a coordinate dictionary is prepared so that dims=\"brand\" can be used in the model.\n\n\n\n\n\n\n\n\n\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [T_0, beta, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\n\n\nLet‚Äôs look at the posterior summary returned by az.summary().\n\naz.summary(idata_3, var_names=[\"T_0\", \"beta\", \"sigma\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nT_0[stanley]\n79.971\n0.487\n79.047\n80.883\n0.008\n0.008\n3452.0\n2791.0\n1.0\n\n\nT_0[aluminio]\n79.895\n0.493\n79.028\n80.855\n0.008\n0.008\n3578.0\n3149.0\n1.0\n\n\nT_0[lumilagro]\n74.971\n0.483\n74.097\n75.889\n0.008\n0.007\n4030.0\n3046.0\n1.0\n\n\nT_0[nn-rosa]\n79.933\n0.487\n79.015\n80.797\n0.008\n0.008\n3840.0\n3018.0\n1.0\n\n\nbeta[stanley]\n0.056\n0.005\n0.047\n0.067\n0.000\n0.000\n2920.0\n2203.0\n1.0\n\n\nbeta[aluminio]\n0.096\n0.005\n0.086\n0.105\n0.000\n0.000\n2860.0\n2534.0\n1.0\n\n\nbeta[lumilagro]\n0.063\n0.005\n0.053\n0.073\n0.000\n0.000\n3685.0\n2653.0\n1.0\n\n\nbeta[nn-rosa]\n0.269\n0.005\n0.259\n0.278\n0.000\n0.000\n3425.0\n2750.0\n1.0\n\n\nsigma\n0.050\n0.014\n0.028\n0.075\n0.000\n0.000\n2049.0\n2109.0\n1.0\n\n\n\n\n\n\n\nThe first thing that stands out is that the posterior means of the initial temperatures fall into two groups: one with values close to 80 and another around 75. This result makes sense, since the initial temperature was 80 ¬∞C for all thermoses except the Lumilagro, which was 75 ¬∞C.\nThe beta values for each thermos also show some heterogeneity. In this case, we can conclude that the ‚Äúnn-rosa‚Äù thermos has the highest heat loss (largest beta value), although it‚Äôs not possible to determine with certainty which one offers the best heat retention.\nFinally, we observe that the effective sample sizes range from 2000 to 4000, exceeding in all cases those obtained in the previous models. This suggests that the posterior has a geometry more accessible to the NUTS sampler used by PyMC.\nUsing the az.plot_forest() function, we can obtain a summary of the marginal distribution of beta for each brand:\n\naz.plot_forest(idata_3, var_names=\"beta\", combined=True, figsize=(6, 4));\n\n\n\n\n\n\n\n\nAs mentioned earlier, the ‚Äúnn-rosa‚Äù shows the highest heat loss, followed by the ‚Äúaluminio‚Äù thermos, and finally the ‚Äúlumilagro‚Äù and ‚Äústanley‚Äù thermoses. At first glance, it might seem that the ‚Äústanley‚Äù has a better ability to retain heat, although this plot alone does not allow us to draw definitive conclusions.\nAlthough the sigma parameter is not inherently relevant, it is interesting to explore its posterior distribution, as it provides a measure of the random uncertainty around our regression line.\n\naz.plot_forest(\n    [idata_2, idata_3],\n    model_names=[\"Modelo 2\", \"Modelo 3\"],\n    var_names=\"sigma\",\n    combined=True,\n    figsize=(6, 4)\n);\n\n\n\n\n\n\n\n\nBy using the data from all thermoses in this third model, we obtain an estimate of sigma with considerably less uncertainty.\nFinally, we can visualize the estimated curves for each thermos, both on their original scale and on the transformed scale:\n\nfor i, brand in enumerate(data):\n    fig, axes = plot_estimated_curves(\n        idata=idata_3.sel(brand=brand),\n        x=time,\n        y=data[brand],\n        T_env=T_env,\n        color=f\"C{i}\",\n    )\n    fig.suptitle(f\"Marca: {brand}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we can see that the uncertainty in the regression line for the ‚Äústanley‚Äù brand has decreased. In addition, it seems that for those thermoses whose water temperature approached the ambient temperature more quickly (i.e., those with lower heat retention capacity), the uncertainty is smaller. The clearest example of this behavior is the ‚Äúnn-rosa‚Äù thermos, whose temperature was almost equal to the ambient temperature by the end of the experiment.\n\n\nModel 4: All brands + partial pooling for $$\nFinally, we will create a hierarchical model in which we assume that the cooling rates \\(\\beta\\) belong to a common population. By sharing information across thermoses in this way, we expect to obtain more precise and stable posteriors, reflecting both the individual characteristics of each thermos and the overall trend of the population.\n\nwith pm.Model(coords=coords) as model_4:\n    T_0 = pm.Normal(\"T_0\", mu=[v.item(0) for v in data.values()], sigma=0.5, dims=\"brand\")\n    alpha = pm.Deterministic(\"alpha\", np.log(T_0 - T_env), dims=\"brand\")\n\n1    beta_mu = pm.Normal(\"beta_mu\", mu=3.3 / 12.8, sigma=0.05)\n2    beta_sigma = pm.InverseGamma(\"beta_sigma\", alpha=4.2, beta=0.3)\n    beta = pm.Gamma(\n        \"beta\",\n        alpha=(beta_mu / beta_sigma)**2,\n        beta=beta_mu / (beta_sigma**2),\n        dims=\"brand\"\n    )\n\n    sigma = pm.Gamma(\"sigma\", alpha=6.2, beta=37)\n    mu = pm.Deterministic(\"mu\", alpha[brand_idx] - beta[brand_idx] * times)\n    pm.Normal(\"log(T - T_env)\", mu=mu, sigma=sigma, observed=y)\n\n\nwith model_4:\n3    idata_4 = pm.sample_prior_predictive(draws=1000, random_seed=random_seed)\n    idata_4.extend(pm.sample(random_seed=random_seed, target_accept=0.95))\n\n\n1\n\nThe beta parameters still have a Gamma prior with a common mean and standard deviation. In this case, however, these parameters are unknown and determined by the data.\n\n2\n\nThe function pz.maxent(pz.InverseGamma(), lower=0.01, upper=0.2, mass=0.95) is used.\n\n3\n\nSamples from the prior are drawn to generate Figure¬†4.\n\n\n\n\nSampling: [T_0, beta, beta_mu, beta_sigma, log(T - T_env), sigma]\nInitializing NUTS using jitter+adapt_diag...\n/home/tomas/miniconda3/envs/pymc-env/lib/python3.12/site-packages/pytensor/tensor/elemwise.py:710: RuntimeWarning: invalid value encountered in log\n  variables = ufunc(*ufunc_args, **ufunc_kwargs)\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [T_0, beta_mu, beta_sigma, beta, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n\n\nBelow are the marginal posteriors of \\(\\beta\\) and \\(\\sigma\\) for both the non-hierarchical and hierarchical models:\n\naz.plot_forest(\n    [idata_3, idata_4],\n    model_names=[\"Modelo 3\", \"Modelo 4\"],\n    var_names=[\"beta\", \"sigma\"],\n    combined=True,\n    figsize=(6, 4)\n);\n\n\n\n\n\n\n\n\nVisually, we can conclude that there are no differences between the marginal posteriors of the two models. In other words, the partial pooling provided by the hierarchical model is practically nonexistent.\nTo understand why no differences are observed between the models, we can examine the prior and posterior of the population mean and standard deviation of \\(\\beta\\) (beta_mu and beta_sigma).\n\naz.plot_dist_comparison(idata_4, var_names=[\"beta_mu\", \"beta_sigma\"], figsize=(12, 15));\n\n\n\n\n\n\n\nFigure¬†4\n\n\n\n\n\nIn both cases, the posterior is very similar to the prior. This is because the available information to estimate beta_mu and beta_sigma is insufficient to obtain posteriors with low uncertainty. This result is expected, since the number of groups is very small (only 4). In situations like this, unless strong prior information is available, the hierarchical approach will not yield appreciable differences compared to a model with independent parameters."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html#conclusions",
    "href": "posts/2025-08-14_temperatura-mate/index.html#conclusions",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "Conclusions",
    "text": "Conclusions\n\nWhich is the best thermos?\nThe best thermos is the one with the lowest cooling rate $$.\nBased on our model, we can obtain a probabilistic result as follows:\n\\[\nP(\\beta_j = \\min\\left\\{\\beta_1, \\beta_2, \\beta_3, \\beta_4 \\right\\} ) \\qquad \\forall j \\in {1, 2, 3, 4}\n\\]\n\nbeta_argmin = idata_4.posterior[\"beta\"].argmin(axis=-1)\np_min = [(beta_argmin == j).mean().item() for j in range(4)]\ndict(zip(data, p_min))\n\n{'stanley': 0.826, 'aluminio': 0.0, 'lumilagro': 0.174, 'nn-rosa': 0.0}\n\n\nAccording to our model, there is an 82% probability that the ‚Äústanley‚Äù thermos is the one that best retains heat.\nIn practice, it is up to us to decide whether that probability is sufficient to conclude that ‚Äústanley‚Äù is indeed superior to ‚Äúlumilagro‚Äù. For example, we could also consider the difference in degrees that the ‚Äústanley‚Äù manages to maintain above the ‚Äúlumilagro‚Äù over time.\nAlthough the result is not particularly surprising, it is also possible to determine probabilistically which is the worst thermos:\n\\[\nP(\\beta_j = \\max\\left\\{\\beta_1, \\beta_2, \\beta_3, \\beta_4 \\right\\} ) \\qquad \\forall j \\in {1, 2, 3, 4}\n\\]\n\nbeta_argmax = idata_4.posterior[\"beta\"].argmax(axis=-1)\np_max = [(beta_argmax == j).mean().item() for j in range(4)]\ndict(zip(data, p_max))\n\n{'stanley': 0.0, 'aluminio': 0.0, 'lumilagro': 0.0, 'nn-rosa': 1.0}"
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html#appendix",
    "href": "posts/2025-08-14_temperatura-mate/index.html#appendix",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "Appendix",
    "text": "Appendix\n\nInference based on uniform priors\nIt is reasonable to ask whether the effort involved in specifying priors is worthwhile. Below, we fit the multi-brand model using uniform priors and compare the results with those obtained previously.\n\nwith pm.Model(coords=coords) as model_5:\n    alpha = pm.Flat(\"alpha\", dims=\"brand\")\n    beta = pm.Flat(\"beta\", dims=\"brand\")\n    sigma = pm.Uniform(\"sigma\", lower=0, upper=100)\n    mu = pm.Deterministic(\"mu\", alpha[brand_idx] - beta[brand_idx] * times)\n    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n\n    idata_5 = pm.sample(random_seed=random_seed, target_accept=0.99, progressbar=False)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, beta, sigma]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\n\n\n\naz.plot_forest(\n    [idata_4, idata_5],\n    model_names=[\"Priors informativos\", \"Priors uniformes\"],\n    var_names=[\"alpha\"],\n    combined=True,\n    figsize=(6, 4)\n);\n\n\n\n\n\n\n\n\n\nax = az.plot_forest(\n    [idata_4, idata_5],\n    model_names=[\"Priors informativos\", \"Priors uniformes\"],\n    var_names=[\"beta\"],\n    combined=True,\n    figsize=(6, 4)\n)\nax[0].grid(axis=\"x\", ls=\"--\");\n\n\n\n\n\n\n\n\n\nax = az.plot_forest(\n    [idata_4, idata_5],\n    model_names=[\"Priors informativos\", \"Priors uniformes\"],\n    var_names=[\"sigma\"],\n    combined=True, figsize=(6, 4)\n)\nax[0].grid(axis=\"x\", ls=\"--\");\n\n\n\n\n\n\n\n\n\nbeta_argmin = idata_5.posterior[\"beta\"].argmin(axis=-1)\np_min = [(beta_argmin == j).mean().item() for j in range(4)]\ndict(zip(data, p_min))\n\n{'stanley': 0.74575, 'aluminio': 0.001, 'lumilagro': 0.25325, 'nn-rosa': 0.0}\n\n\n\nbeta_argmax = idata_5.posterior[\"beta\"].argmax(axis=-1)\np_max = [(beta_argmax == j).mean().item() for j in range(4)]\ndict(zip(data, p_max))\n\n{'stanley': 0.0, 'aluminio': 0.0, 'lumilagro': 0.0, 'nn-rosa': 1.0}\n\n\nBased on a model with uniform priors, we reach conclusions in the same direction, but with a higher level of uncertainty."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html#thats-all-folks",
    "href": "posts/2025-08-14_temperatura-mate/index.html#thats-all-folks",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "That‚Äôs all folks",
    "text": "That‚Äôs all folks\n\n\n\nA lovely Thomas Bayes enjoying mate"
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/index.html#footnotes",
    "href": "posts/2025-08-14_temperatura-mate/index.html#footnotes",
    "title": "In search of the best thermos for mate with Thomas Bayes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEn este blog no hacemos uso de los traceplots porque las cadenas siempre se mezclan bien y resulta suficiente usar el tama√±o efectivo de muestra y el \\(\\hat{R}\\).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-06-26_tidypolars/index.html",
    "href": "posts/2022-06-26_tidypolars/index.html",
    "title": "Let‚Äôs use tidypolars more",
    "section": "",
    "text": "In this blogpost I‚Äôm going to show how to perform the same task with pandas, the most popular library for data analysis in Python, and tidypolars, a new library to do data analysis with tabular data inspired on the tidyverse.\nThe task consists of computing a variable transformation that relies on grouped aggregations. In particular, we will be computing the standardized version of a numeric variable by group.\n\nimport palmerpenguins\n\nimport tidypolars as tp\n\nfrom tidypolars import col\n\nI‚Äôm going to work with the famous palmer penguins dataset. In Python this can be loaded very easily thanks to the palmerpenguins library.\n\ndata = palmerpenguins.load_penguins()\ndata\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n2009\n\n\n340\nChinstrap\nDream\n43.5\n18.1\n202.0\n3400.0\nfemale\n2009\n\n\n341\nChinstrap\nDream\n49.6\n18.2\n193.0\n3775.0\nmale\n2009\n\n\n342\nChinstrap\nDream\n50.8\n19.0\n210.0\n4100.0\nmale\n2009\n\n\n343\nChinstrap\nDream\n50.2\n18.7\n198.0\n3775.0\nfemale\n2009\n\n\n\n\n344 rows √ó 8 columns\n\n\n\n\n\nBefore seeing an example using tidypolars I‚Äôm going to perform some basic data wrangling with pandas. One of the first things one usually do with a data frame is exploring its first rows visually. The .head() method comes very handy.\n\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\nI‚Äôm going to work with the species, sex, and body_mass_g columns only. To subset the dataframe we only need to pass the name of these columns as a list within brackets.\n\ndata = data[[\"species\", \"sex\", \"body_mass_g\"]].reset_index(drop=True)\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\n\n\n\n\n0\nAdelie\nmale\n3750.0\n\n\n1\nAdelie\nfemale\n3800.0\n\n\n2\nAdelie\nfemale\n3250.0\n\n\n3\nAdelie\nNaN\nNaN\n\n\n4\nAdelie\nfemale\n3450.0\n\n\n\n\n\n\n\nI add the .reset_index(drop=True) to avoid some SettingWithCopyWarnings later. I also want to drop any observations with missing values so I use .dropna() to do that.\n\ndata = data.dropna()\n\nSuppose now I want to standardize the variable body_mass_g. Pandas vectorized operations make it extremely easy. One can save the result in a new column in a very intuitive way as well.\n\ndata[\"body_mass_z\"] = (data[\"body_mass_g\"] - data[\"body_mass_g\"].mean()) / data[\"body_mass_g\"].std()\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n0\nAdelie\nmale\n3750.0\n-0.567621\n\n\n1\nAdelie\nfemale\n3800.0\n-0.505525\n\n\n2\nAdelie\nfemale\n3250.0\n-1.188572\n\n\n4\nAdelie\nfemale\n3450.0\n-0.940192\n\n\n5\nAdelie\nmale\n3650.0\n-0.691811\n\n\n\n\n\n\n\nWe could consider it is more appropiate to standardize considering the species variable. We still perform the same operation than above, but we do it within each group.\nThe way to perform such operations in pandas is to use the .groupby() method. Then one can select the desired column and compute the aggregation.\nFor example, to compute the mean body mass by species we can do the following.\n\ndata.groupby(\"species\")[\"body_mass_g\"].mean()\n\nspecies\nAdelie       3706.164384\nChinstrap    3733.088235\nGentoo       5092.436975\nName: body_mass_g, dtype: float64\n\n\nWe obtained a pandas Series with three values, the mean for each species. If we want to obtain a Series of the same length than the original data, it is, for each row the mean of the species the observation belongs to, we can use .transform().\n\ndata.groupby(\"species\")[\"body_mass_g\"].transform(\"mean\")\n\n0      3706.164384\n1      3706.164384\n2      3706.164384\n4      3706.164384\n5      3706.164384\n          ...     \n339    3733.088235\n340    3733.088235\n341    3733.088235\n342    3733.088235\n343    3733.088235\nName: body_mass_g, Length: 333, dtype: float64\n\n\nThe same can be done with other transformations such as the standard deviation.\n\ndata.groupby(\"species\")[\"body_mass_g\"].transform(\"std\")\n\n0      458.620135\n1      458.620135\n2      458.620135\n4      458.620135\n5      458.620135\n          ...    \n339    384.335081\n340    384.335081\n341    384.335081\n342    384.335081\n343    384.335081\nName: body_mass_g, Length: 333, dtype: float64\n\n\nNow, putting all the pieces together, we can compute the standardized body mass by species.\n\ndata[\"body_mass_z\"] = (\n    (data[\"body_mass_g\"] - data.groupby(\"species\")[\"body_mass_g\"].transform(\"mean\")) \n    / data.groupby(\"species\")[\"body_mass_g\"].transform(\"std\")\n)\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n0\nAdelie\nmale\n3750.0\n0.095582\n\n\n1\nAdelie\nfemale\n3800.0\n0.204604\n\n\n2\nAdelie\nfemale\n3250.0\n-0.994645\n\n\n4\nAdelie\nfemale\n3450.0\n-0.558555\n\n\n5\nAdelie\nmale\n3650.0\n-0.122464\n\n\n\n\n\n\n\nIt‚Äôs also possible to add more variables to the groups. For example, this is how we can perform the same compution considering groups given by species and sex.\n\ndata[\"body_mass_z\"] = (\n    (data[\"body_mass_g\"] - data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"mean\")) \n    / data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"std\")\n)\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n0\nAdelie\nmale\n3750.0\n-0.846261\n\n\n1\nAdelie\nfemale\n3800.0\n1.600580\n\n\n2\nAdelie\nfemale\n3250.0\n-0.441145\n\n\n4\nAdelie\nfemale\n3450.0\n0.301301\n\n\n5\nAdelie\nmale\n3650.0\n-1.134602\n\n\n\n\n\n\n\nAnd finally, I can sort by the standardized body mass in ascending order.\n\ndata.sort_values(\"body_mass_z\")\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n314\nChinstrap\nfemale\n2700.0\n-2.899080\n\n\n192\nGentoo\nfemale\n3950.0\n-2.591611\n\n\n195\nGentoo\nmale\n4750.0\n-2.346530\n\n\n298\nChinstrap\nfemale\n2900.0\n-2.198147\n\n\n119\nAdelie\nmale\n3325.0\n-2.071711\n\n\n...\n...\n...\n...\n...\n\n\n114\nAdelie\nfemale\n3900.0\n1.971803\n\n\n109\nAdelie\nmale\n4775.0\n2.109234\n\n\n284\nChinstrap\nfemale\n4150.0\n2.182685\n\n\n313\nChinstrap\nmale\n4800.0\n2.377631\n\n\n169\nGentoo\nmale\n6300.0\n2.603039\n\n\n\n\n333 rows √ó 4 columns\n\n\n\n\n\n\nNow it‚Äôs time to see the same operations performed with tidypolars. Tidypolars is inspired on the tidyverse, a set of packages following a consistent design phillosophy that has revolutionated the way we do data science in R and other languages as well.\nIts description says &gt; tidypolars is a data frame library built on top of the blazingly fast polars library that gives access to methods and functions familiar to R tidyverse users.\nTidypolars does not rely on any pandas data structure because it‚Äôs built on top of polars, not pandas. In addition, it works with a new data frame structure called Tibble, borrowing its name from the tibble in the R package of the same name.\nWe can convert a pandas DataFrame to a tibble with the .from_pandas() function.\n\ntibble = tp.from_pandas(palmerpenguins.load_penguins())\ntype(tibble)\n\ntidypolars.tibble.Tibble\n\n\nWe still have a .head() method that prints the first rows. The representation is very similar to a pandas data frame.\n\ntibble.head()\n\n\n\nshape: (5, 8)\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\nyear\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nstr\n\n\ni64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\nnull\n\n\nnull\n\n\nnull\n\n\nnull\n\n\nnull\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\n\n\nTo filter rows we can use the .filter() method. In the next chunk of code we use col(\"*\") to select all columns, tp.is_not_null() to flag observations with non-null values, and .filter() to use those booleans to actually perform the filtering.\nUpdate: See this issue where the main author of tidypolars lets me know that we can use the .drop_null() method instead.\n\ntibble.drop_null()\n\n\n\nshape: (333, 8)\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\nyear\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nstr\n\n\ni64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.3\n\n\n20.6\n\n\n190.0\n\n\n3650.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n38.9\n\n\n17.8\n\n\n181.0\n\n\n3625.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.2\n\n\n19.6\n\n\n195.0\n\n\n4675.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n41.1\n\n\n17.6\n\n\n182.0\n\n\n3200.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n38.6\n\n\n21.2\n\n\n191.0\n\n\n3800.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n34.6\n\n\n21.1\n\n\n198.0\n\n\n4400.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n36.6\n\n\n17.8\n\n\n185.0\n\n\n3700.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n38.7\n\n\n19.0\n\n\n195.0\n\n\n3450.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n45.2\n\n\n16.6\n\n\n191.0\n\n\n3250.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n49.3\n\n\n19.9\n\n\n203.0\n\n\n4050.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n50.2\n\n\n18.8\n\n\n202.0\n\n\n3800.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n45.6\n\n\n19.4\n\n\n194.0\n\n\n3525.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n51.9\n\n\n19.5\n\n\n206.0\n\n\n3950.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n46.8\n\n\n16.5\n\n\n189.0\n\n\n3650.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n45.7\n\n\n17.0\n\n\n195.0\n\n\n3650.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n55.8\n\n\n19.8\n\n\n207.0\n\n\n4000.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n43.5\n\n\n18.1\n\n\n202.0\n\n\n3400.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n49.6\n\n\n18.2\n\n\n193.0\n\n\n3775.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n50.8\n\n\n19.0\n\n\n210.0\n\n\n4100.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n50.2\n\n\n18.7\n\n\n198.0\n\n\n3775.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\n\n\nIt‚Äôs very easy to select columns with the .select() method.\n\ntibble.select(\"species\", \"sex\", \"body_mass_g\")\n\n\n\nshape: (344, 3)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3800.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\nnull\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3625.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n3475.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n4250.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n3300.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n3700.0\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4050.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3800.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3525.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3950.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4000.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3400.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3775.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4100.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3775.0\n\n\n\n\n\n\n\nWhat‚Äôs even better, we can chain these operations. This is where one can start seeing how powerful this approach is.\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n)\n\n\n\nshape: (333, 3)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3800.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3625.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3200.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3800.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4400.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3700.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4050.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3800.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3525.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3950.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4000.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3400.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3775.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4100.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3775.0\n\n\n\n\n\n\n\nWe then have the .summarise() method to compute summaries (or aggregations) by groups. Note we use functions available in the tidypolars namespace such as tp.mean().\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .summarise(\n        body_mass_mean=tp.mean(\"body_mass_g\"),\n        by=\"species\"\n    )\n)\n\n\n\nshape: (3, 2)\n\n\n\n\nspecies\n\n\nbody_mass_mean\n\n\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"Adelie\"\n\n\n3706.164384\n\n\n\n\n\"Gentoo\"\n\n\n5092.436975\n\n\n\n\n\"Chinstrap\"\n\n\n3733.088235\n\n\n\n\n\n\n\nIf we want to get a behavior similar to .groupby() and .transform() in pandas, we can use another verb, mutate().\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_mean=tp.mean(\"body_mass_g\"),\n        by=\"species\"\n    )\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_mean\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4500.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4450.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5400.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4550.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4800.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5200.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4400.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5150.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4650.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5550.0\n\n\n5092.436975\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3400.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3475.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3050.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3725.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3000.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4250.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3475.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3700.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4000.0\n\n\n3706.164384\n\n\n\n\n\n\n\nThis verb is very powerful. We can compute more complex expressions, such as the one involved in the standardization of a variable.\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=(col(\"body_mass_g\") - tp.mean(\"body_mass_g\")) / tp.sd(\"body_mass_g\"),\n        by=\"species\"\n    )\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4500.0\n\n\n-1.181386\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n1.211549\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4450.0\n\n\n-1.281092\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n1.211549\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5400.0\n\n\n0.613315\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4550.0\n\n\n-1.0816\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4800.0\n\n\n-0.583152\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5200.0\n\n\n0.214493\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4400.0\n\n\n-1.380797\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5150.0\n\n\n0.114787\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4650.0\n\n\n-0.882269\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5550.0\n\n\n0.912432\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3400.0\n\n\n-0.667577\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3475.0\n\n\n-0.504043\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3050.0\n\n\n-1.430736\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3725.0\n\n\n0.041\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3000.0\n\n\n-1.539759\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n-0.122464\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4250.0\n\n\n1.185808\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3475.0\n\n\n-0.504043\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n-0.558555\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n0.095582\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3700.0\n\n\n-0.013441\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4000.0\n\n\n0.640695\n\n\n\n\n\n\n\nAnd finally, we can use the .arrange() verb to sort observations by the standardized body mass in ascending order.\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=(col(\"body_mass_g\") - tp.mean(\"body_mass_g\")) / tp.sd(\"body_mass_g\"),\n        by=\"species\"\n    )\n    .arrange(\"body_mass_z\")\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2700.0\n\n\n-2.687988\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n3950.0\n\n\n-2.278148\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-2.167609\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4150.0\n\n\n-1.879326\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6050.0\n\n\n1.909489\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4500.0\n\n\n1.995425\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4650.0\n\n\n2.0579\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n2.112501\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4550.0\n\n\n2.1255\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4700.0\n\n\n2.167013\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4725.0\n\n\n2.221524\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4775.0\n\n\n2.330547\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6300.0\n\n\n2.408017\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4800.0\n\n\n2.775994\n\n\n\n\n\n\n\nWe can make it even clearer if we wrap the standardization operation within a function.\n\ndef standardize(name):\n    return (col(name) - tp.mean(name)) / tp.sd(name)\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=standardize(\"body_mass_g\"),\n        by=\"species\"\n    )\n    .arrange(\"body_mass_z\")\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2700.0\n\n\n-2.687988\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n3950.0\n\n\n-2.278148\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-2.167609\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4150.0\n\n\n-1.879326\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6050.0\n\n\n1.909489\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4500.0\n\n\n1.995425\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4650.0\n\n\n2.0579\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n2.112501\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4550.0\n\n\n2.1255\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4700.0\n\n\n2.167013\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4725.0\n\n\n2.221524\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4775.0\n\n\n2.330547\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6300.0\n\n\n2.408017\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4800.0\n\n\n2.775994\n\n\n\n\n\n\n\nIf we want to carry on the .mutate() operation grouped by more than one variable, we can simply pass the names in a list.\n\n\n\nLet‚Äôs see both approaches in action so we can better appreciate the differences.\n\n\n\ndata = palmerpenguins.load_penguins()\ndata = data[[\"species\", \"sex\", \"body_mass_g\"]].dropna().reset_index(drop=True)\ndata[\"body_mass_z\"] = (\n    (data[\"body_mass_g\"] - data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"mean\")) \n    / data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"std\")\n)\ndata.sort_values(\"body_mass_z\")\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n303\nChinstrap\nfemale\n2700.0\n-2.899080\n\n\n185\nGentoo\nfemale\n3950.0\n-2.591611\n\n\n188\nGentoo\nmale\n4750.0\n-2.346530\n\n\n287\nChinstrap\nfemale\n2900.0\n-2.198147\n\n\n113\nAdelie\nmale\n3325.0\n-2.071711\n\n\n...\n...\n...\n...\n...\n\n\n108\nAdelie\nfemale\n3900.0\n1.971803\n\n\n103\nAdelie\nmale\n4775.0\n2.109234\n\n\n273\nChinstrap\nfemale\n4150.0\n2.182685\n\n\n302\nChinstrap\nmale\n4800.0\n2.377631\n\n\n163\nGentoo\nmale\n6300.0\n2.603039\n\n\n\n\n333 rows √ó 4 columns\n\n\n\n\n\n\n\ndef standardize(name):\n    return (col(name) - tp.mean(name)) / tp.sd(name)\n\n\ntibble = tp.from_pandas(palmerpenguins.load_penguins())\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=standardize(\"body_mass_g\"),\n        by=[\"species\", \"sex\"]\n    )\n    .arrange(\"body_mass_z\")\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2700.0\n\n\n-2.899\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n3950.0\n\n\n-2.591611\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n4750.0\n\n\n-2.3465\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-2.198147\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3325.0\n\n\n-2.071711\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.926035\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.926035\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3250.0\n\n\n-1.902511\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4150.0\n\n\n-1.881329\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n4925.0\n\n\n-1.787708\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3425.0\n\n\n-1.7833\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3300.0\n\n\n-1.764442\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4650.0\n\n\n1.748808\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6050.0\n\n\n1.804721\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n1.820893\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n5200.0\n\n\n1.847652\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n5200.0\n\n\n1.847652\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4700.0\n\n\n1.892979\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4725.0\n\n\n1.965064\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3900.0\n\n\n1.971803\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4775.0\n\n\n2.109234\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n4150.0\n\n\n2.182685\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4800.0\n\n\n2.377631\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6300.0\n\n\n2.603039\n\n\n\n\n\n\n\n\n\n\nPandas is great, so many people love it, and it will be the most used tool to work with tabular data in Python for a long time. In my case, it made me struggle a lot when I started to do data analysis in Python after working several years with R. Later, and fortunately, I began to understand how it worked and I became better at it.\nHowever, I never felt as comfortable with pandas as I do with dplyr syntax. Maybe it‚Äôs my R background? I don‚Äôt know. I just know that one of the things I enjoy the most in R is how easy and clear is to compose data manipulation operations with dplyr and friends. And having that expressiveness in Python is fantastic."
  },
  {
    "objectID": "posts/2022-06-26_tidypolars/index.html#data-wrangling-with-pandas",
    "href": "posts/2022-06-26_tidypolars/index.html#data-wrangling-with-pandas",
    "title": "Let‚Äôs use tidypolars more",
    "section": "",
    "text": "Before seeing an example using tidypolars I‚Äôm going to perform some basic data wrangling with pandas. One of the first things one usually do with a data frame is exploring its first rows visually. The .head() method comes very handy.\n\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\nI‚Äôm going to work with the species, sex, and body_mass_g columns only. To subset the dataframe we only need to pass the name of these columns as a list within brackets.\n\ndata = data[[\"species\", \"sex\", \"body_mass_g\"]].reset_index(drop=True)\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\n\n\n\n\n0\nAdelie\nmale\n3750.0\n\n\n1\nAdelie\nfemale\n3800.0\n\n\n2\nAdelie\nfemale\n3250.0\n\n\n3\nAdelie\nNaN\nNaN\n\n\n4\nAdelie\nfemale\n3450.0\n\n\n\n\n\n\n\nI add the .reset_index(drop=True) to avoid some SettingWithCopyWarnings later. I also want to drop any observations with missing values so I use .dropna() to do that.\n\ndata = data.dropna()\n\nSuppose now I want to standardize the variable body_mass_g. Pandas vectorized operations make it extremely easy. One can save the result in a new column in a very intuitive way as well.\n\ndata[\"body_mass_z\"] = (data[\"body_mass_g\"] - data[\"body_mass_g\"].mean()) / data[\"body_mass_g\"].std()\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n0\nAdelie\nmale\n3750.0\n-0.567621\n\n\n1\nAdelie\nfemale\n3800.0\n-0.505525\n\n\n2\nAdelie\nfemale\n3250.0\n-1.188572\n\n\n4\nAdelie\nfemale\n3450.0\n-0.940192\n\n\n5\nAdelie\nmale\n3650.0\n-0.691811\n\n\n\n\n\n\n\nWe could consider it is more appropiate to standardize considering the species variable. We still perform the same operation than above, but we do it within each group.\nThe way to perform such operations in pandas is to use the .groupby() method. Then one can select the desired column and compute the aggregation.\nFor example, to compute the mean body mass by species we can do the following.\n\ndata.groupby(\"species\")[\"body_mass_g\"].mean()\n\nspecies\nAdelie       3706.164384\nChinstrap    3733.088235\nGentoo       5092.436975\nName: body_mass_g, dtype: float64\n\n\nWe obtained a pandas Series with three values, the mean for each species. If we want to obtain a Series of the same length than the original data, it is, for each row the mean of the species the observation belongs to, we can use .transform().\n\ndata.groupby(\"species\")[\"body_mass_g\"].transform(\"mean\")\n\n0      3706.164384\n1      3706.164384\n2      3706.164384\n4      3706.164384\n5      3706.164384\n          ...     \n339    3733.088235\n340    3733.088235\n341    3733.088235\n342    3733.088235\n343    3733.088235\nName: body_mass_g, Length: 333, dtype: float64\n\n\nThe same can be done with other transformations such as the standard deviation.\n\ndata.groupby(\"species\")[\"body_mass_g\"].transform(\"std\")\n\n0      458.620135\n1      458.620135\n2      458.620135\n4      458.620135\n5      458.620135\n          ...    \n339    384.335081\n340    384.335081\n341    384.335081\n342    384.335081\n343    384.335081\nName: body_mass_g, Length: 333, dtype: float64\n\n\nNow, putting all the pieces together, we can compute the standardized body mass by species.\n\ndata[\"body_mass_z\"] = (\n    (data[\"body_mass_g\"] - data.groupby(\"species\")[\"body_mass_g\"].transform(\"mean\")) \n    / data.groupby(\"species\")[\"body_mass_g\"].transform(\"std\")\n)\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n0\nAdelie\nmale\n3750.0\n0.095582\n\n\n1\nAdelie\nfemale\n3800.0\n0.204604\n\n\n2\nAdelie\nfemale\n3250.0\n-0.994645\n\n\n4\nAdelie\nfemale\n3450.0\n-0.558555\n\n\n5\nAdelie\nmale\n3650.0\n-0.122464\n\n\n\n\n\n\n\nIt‚Äôs also possible to add more variables to the groups. For example, this is how we can perform the same compution considering groups given by species and sex.\n\ndata[\"body_mass_z\"] = (\n    (data[\"body_mass_g\"] - data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"mean\")) \n    / data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"std\")\n)\ndata.head()\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n0\nAdelie\nmale\n3750.0\n-0.846261\n\n\n1\nAdelie\nfemale\n3800.0\n1.600580\n\n\n2\nAdelie\nfemale\n3250.0\n-0.441145\n\n\n4\nAdelie\nfemale\n3450.0\n0.301301\n\n\n5\nAdelie\nmale\n3650.0\n-1.134602\n\n\n\n\n\n\n\nAnd finally, I can sort by the standardized body mass in ascending order.\n\ndata.sort_values(\"body_mass_z\")\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n314\nChinstrap\nfemale\n2700.0\n-2.899080\n\n\n192\nGentoo\nfemale\n3950.0\n-2.591611\n\n\n195\nGentoo\nmale\n4750.0\n-2.346530\n\n\n298\nChinstrap\nfemale\n2900.0\n-2.198147\n\n\n119\nAdelie\nmale\n3325.0\n-2.071711\n\n\n...\n...\n...\n...\n...\n\n\n114\nAdelie\nfemale\n3900.0\n1.971803\n\n\n109\nAdelie\nmale\n4775.0\n2.109234\n\n\n284\nChinstrap\nfemale\n4150.0\n2.182685\n\n\n313\nChinstrap\nmale\n4800.0\n2.377631\n\n\n169\nGentoo\nmale\n6300.0\n2.603039\n\n\n\n\n333 rows √ó 4 columns"
  },
  {
    "objectID": "posts/2022-06-26_tidypolars/index.html#data-wrangling-with-tidypolars",
    "href": "posts/2022-06-26_tidypolars/index.html#data-wrangling-with-tidypolars",
    "title": "Let‚Äôs use tidypolars more",
    "section": "",
    "text": "Now it‚Äôs time to see the same operations performed with tidypolars. Tidypolars is inspired on the tidyverse, a set of packages following a consistent design phillosophy that has revolutionated the way we do data science in R and other languages as well.\nIts description says &gt; tidypolars is a data frame library built on top of the blazingly fast polars library that gives access to methods and functions familiar to R tidyverse users.\nTidypolars does not rely on any pandas data structure because it‚Äôs built on top of polars, not pandas. In addition, it works with a new data frame structure called Tibble, borrowing its name from the tibble in the R package of the same name.\nWe can convert a pandas DataFrame to a tibble with the .from_pandas() function.\n\ntibble = tp.from_pandas(palmerpenguins.load_penguins())\ntype(tibble)\n\ntidypolars.tibble.Tibble\n\n\nWe still have a .head() method that prints the first rows. The representation is very similar to a pandas data frame.\n\ntibble.head()\n\n\n\nshape: (5, 8)\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\nyear\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nstr\n\n\ni64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\nnull\n\n\nnull\n\n\nnull\n\n\nnull\n\n\nnull\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\n\n\nTo filter rows we can use the .filter() method. In the next chunk of code we use col(\"*\") to select all columns, tp.is_not_null() to flag observations with non-null values, and .filter() to use those booleans to actually perform the filtering.\nUpdate: See this issue where the main author of tidypolars lets me know that we can use the .drop_null() method instead.\n\ntibble.drop_null()\n\n\n\nshape: (333, 8)\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\nyear\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nstr\n\n\ni64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.3\n\n\n20.6\n\n\n190.0\n\n\n3650.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n38.9\n\n\n17.8\n\n\n181.0\n\n\n3625.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n39.2\n\n\n19.6\n\n\n195.0\n\n\n4675.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n41.1\n\n\n17.6\n\n\n182.0\n\n\n3200.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n38.6\n\n\n21.2\n\n\n191.0\n\n\n3800.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n34.6\n\n\n21.1\n\n\n198.0\n\n\n4400.0\n\n\n\"male\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n36.6\n\n\n17.8\n\n\n185.0\n\n\n3700.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n\"Adelie\"\n\n\n\"Torgersen\"\n\n\n38.7\n\n\n19.0\n\n\n195.0\n\n\n3450.0\n\n\n\"female\"\n\n\n2007\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n45.2\n\n\n16.6\n\n\n191.0\n\n\n3250.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n49.3\n\n\n19.9\n\n\n203.0\n\n\n4050.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n50.2\n\n\n18.8\n\n\n202.0\n\n\n3800.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n45.6\n\n\n19.4\n\n\n194.0\n\n\n3525.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n51.9\n\n\n19.5\n\n\n206.0\n\n\n3950.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n46.8\n\n\n16.5\n\n\n189.0\n\n\n3650.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n45.7\n\n\n17.0\n\n\n195.0\n\n\n3650.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n55.8\n\n\n19.8\n\n\n207.0\n\n\n4000.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n43.5\n\n\n18.1\n\n\n202.0\n\n\n3400.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n49.6\n\n\n18.2\n\n\n193.0\n\n\n3775.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n50.8\n\n\n19.0\n\n\n210.0\n\n\n4100.0\n\n\n\"male\"\n\n\n2009\n\n\n\n\n\"Chinstrap\"\n\n\n\"Dream\"\n\n\n50.2\n\n\n18.7\n\n\n198.0\n\n\n3775.0\n\n\n\"female\"\n\n\n2009\n\n\n\n\n\n\n\nIt‚Äôs very easy to select columns with the .select() method.\n\ntibble.select(\"species\", \"sex\", \"body_mass_g\")\n\n\n\nshape: (344, 3)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3800.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\nnull\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3625.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n3475.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n4250.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n3300.0\n\n\n\n\n\"Adelie\"\n\n\nnull\n\n\n3700.0\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4050.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3800.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3525.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3950.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4000.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3400.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3775.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4100.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3775.0\n\n\n\n\n\n\n\nWhat‚Äôs even better, we can chain these operations. This is where one can start seeing how powerful this approach is.\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n)\n\n\n\nshape: (333, 3)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3800.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3625.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3200.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3800.0\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4400.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3700.0\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3250.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4050.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3800.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3525.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3950.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3650.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4000.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3400.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3775.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4100.0\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n3775.0\n\n\n\n\n\n\n\nWe then have the .summarise() method to compute summaries (or aggregations) by groups. Note we use functions available in the tidypolars namespace such as tp.mean().\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .summarise(\n        body_mass_mean=tp.mean(\"body_mass_g\"),\n        by=\"species\"\n    )\n)\n\n\n\nshape: (3, 2)\n\n\n\n\nspecies\n\n\nbody_mass_mean\n\n\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"Adelie\"\n\n\n3706.164384\n\n\n\n\n\"Gentoo\"\n\n\n5092.436975\n\n\n\n\n\"Chinstrap\"\n\n\n3733.088235\n\n\n\n\n\n\n\nIf we want to get a behavior similar to .groupby() and .transform() in pandas, we can use another verb, mutate().\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_mean=tp.mean(\"body_mass_g\"),\n        by=\"species\"\n    )\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_mean\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4500.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4450.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5400.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4550.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4800.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5200.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4400.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5150.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4650.0\n\n\n5092.436975\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5550.0\n\n\n5092.436975\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3400.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3475.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3050.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3725.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3000.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4250.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3475.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3700.0\n\n\n3706.164384\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4000.0\n\n\n3706.164384\n\n\n\n\n\n\n\nThis verb is very powerful. We can compute more complex expressions, such as the one involved in the standardization of a variable.\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=(col(\"body_mass_g\") - tp.mean(\"body_mass_g\")) / tp.sd(\"body_mass_g\"),\n        by=\"species\"\n    )\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4500.0\n\n\n-1.181386\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n1.211549\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4450.0\n\n\n-1.281092\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5700.0\n\n\n1.211549\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5400.0\n\n\n0.613315\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4550.0\n\n\n-1.0816\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4800.0\n\n\n-0.583152\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5200.0\n\n\n0.214493\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4400.0\n\n\n-1.380797\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5150.0\n\n\n0.114787\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4650.0\n\n\n-0.882269\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n5550.0\n\n\n0.912432\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3400.0\n\n\n-0.667577\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3475.0\n\n\n-0.504043\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3050.0\n\n\n-1.430736\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3725.0\n\n\n0.041\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3000.0\n\n\n-1.539759\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3650.0\n\n\n-0.122464\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4250.0\n\n\n1.185808\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3475.0\n\n\n-0.504043\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3450.0\n\n\n-0.558555\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3750.0\n\n\n0.095582\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3700.0\n\n\n-0.013441\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4000.0\n\n\n0.640695\n\n\n\n\n\n\n\nAnd finally, we can use the .arrange() verb to sort observations by the standardized body mass in ascending order.\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=(col(\"body_mass_g\") - tp.mean(\"body_mass_g\")) / tp.sd(\"body_mass_g\"),\n        by=\"species\"\n    )\n    .arrange(\"body_mass_z\")\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2700.0\n\n\n-2.687988\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n3950.0\n\n\n-2.278148\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-2.167609\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4150.0\n\n\n-1.879326\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6050.0\n\n\n1.909489\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4500.0\n\n\n1.995425\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4650.0\n\n\n2.0579\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n2.112501\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4550.0\n\n\n2.1255\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4700.0\n\n\n2.167013\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4725.0\n\n\n2.221524\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4775.0\n\n\n2.330547\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6300.0\n\n\n2.408017\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4800.0\n\n\n2.775994\n\n\n\n\n\n\n\nWe can make it even clearer if we wrap the standardization operation within a function.\n\ndef standardize(name):\n    return (col(name) - tp.mean(name)) / tp.sd(name)\n\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=standardize(\"body_mass_g\"),\n        by=\"species\"\n    )\n    .arrange(\"body_mass_z\")\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2700.0\n\n\n-2.687988\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n3950.0\n\n\n-2.278148\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-2.167609\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4150.0\n\n\n-1.879326\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.866827\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4200.0\n\n\n-1.7796\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-1.757804\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6050.0\n\n\n1.909489\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4600.0\n\n\n1.948967\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4500.0\n\n\n1.995425\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4650.0\n\n\n2.0579\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n2.112501\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4550.0\n\n\n2.1255\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4700.0\n\n\n2.167013\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4725.0\n\n\n2.221524\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4775.0\n\n\n2.330547\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6300.0\n\n\n2.408017\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4800.0\n\n\n2.775994\n\n\n\n\n\n\n\nIf we want to carry on the .mutate() operation grouped by more than one variable, we can simply pass the names in a list."
  },
  {
    "objectID": "posts/2022-06-26_tidypolars/index.html#comparison",
    "href": "posts/2022-06-26_tidypolars/index.html#comparison",
    "title": "Let‚Äôs use tidypolars more",
    "section": "",
    "text": "Let‚Äôs see both approaches in action so we can better appreciate the differences.\n\n\n\ndata = palmerpenguins.load_penguins()\ndata = data[[\"species\", \"sex\", \"body_mass_g\"]].dropna().reset_index(drop=True)\ndata[\"body_mass_z\"] = (\n    (data[\"body_mass_g\"] - data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"mean\")) \n    / data.groupby([\"species\", \"sex\"])[\"body_mass_g\"].transform(\"std\")\n)\ndata.sort_values(\"body_mass_z\")\n\n\n\n\n\n\n\n\nspecies\nsex\nbody_mass_g\nbody_mass_z\n\n\n\n\n303\nChinstrap\nfemale\n2700.0\n-2.899080\n\n\n185\nGentoo\nfemale\n3950.0\n-2.591611\n\n\n188\nGentoo\nmale\n4750.0\n-2.346530\n\n\n287\nChinstrap\nfemale\n2900.0\n-2.198147\n\n\n113\nAdelie\nmale\n3325.0\n-2.071711\n\n\n...\n...\n...\n...\n...\n\n\n108\nAdelie\nfemale\n3900.0\n1.971803\n\n\n103\nAdelie\nmale\n4775.0\n2.109234\n\n\n273\nChinstrap\nfemale\n4150.0\n2.182685\n\n\n302\nChinstrap\nmale\n4800.0\n2.377631\n\n\n163\nGentoo\nmale\n6300.0\n2.603039\n\n\n\n\n333 rows √ó 4 columns\n\n\n\n\n\n\n\ndef standardize(name):\n    return (col(name) - tp.mean(name)) / tp.sd(name)\n\n\ntibble = tp.from_pandas(palmerpenguins.load_penguins())\n(\n    tibble\n    .drop_null()\n    .select(\"species\", \"sex\", \"body_mass_g\")\n    .mutate(\n        body_mass_z=standardize(\"body_mass_g\"),\n        by=[\"species\", \"sex\"]\n    )\n    .arrange(\"body_mass_z\")\n)\n\n\n\nshape: (333, 4)\n\n\n\n\nspecies\n\n\nsex\n\n\nbody_mass_g\n\n\nbody_mass_z\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2700.0\n\n\n-2.899\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n3950.0\n\n\n-2.591611\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n4750.0\n\n\n-2.3465\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n2900.0\n\n\n-2.198147\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3325.0\n\n\n-2.071711\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.926035\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n2850.0\n\n\n-1.926035\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3250.0\n\n\n-1.902511\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n4150.0\n\n\n-1.881329\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n4925.0\n\n\n-1.787708\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n3425.0\n\n\n-1.7833\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n3300.0\n\n\n-1.764442\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4650.0\n\n\n1.748808\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6050.0\n\n\n1.804721\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4675.0\n\n\n1.820893\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n5200.0\n\n\n1.847652\n\n\n\n\n\"Gentoo\"\n\n\n\"female\"\n\n\n5200.0\n\n\n1.847652\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4700.0\n\n\n1.892979\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4725.0\n\n\n1.965064\n\n\n\n\n\"Adelie\"\n\n\n\"female\"\n\n\n3900.0\n\n\n1.971803\n\n\n\n\n\"Adelie\"\n\n\n\"male\"\n\n\n4775.0\n\n\n2.109234\n\n\n\n\n\"Chinstrap\"\n\n\n\"female\"\n\n\n4150.0\n\n\n2.182685\n\n\n\n\n\"Chinstrap\"\n\n\n\"male\"\n\n\n4800.0\n\n\n2.377631\n\n\n\n\n\"Gentoo\"\n\n\n\"male\"\n\n\n6300.0\n\n\n2.603039\n\n\n\n\n\n\n\n\n\n\nPandas is great, so many people love it, and it will be the most used tool to work with tabular data in Python for a long time. In my case, it made me struggle a lot when I started to do data analysis in Python after working several years with R. Later, and fortunately, I began to understand how it worked and I became better at it.\nHowever, I never felt as comfortable with pandas as I do with dplyr syntax. Maybe it‚Äôs my R background? I don‚Äôt know. I just know that one of the things I enjoy the most in R is how easy and clear is to compose data manipulation operations with dplyr and friends. And having that expressiveness in Python is fantastic."
  },
  {
    "objectID": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html",
    "href": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html",
    "title": "Robust linear regression in Bambi",
    "section": "",
    "text": "The next thing in my TODO list for this Google Summer of Code season with NumFOCUS is to add new families of models to Bambi. This is still a WIP but I wanted to show you how to build a robust linear regression model using the Family class in Bambi.\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib.patches import Rectangle, FancyArrowPatch\nfrom scipy import stats"
  },
  {
    "objectID": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#what-do-we-mean-with-robust",
    "href": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#what-do-we-mean-with-robust",
    "title": "Robust linear regression in Bambi",
    "section": "What do we mean with robust?",
    "text": "What do we mean with robust?\nBefore showing how to build a robust regression with Bambi we need to be clear about what we mean when we say that a model is robust. Robust to what? How is linear regression non-robust?\nIn this post, we say a method is robust if its inferences aren‚Äôt (seriously) affected by the presence of outliers."
  },
  {
    "objectID": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#how-do-outliers-affect-linear-regression",
    "href": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#how-do-outliers-affect-linear-regression",
    "title": "Robust linear regression in Bambi",
    "section": "How do outliers affect linear regression?",
    "text": "How do outliers affect linear regression?\nI think it will be easier to understand how outliers affect linear regressions via an example based on the least squares method. This is not exactly how linear regression works in our Bayesian world, but outlier‚Äôs bad consequences are similar.\nIn classic statistics, linear regression models are usually fitted by ordinary least-squares method. This is equivalent to assuming the conditional distribution of the response given the predictors is normal (i.e.¬†\\(y_i|\\boldsymbol{X}_i \\sim N(\\mu_i, \\sigma)\\)) and using the maximum likelihood estimator.\nLet‚Äôs get started by simulating some toy data.\n\nx = np.array([1., 2., 4., 5.])\ny = np.array([1.25, 1.45, 4.75, 4.8])\n\nThen, fit a linear regression between and visualize the result.\nThe next plot shows the data, the fitted line, and the contribution of each data point to the total (squared) error as a blue square (one way to see the least squares method is as the method that minimizes the sum of the areas of the squares associated to all the points).\n\nb, a = np.polyfit(x, y, 1)\ny_hat = a + b * x\nresidual = y_hat - y\n\n\narrowstyle = \"Simple, tail_width=0.3, head_width=4, head_length=4\"\nconnectiontyle = \"arc3, rad=0.4\"\narrowstyles = {\"color\": \"0.2\", \"arrowstyle\": arrowstyle, \"connectionstyle\": connectiontyle}\n\nfig, ax = plt.subplots(figsize=(6, 6), dpi=120)\nfig.set_facecolor(\"w\")\nax.set_xlim(0.25, 6)\nax.set_ylim(0.25, 6)\n\nax.scatter(x, y, s=50, ec=\"k\")\nax.plot(x, y_hat, color=\"0.2\", lw=2.5)\n\n# Add rectangles\nfor xy, r in zip(zip(x, y ), residual):\n    ax.add_patch(Rectangle(xy, abs(r), r, alpha=0.7, ec=\"k\", zorder=-1, lw=1))\n\n# Add arrows\nx_end = x + residual * np.array([0, 1, 0, 1])\nx_start = x_end + np.array([-0.3, 0.4, -0.4, 0.3])\ny_end = y + residual / 2\ny_start = y\ny_text = y_end + np.array([0.2, -0.45, 0.35, -0.3])\n\nfor xy0, xy1, r, yt in zip(zip(x_start, y_start), zip(x_end, y_end), residual, y_text):\n    ax.add_patch(FancyArrowPatch(xy0, xy1, **arrowstyles))\n    ax.text(xy0[0], yt, str(round(abs(r ** 2), 4)), ha=\"center\")\n\nax.text(\n    0, 1.01, f\"Sum of squares: {round(np.sum(residual ** 2), 4)}\", \n    size=12, transform=ax.transAxes, va=\"baseline\"\n);\n\n\n\n\n\n\n\n\nSo far so good! It looks like the fitted line is a good representation of the relationship between the variables.\nWhat happens if we introduce an outlier? In other words, what happens if there‚Äôs a new point that deviates too much from the pattern we‚Äôve just seen above? Let‚Äôs see it!\n\nx = np.insert(x, 2, 2.25)\ny = np.insert(y, 2, 5.8)\n\n\nb, a = np.polyfit(x, y, 1)\ny_hat = a + b * x\nresidual = y_hat - y\n\n\nfig, ax = plt.subplots(figsize=(6, 6), dpi=120)\nfig.set_facecolor(\"w\")\nax.set_xlim(0, 6.5)\nax.set_ylim(0, 6.5)\n\nax.scatter(x, y, s=50, ec=\"k\")\nax.plot(x, y_hat, color=\"0.2\", lw=2.5)\n\n# Add rectangles\nfor xy, r in zip(zip(x, y ), residual):\n    ax.add_patch(Rectangle(xy, abs(r), r, alpha=0.7, ec=\"k\", zorder=-1, lw=1))\n\n# Add arrows\nx_end = x + np.abs(residual) * np.array([0, 1, 0, 1, 1])\nx_start = x_end + np.array([-0.4, 0.4, -0.5, 0.3, 0.3])\ny_end = y + residual / 2\ny_start = y + np.array([0.8, 0.4, -0.8, -0.4, 0])\ny_text = y_start + np.array([0.1, -0.1, 0.1, -0.1, -0.1])\n\nfor xy0, xy1, r, yt in zip(zip(x_start, y_start), zip(x_end, y_end), residual, y_text):\n    ax.add_patch(FancyArrowPatch(xy0, xy1, **arrowstyles))\n    ax.text(xy0[0], yt, str(round(abs(r ** 2), 4)), ha=\"center\", va=\"center\")\n\nax.text(\n    0, 1.01, f\"Sum of squares: {round(np.sum(residual ** 2), 4)}\", \n    size=12, transform=ax.transAxes, va=\"baseline\"\n);\n\n\n\n\n\n\n\n\nWhat a bummer! Why do we have such a huge error? It‚Äôs 10 times the previous error with only one extra data point! Why?!\nIt happens that each point‚Äôs contribution to the error grows quadratically as it moves away from the rest. Outliers not only contribute a lot to the total error, they also bias the estimation towards themselves, increasing the error associated with other points too. The final result? the fitted line is not a faithful representation of the relationship between the variables."
  },
  {
    "objectID": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#linear-regression-in-a-bayesian-way",
    "href": "posts/2021-07-05_robust-linear-regression-with-Bambi/index.html#linear-regression-in-a-bayesian-way",
    "title": "Robust linear regression in Bambi",
    "section": "Linear regression in a Bayesian way",
    "text": "Linear regression in a Bayesian way\nNow that we‚Äôve seen how bad outliers can be above, let‚Äôs see how one can robust a Bayesian linear regression. This part of the post is based on the Robust Linear Regression in PyMC3 docs.\nHere, we simulate data suitable for a normal linear regression and contaminate it with a few outliers.\n\nsize = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ntrue_regression_line = true_intercept + true_slope * x\ny = true_regression_line + np.random.normal(scale=0.5, size=size)\n\nx_out = np.append(x, [0.1, 0.15, 0.2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = pd.DataFrame(dict(x = x_out, y = y_out))\n\n\nfig, ax = plt.subplots(figsize=(10, 7), dpi=120)\nfig.set_facecolor(\"w\")\n\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7);\n\n\n\n\n\n\n\n\n\nNormal linear regression\nThe normal linear regression is as follows\n\\[\ny_i \\sim \\text{Normal}(\\mu_i, \\sigma)\n\\]\nwhere \\(\\mu_i = \\beta_0 + \\beta_1 x_i\\), and the priors are of the form\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal} \\\\\n\\beta_1 \\sim \\text{Normal}  \\\\\n\\sigma \\sim \\text{HalfStudentT}\n\\end{array}\n\\]\nwith their parameters automatically set by Bambi.\n\nmodel = bmb.Model(\"y ~ x\", data=data)\nidata = model.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_sigma, x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:01&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\n\n\nTo evaluate the fit, we use the posterior predictive regression lines. The line in black is the true regression line.\n\n# Prepare data\nx = np.linspace(0, 1, num=200)\nposterior_stacked = idata.posterior.stack(samples=(\"chain\", \"draw\"))\nintercepts = posterior_stacked[\"Intercept\"].values\nslopes = posterior_stacked[\"x\"].values\n\n# Create plot\nfig, ax = plt.subplots(figsize=(10, 7), dpi=120)\n\n# Data points\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7)\n\n# Posterior regression lines\nfor a, b in zip(intercepts, slopes):\n    ax.plot(x, a + b * x, color =\"0.5\", alpha=0.3, zorder=-1)\n\n# True regression line\nax.plot(x, true_intercept + true_slope * x, color=\"k\", lw=2);\n\n\n\n\n\n\n\n\nAs you can see, the posterior distribution fo the regression lines is not centered around the true regression line, which means the estimations are highly biased. This is the same phenomena we saw above with the least-squares toy example.\nWhy does it happen here? The reason is that the normal distribution does not have a lot of mass in the tails and consequently, an outlier will affect the fit strongly.\nSince the problem is the light tails of the Normal distribution we can instead assume that our data is not normally distributed but instead distributed according to the Student T distribution which has heavier tails as shown next.\n\n\nNormal and Student-T distributions\nHere we plot the pdf of a standard normal distribution and the pdf of a student-t distribution with 3 degrees of freedom.\n\nx = np.linspace(-8, 8, num=400)\ny_normal = stats.norm.pdf(x)\ny_t = stats.t.pdf(x, df = 3)\n\n\nfig, ax = plt.subplots(figsize=(10, 6), dpi=120)\nfig.set_facecolor(\"w\")\nax.set_ylim(0, 0.41)\n\nax.plot(x, y_normal, lw=2)\nax.plot(x, y_t, lw=2)\n\nax.add_patch(FancyArrowPatch((-3, 0.36), (x[180], y_normal[180]), **arrowstyles, zorder=1))\nax.add_patch(FancyArrowPatch((3, 0.31), (x[205], y_t[205]), **arrowstyles, zorder=1))\n\nax.text(-3, 0.37, \"Normal\", size=13, ha=\"center\", va=\"center\")\nax.text(3, 0.30, \"Student's T\", size=13, ha=\"center\", va=\"center\");\n\n\n\n\n\n\n\n\nAs you can see, the probability of values far away from the mean are much more likely under the Student-T distribution than under the Normal distribution.\n\n\nRobust linear regression\nThe difference with the model above is that this one uses a StudentT likelihood instead of a Normal one.\nBambi does not support yet to use the student-t distribution as the likelihood function for linear regression. However, we can construct our own custom family and Bambi will understand how to work with it.\nCustom families are represented by the Family class in Bambi. Let‚Äôs see what we need to create a custom family.\nFirst of all, we need a name. In this case the name is going to be just \"t\". Second, there is the likelihood function. This is represented by an object of class Likelihood in Bambi. To define a likelihood function we need the following:\n\nThe name of the distribution in PyMC3. In this case, it is \"StudentT\".\nThe name of the parent parameter (the mean). It is \"mu\".\nThe prior distributions for the auxiliary parameters in the distribution. These are nu and sigma in the StudentT distribution.\n\nFinally, we pass the link function. This can be a string or an object of class Link. In this case it‚Äôs simply the identity function, which can be passed as a string.\n\n# Construct likelihood\nnu = bmb.Prior(\"Gamma\", alpha=3, beta=1)\nsigma = bmb.Prior(\"HalfStudentT\", nu=4, sigma=1)\nlikelihood = bmb.Likelihood(name=\"StudentT\", parent=\"mu\", sigma=sigma, nu=nu)\n\n# Construct family\nt_family = bmb.Family(name = \"t\", likelihood = likelihood, link = \"identity\")\n\n# In addition, we pass our custom priors for the terms in the model.\npriors = {\n  \"Intercept\": bmb.Prior(\"Normal\", mu=2, sigma=5),\n  \"x\": bmb.Prior(\"Normal\", mu=0, sigma=10)\n}\n\n# Just add the `prior` and `family` arguments\nmodel = bmb.Model(\"y ~ x\", data, priors=priors, family=t_family)\nidata = model.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_nu, y_sigma, x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:02&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 2 seconds.\n\n\n\n# Prepare data\nx = np.linspace(0, 1, num=200)\nposterior_stacked = idata.posterior.stack(samples=(\"chain\", \"draw\"))\nintercepts = posterior_stacked[\"Intercept\"].values\nslopes = posterior_stacked[\"x\"].values\n\n# Create plot\nfig, ax = plt.subplots(figsize=(10, 7), dpi=120)\n\n# Data points\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7)\n\n# Posterior regression lines\nfor a, b in zip(intercepts, slopes):\n    ax.plot(x, a + b * x, color =\"0.5\", alpha=0.3, zorder=-1)\n\n# True regression line\nax.plot(x, true_intercept + true_slope * x, color=\"k\", lw=2);\n\n\n\n\n\n\n\n\nMuch better now! The posterior distribution of the regression lines is almost centered around the true regression line, and uncertainty has decreased, that‚Äôs great! The outliers are barely influencing our estimation because our likelihood function assumes that outliers are much more probable than under the Normal distribution."
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html",
    "title": "Binomial family in Bambi",
    "section": "",
    "text": "Although GSoC 2021 is close to come to an end, there‚Äôs still a lot of exciting things going on around Bambi. Today I‚Äôm going to talk about another new family that‚Äôs about to be merged into the main branch, the Binomial family.\nLet‚Äôs get started by trying to see why we need to have another new family for modeling binary data in Bambi."
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#introduction",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#introduction",
    "title": "Binomial family in Bambi",
    "section": "",
    "text": "Although GSoC 2021 is close to come to an end, there‚Äôs still a lot of exciting things going on around Bambi. Today I‚Äôm going to talk about another new family that‚Äôs about to be merged into the main branch, the Binomial family.\nLet‚Äôs get started by trying to see why we need to have another new family for modeling binary data in Bambi."
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#aggregated-vs-disaggregated-data",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#aggregated-vs-disaggregated-data",
    "title": "Binomial family in Bambi",
    "section": "Aggregated vs disaggregated data",
    "text": "Aggregated vs disaggregated data\nBambi already has the Bernoulli family to model binary data. This family fits very well when you have a data set where each row represents a single observation and there‚Äôs a column that represents the binary outcome ( i.e the result of the Bernoulli trial) as well as other columns with the predictor variables.\nLet‚Äôs say we want to study the lethality of a certain drug and we have a group of mice to experiment with. An approach could be to divide the mice into smaller groups, assign a certain dose to all the mice in each group, and then finally count the number of units that died after a fixed amount of time. Under the Bernoulli family paradigm, each row has to represent a single observation, looking like this:\n\n\n\n\nObs\nDose\nDied\n\n\n\n\n1\n1.3\n0\n\n\n2\n1.8\n1\n\n\n3\n2.2\n1\n\n\n\n\nwhere each row represents a single mouse (i.e.¬†a single Bernoulli trial). The 0 is used to represent a failure/survival, and 1 is used to represent a successes/death.\nWhat if our data is aggregated? The nature of the experiment makes it natural to have rows representing groups, a column representing the number of deaths, and another column representing the number of mice in the group.\n\n\n\n\nGroup\nDose\nDead\nTotal\n\n\n\n\n1\n1.3\n12\n20\n\n\n2\n1.8\n18\n25\n\n\n3\n2.2\n24\n34\n\n\n\n\nwhere each row represents a group of mice. Dose is the dose applied to all the units in the group, Dead is the number of mice that died, and Total is the number of mice in the group. If we focus on the Dead and Total columns we can easily see they resemble data coming from a Binomial distribution (i.e.¬†number of successes out of a series of \\(n\\) independent Bernoulli trials). In other words, for a given row, we can think there‚Äôs a Binomial distribution where Dead represents the number of successes out of Total number of trials (each mouse is a trial).\nBefore continuing, it‚Äôs important to note that if the data is originally aggregated as in the lower table, it can always be disaggregated to resemble the one in the upper table. So what‚Äôs the problem?\nThe answer is that there‚Äôs actually nothing wrong with having the data in such a granular form! But, if the data already comes aggregated, why doing extra work when we now have the Binomial family? Let‚Äôs have a look at the examples below!\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\naz.style.use(\"arviz-darkgrid\")\nBLUE = \"#003f5c\"\nPURPLE = \"#7a5195\"\nPINK = \"#ef5675\"\n\nWe‚Äôre going to use real data in this example1. This data consists of the numbers of beetles dead after five hours of exposure to gaseous carbon disulphide at various concentrations:\n\n\n\n\n\n\n\n\n\nDose, \\(x_i\\) (\\(\\log_{10}\\text{CS}_2\\text{mgl}^{-1}\\))\nNumber of beetles, \\(n_i\\)\nNumber killed, \\(y_i\\)\n\n\n\n\n1.6907\n59\n6\n\n\n1.7242\n60\n13\n\n\n1.7552\n62\n18\n\n\n1.7842\n56\n28\n\n\n1.8113\n63\n52\n\n\n1.8369\n59\n53\n\n\n1.8610\n62\n61\n\n\n1.8839\n60\n60\n\n\n\n\n\nx = np.array([1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839])\nn = np.array([59, 60, 62, 56, 63, 59, 62, 60])\ny = np.array([6, 13, 18, 28, 52, 53, 61, 60])\n\ndata = pd.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"n\": n\n})\n\nQuite simple, right? Can we use it as it is with the Bernoulli family? Let‚Äôs have a look below."
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#bernoulli-family",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#bernoulli-family",
    "title": "Binomial family in Bambi",
    "section": "Bernoulli family",
    "text": "Bernoulli family\nNope, no surprises today. To use the Bernoulli family, we first need to transform the data into the dissagregated or long format. One approach is the following\n\ndata_bernoulli = pd.DataFrame({\n    \"x\": np.concatenate([np.repeat(x, n) for x, n in zip(x, n)]),\n    \"killed\": np.concatenate([np.repeat([1, 0], [y, n - y]) for y, n in zip(y, n)])\n})\n\nDo you realize how bothering it can be to do that if we have many more variables? Nevermind, let‚Äôs keep going.\nNow let‚Äôs initialize a Bambi model and sample from the posterior:\n\nmodel_brn = bmb.Model(\"killed ~ x\", data_bernoulli, family=\"bernoulli\")\nidata_brn = model_brn.fit()\n\nModeling the probability that killed==1\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:03&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds.\n\n\nand explore the marginal posteriors\n\naz.summary(idata_brn, kind=\"stats\")\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\nIntercept\n-61.021\n5.250\n-70.606\n-51.311\n\n\nx\n34.443\n2.954\n28.933\n39.745\n\n\n\n\n\n\n\nWe can predict the the probability of dying for out-of-sample data to see how it evolves with the different concentration levels.\n\nnew_data = pd.DataFrame({\"x\": np.linspace(1.6, 2, num=200)})\nmodel_brn.predict(idata_brn, data=new_data)\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\n\n# Plot HDI for the mean of the probability of dying\naz.plot_hdi(\n  new_data[\"x\"], \n  idata_brn.posterior[\"killed_mean\"].values, \n  color=BLUE,\n  ax=ax\n)\n\nax.plot(\n  new_data[\"x\"], \n  idata_brn.posterior[\"killed_mean\"].values.mean((0, 1)), \n  color=BLUE\n)\n\nax.scatter(x, y / n, s=50, color=PURPLE, edgecolors=\"black\", zorder=10)\nax.set_ylabel(\"Probability of death\")\nax.set_xlabel(r\"Dose $\\log_{10}CS_2mgl^{-1}$\")\nax.set_title(\"family='bernoulli'\")\n\nplt.show()"
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#binomial-family",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#binomial-family",
    "title": "Binomial family in Bambi",
    "section": "Binomial family",
    "text": "Binomial family\nBefore writing down the model with the Binomial family, let‚Äôs take a moment to review new notation that was added specifically for this purpose.\nThe model formula syntax only allows us to pass one variable on its LHS. Then, how do we tell Bambi that what we want to model is the proportion that results from dividing y over n?\nThanks to recent developments, it‚Äôs as easy as writing proportion(y, n), or any of its aliases prop(y, n) and p(y, n). To keep it shorter, let‚Äôs use the last one.\n\nmodel_bnml = bmb.Model(\"p(y, n) ~ x\", data, family=\"binomial\")\nidata_bnml = model_bnml.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:03&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\n\n\nQuite simple, right? The code here is very similar to the one for the model with the Bernoulli family. However, the new Binomial family allows us to use the data in its original form.\nLet‚Äôs finish this section by getting the marginal posteriors as well as a figure as the one displayed above.\n\naz.summary(idata_bnml, kind=\"stats\")\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\nIntercept\n-61.045\n4.969\n-69.905\n-51.495\n\n\nx\n34.452\n2.793\n29.185\n39.552\n\n\n\n\n\n\n\n\nmodel_bnml.predict(idata_bnml, data=new_data)\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\n\naz.plot_hdi(\n  new_data[\"x\"],\n  idata_bnml.posterior[\"p(y, n)_mean\"].values,\n  color=BLUE,\n  ax=ax\n)\n\nax.plot(\n  new_data[\"x\"], \n  idata_bnml.posterior[\"p(y, n)_mean\"].values.mean((0, 1)), \n  color=BLUE\n)\n\nax.scatter(x, y / n, s=50, color=PURPLE, edgecolors=\"black\", zorder=10)\nax.set_ylabel(\"Probability of death\")\nax.set_xlabel(r\"Dose $\\log_{10}CS_2mgl^{-1}$\")\nax.set_title(\"family='binomial'\")\nfig.set_facecolor(\"w\")\nfig.savefig(\"imgs/plot.png\")\nplt.show()"
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#conclusions",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#conclusions",
    "title": "Binomial family in Bambi",
    "section": "Conclusions",
    "text": "Conclusions\nThis blog post introduced the new Binomial family. This new family saves us from having to manipulate aggregated data prior to modeling, making it more pleasant and simpler to specify and fit models for binary data in Bambi."
  },
  {
    "objectID": "posts/2021-08-03_binomial-family-in-Bambi/index.html#footnotes",
    "href": "posts/2021-08-03_binomial-family-in-Bambi/index.html#footnotes",
    "title": "Binomial family in Bambi",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis data can be found in An Introduction to Generalized Linear Models by A. J. Dobson and A. G. Barnett, but the original source is (Bliss, 1935).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html",
    "title": "How to create a custom family in Bambi?",
    "section": "",
    "text": "Bambi is the project I dedicate the vast majority of my open-source development time. My goal is to make it a little better every time I push to the main branch. Lately I‚Äôve been working to expand the class of models that Bambi supports. Until now, Bambi supported Generalized Linear Mixed Models (GLMMs). After the latest changes1 Bambi supports a wider class known as Generalized Linear Mixed Models for Location, Scale, and Shape. I also like the terminology used in brms, Distributional Models.\nThe new additions to the library allow us to work with more flexible custom model families in an easier way. The question is: How to create a custom family in Bambi?\nüö® You don‚Äôt have time or simply don‚Äôt want to read the whole thing? Here‚Äôs your TL;DR"
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html#imports",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html#imports",
    "title": "How to create a custom family in Bambi?",
    "section": "Imports",
    "text": "Imports\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\n\nfrom bambi.plots import plot_cap\nfrom matplotlib.lines import Line2D\nfrom scipy.special import expit"
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html#the-zero-inflated-poisson-distribution",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html#the-zero-inflated-poisson-distribution",
    "title": "How to create a custom family in Bambi?",
    "section": "The Zero-Inflated Poisson distribution",
    "text": "The Zero-Inflated Poisson distribution\nI‚Äôm going to use the Zero-Inflated Poisson distribution to demonstrate how to add a custom family in Bambi. This distribution applies to random variables that show an excess of zeros when describing the number of events that occur in a certain space or time period.\nQuoting Wikipedia 2 ‚ÄúThe zero-inflated Poisson (ZIP) model mixes two zero generating processes. The first process generates zeros. The second process is governed by a Poisson distribution that generates counts, some of which may be zero‚Äù. The result is a mixture distribution that can be described as follows:\n\\[\n\\begin{array}{ll}\nP(Y = 0) = (1 - \\psi) + \\psi e^{-\\mu} \\\\\nP(Y = y_i) = \\displaystyle \\psi \\frac{e^{-\\mu}\\mu^y_i}{y_i!} & \\text{with } y_i=1,2,3,\\ldots\n\\end{array}\n\\]\nWhere\n\n\\(y_i\\) is the outcome, \\(y_i \\in \\mathbb{Z}^+\\)\n\\(\\mu\\) is the mean of the Poisson process, \\(\\mu \\ge 0\\)\n\\(\\psi\\) is the probability of the Poisson process, \\(0 &lt; \\psi &lt; 1\\)\n\nThe mean is \\(\\psi\\mu\\) and the variance is \\(\\displaystyle \\mu + \\frac{1-\\psi}{\\psi}\\mu^2\\)."
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html#the-zip-regression-model",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html#the-zip-regression-model",
    "title": "How to create a custom family in Bambi?",
    "section": "The ZIP Regression model",
    "text": "The ZIP Regression model\nWe have a response variable \\(Y\\), which represents the number of events that occur in a certain space or time period, and \\(p\\) predictors \\(X_1, \\cdots, X_p\\). In the most general formulation of the model we consider, a function of the parameters in the response distribution is given by a linear combination of the predictors.\n\\[\n\\begin{aligned}\nY_i &\\sim \\text{ZIPoisson}(\\mu_i, \\psi_i) \\\\\ng(\\mu_i) &= \\beta_0 + \\beta_1 X_{1i} + ... + \\beta_p X_{pi}\\\\\nh(\\psi_i) &= \\alpha_0 + \\alpha_1 X_{1i} + ... + \\alpha_p X_{pi}\n\\end{aligned}\n\\]\n\n\\(g\\) is the link function for the \\(\\mu\\) parameter\n\\(h\\) is the link function for the \\(\\psi\\) parameter\n\nIt‚Äôs possible to see this model written as\n\\[\n\\begin{aligned}\nY_i &\\sim \\text{ZIPoisson}(\\mu_i, \\psi_i) \\\\\n\\mu_i &= g^{-1}(\\beta_0 + \\beta_1 X_{1i} + ...) \\\\\n\\psi_i &= h^{-1}(\\alpha_0 + \\alpha_1 X_{1i} + ...)\n\\end{aligned}\n\\]\nwhere the requirement that \\(g\\) and \\(h\\) are both invertible becomes evident."
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html#create-the-zeroinflatedpoisson-family",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html#create-the-zeroinflatedpoisson-family",
    "title": "How to create a custom family in Bambi?",
    "section": "Create the ZeroInflatedPoisson Family",
    "text": "Create the ZeroInflatedPoisson Family\nIt comes one of the most important parts of the blogpost. If you care about creating new model families, this is what you need. Spoiler: it‚Äôs very easy.\nTo create a model family, we first need to understand what is a family. A family is an entity that is defined by the combination of two objects\n\nA likelihood function ‚Äì the probability distribution for the response variable\nA link function for the parameters of the likelihood function.\n\nEach of these three objects have their own class in Bambi. Families are created with the Family class, likelihood functions are created with the Likelihood class, and for link functions, well, we have Link.\n\nCreate the likelihood\nLet‚Äôs get started with the likelihood function. Here we need:\n\nThe name of the likelihood, which must be a valid PyMC distribution 3\nThe names of the parameters in the PyMC distribution\nThe identification of a parent or main parameter. In regression settings this is usually the mean\n\nIn our case, we want to use the ZeroInflatedPoisson distribution from PyMC, which has parameters mu and psi, and mu is the parent parameter.\nlikelihood = bmb.Likelihood(\"ZeroInflatedPoisson\", params=[\"mu\", \"psi\"], parent=\"mu\")\nThat‚Äôs it! We have our custom likelihood function.\n\n\nCreate the link\nBefore telling Bambi which link functions we want for the parameters, we need to choose them. I‚Äôm going to use the logarithm function for \\(\\mu\\) and the logit function for \\(\\psi\\). In the original formulation of the model we replace \\(g\\) with \\(\\text{log}\\) and \\(h\\) with \\(\\text{logit}\\).\n\\[\n\\begin{aligned}\nY_i &\\sim \\text{ZIPoisson}(\\mu_i, \\psi_i) \\\\\n\\text{log}(\\mu_i) &= \\beta_0 + \\beta_1 X_{1i} + ... + \\beta_p X_{pi} \\\\\n\\text{logit}(\\psi_i) &= \\alpha_0 + \\alpha_1 X_{1i} + ... + \\alpha_p X_{pi}\n\\end{aligned}\n\\]\nNext, we define them in code. Here we could use bmb.Link, but it‚Äôs not needed in our case because both the \\(\\text{log}\\) and \\(\\text{logit}\\) link functions are already implemented in Bambi. We just need to pass their names and Bambi will know how to handle it 4. We create a dictionary where the keys are the names of the parameters and the values are the link functions.\nlinks = {\"mu\": \"log\", \"psi\": \"logit\"}\n\n\nCreate the family\nFinally we can put the pieces together to create the family. Here we have all the code again so we see how simple it is. Notice we need to give the family a name. We can pass whatever we want here.\n\nlikelihood = bmb.Likelihood(\"ZeroInflatedPoisson\", params=[\"mu\", \"psi\"], parent=\"mu\")\nlinks = {\"mu\": \"log\", \"psi\": \"logit\"}\nzip_family = bmb.Family(\"zip\", likelihood, links)\nzip_family\n\nFamily: zip\nLikelihood: Likelihood(  \n  name: ZeroInflatedPoisson,\n  params: ['mu', 'psi'],\n  parent: mu\n)\nLink: {'mu': Link(  \n  name: log,\n  link: &lt;ufunc 'log'&gt;,\n  linkinv: &lt;ufunc 'exp'&gt;\n), 'psi': Link(  \n  name: logit,\n  link: &lt;function logit at 0x7f2fecbb7880&gt;,\n  linkinv: &lt;function expit at 0x7f2fecbb77f0&gt;\n)}\n\n\nWhen we print the family we get information about the name, the likelihood, and the links. It contains many details for the link functions, but they‚Äôre not important now 5.\nNow that we have our brand new custom family, it‚Äôs time to test it!"
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html#the-simplest-case",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html#the-simplest-case",
    "title": "How to create a custom family in Bambi?",
    "section": "The simplest case",
    "text": "The simplest case\nWe need some data to use our new family. How can we get it? Simulation!\nHere we consider the case where we obtain samples from a single ZIPoisson distribution. There are no predictors that allow us to distinguish observations. Both \\(\\mu\\) and \\(\\psi\\) are the same for all observations.\n\\[\n\\begin{aligned}\nY_i &\\sim \\text{ZIPoisson}(\\mu, \\psi) \\\\\n\\mu &\\sim \\text{Some prior} \\\\\n\\psi &\\sim \\text{Some prior}\n\\end{aligned}\n\\]\n\nSimulate the data\nTo simulate draws from this ZIPoisson distribution we can use plain NumPy in a very a manual way, simply concatenate a bunch of zeros to an array with draws from a Poisson distribution and we‚Äôre done.\n\nrng = np.random.default_rng(121195)\nx = np.concatenate([np.zeros(250), rng.poisson(lam=3, size=750)])\ndf = pd.DataFrame({\"response\": x})\n\nLet‚Äôs explore the data now. A simple barchart is enough to get the information we need.\n\n\n\n\n\n\n\n\n\nDefinetely, it looks like a Poisson distribution but the bar at zero is unusually high, indicating excess of zeros or zero-inflation.\n\n\nBuild and fit the model\nWithout much preamble we‚Äôre going to build the model. In this basic model we don‚Äôt have predictors for any of the parameters. This means we specify an intercept-only model for \\(\\mu\\), and a prior distribution for \\(\\psi\\). Here I use a Beta distribution that guarantees the values of \\(\\psi\\) are bounded between 0 and 1.\n\npriors = {\"psi\": bmb.Prior(\"Beta\", alpha=3, beta=3)}\nmodel = bmb.Model(\"response ~ 1\", df, family=zip_family, priors=priors)\nmodel\n\n       Formula: response ~ 1\n        Family: zip\n          Link: mu = log\n  Observations: 1000\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 0.0, sigma: 2.5)\n        \n        Auxiliary parameters\n            response_psi ~ Beta(alpha: 3.0, beta: 3.0)\n\n\nAnd now‚Ä¶ let‚Äôs get the sampler running!\n\nidata = model.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [response_psi, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:05&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 6 seconds.\n\n\nIt worked fast, without warnings or errors, I would say that‚Äôs a great start. The next question is: does the fit make sense? I‚Äôm not going to investigate it thoroughly now. A visualization of the posterior predictive distribution will be enough.\n\nmodel.predict(idata, kind=\"pps\") # get draws from the posterior predictive distribution\naz.plot_ppc(idata);\n\n\n\n\n\n\n\n\nAwesome! Feels like magic. We created a custom family and it just‚Ä¶ worked."
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html#a-second-example-now-with-a-predictor",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html#a-second-example-now-with-a-predictor",
    "title": "How to create a custom family in Bambi?",
    "section": "A second example, now with a predictor",
    "text": "A second example, now with a predictor\nLet‚Äôs keep testing our zip family. Now we‚Äôre going to simulate data again. This time we have a numerical predictor that is related with both \\(\\mu\\) and \\(\\psi\\).\nThe model is\n\\[\n\\begin{aligned}\nY_i &\\sim \\text{ZIPoisson}(\\mu_i, \\psi_i) \\\\\n\\text{log}(\\mu_i) &= \\beta_0 + \\beta_1 X_{1i} \\\\\n\\text{logit}(\\psi_i) &= \\alpha_0 + \\alpha_1 X_{1i}\n\\end{aligned}\n\\]\nbut we first need the data‚Ä¶\n\nSimulate the data\nThis time we use NumPy to generate random values for the predictor, but we use pm.draw() to get draws from a ZeroInflatedPoisson distribution.\n\nrng = np.random.default_rng(121195)\nx = np.sort(rng.uniform(0.2, 3, size=1000))\n\nb0, b1 = 0.2, 0.9\na0, a1 = 2.5, -0.7\nmu = np.exp(b0 + b1 * x)\npsi = expit(a0 + a1 * x)\n\ny = pm.draw(pm.ZeroInflatedPoisson.dist(mu=mu, psi=psi))\ndf = pd.DataFrame({\"y\": y, \"x\": x})\n\nVisualizations are the key to understand how the predictor is associated with the parameters of the likelihood.\n\n\n\n\n\n\n\n\n\nHere we have two main takeaways\n\nThe mean of the Poisson process increases with the value of the predictor\nThe importance of the Poisson process in the mixture decreases with the value of the predictor\n\nFinally, why not displaying the observations in a scatterplot of the predictor versus the response.\n\n\n\n\n\n\n\n\n\nSee how the response values tend to increase as the predictor is larger, but also it tends to be more observations with a zero count.\n\n\nBuild and fit the model (again)\nIt‚Äôs time to define the new model. Since we model both \\(\\mu\\) and \\(\\psi\\) components as a function of the predictor, they both need a model formula. This is when we use bmb.Formula(). The first formula is the one for the parent parameter, which is \\(\\mu\\), and the following formulas are for the non-parent parameters. In this case, we only have \\(\\psi\\). Finally notice we don‚Äôt need to create the family again, we just reuse it.\n\nformula = bmb.Formula(\"y ~ x\", \"psi ~ x\")\nmodel = bmb.Model(formula, df, family=zip_family)\nmodel\n\n       Formula: y ~ x\n                psi ~ x\n        Family: zip\n          Link: mu = log\n                psi = logit\n  Observations: 1000\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 0.0, sigma: 5.7176)\n            x ~ Normal(mu: 0.0, sigma: 3.0682)\n    target = psi\n        Common-level effects\n            psi_Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n            psi_x ~ Normal(mu: 0.0, sigma: 1.0)\n\n\nThe model summary contains so much information. It shows the priors for the parameters in the linear predictors of both parameters in different sections. First we have the priors for mu below target = mu, and then the priors for psi below target = psi.\nIt‚Äôs also helpful to show a graph representation of the model.\n\nmodel.build()\nmodel.graph()\n\n\n\n\n\n\n\n\nModel fitting is the same way as always.\n\nidata = model.fit(random_seed=121195)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [Intercept, x, psi_Intercept, psi_x]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:10&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 11 seconds.\n\n\n\n\nEvaluate the inference\nBecause it‚Äôs a simulated scenario, we know the true parameter values. This allows us to verify if the posteriors are recovering them.\n\nfig, axes = plt.subplots(2, 2, figsize=(9, 7), layout=\"constrained\")\naz.plot_posterior(\n    idata,\n    var_names=[\"Intercept\", \"x\", \"psi_Intercept\", \"psi_x\"], \n    ref_val=[b0, b1, a0, a1],\n    ax=axes\n);\n\n\n\n\n\n\n\n\nGreat news! The true values are contained with the 94% HDIs, meaning the model and the inference process is able to recover true parameter values.\n\n\nShow-off Bambi a little more\nBefore concluding I would like to show-off another function I‚Äôve been working recently, plot_cap(). This function allows us to see how a model parameter evolves as we change values of one or more predictors. It‚Äôs so flexible we can get plots for both \\(\\mu\\) and \\(\\psi\\).\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nplot_cap(model, idata, \"x\", ax=axes[0]) # By default it plots the \"parent\" parameter\nplot_cap(model, idata, \"x\", target=\"psi\", ax=axes[1])\n\naxes[0].plot(x, mu, color=\"black\", ls=\"--\")\naxes[1].plot(x, psi, color=\"black\", ls=\"--\")\n\naxes[0].set_xlabel(\"Predictor\", size=14)\naxes[1].set_xlabel(\"Predictor\", size=14)\naxes[0].set_ylabel(\"$\\\\mu$\", size=14)\naxes[1].set_ylabel(\"$\\\\psi$\", size=14)\n\n\nhandles = [Line2D([], [], color=\"black\", ls=\"--\"), Line2D([], [], color=\"C0\")]\nlabels = (\"True\", \"Estimate\")\naxes[0].legend(handles, labels, loc=\"upper left\")\naxes[1].legend(handles, labels, loc=\"upper right\");\n\n\n\n\n\n\n\n\nNotice the majority of the code handles Matplotlib specific details. To get the estimated lines and the credible bands we used the same one-liner twice, plot_cap(), and it did the magic for us.\nLeaving that parise aside, this is a different way to see the model is recovering parameters very well. We can compare the estimated curves for both parameters with the real ones, and see they‚Äôre so close."
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html#conclusion",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html#conclusion",
    "title": "How to create a custom family in Bambi?",
    "section": "Conclusion",
    "text": "Conclusion\nWe work a lot to make Bambi as flexible as possible, without asking users to do too much on their end. I think custom families is a subject where all the work starts to pay off. In this blogpost we were able to define a custom family in just three lines of code (!). What‚Äôs more, we got so many things for free in exchange for our effort. To mention a few\n\nCompute the posterior distribution of parameters of the response distribution\n\nThis works for both in-sample and out-of-sample data\n\nCompute the posterior predictive distribution\n\nAlso works for in-sample or out-of-sample data\n\nOut of the box visualizations to evaluate model fit\nNo need to write any PyMC code or low-level code\n\nI hope you found this writeup useful and needless to say I‚Äôm happy to collect feedback, suggestions, and questions."
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html#tl-dr",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html#tl-dr",
    "title": "How to create a custom family in Bambi?",
    "section": "TL;DR",
    "text": "TL;DR\nIf you want a custom family you need to\n\nCreate a Likelihood instance\nDefine link functions\nUse them to instantiate a Family object\n\nlikelihood = bmb.Likelihood(\"ZeroInflatedPoisson\", params=[\"mu\", \"psi\"], parent=\"mu\")\nlinks = {\"mu\": \"log\", \"psi\": \"logit\"}\nzip_family = bmb.Family(\"zero-inflated-poisson\", likelihood, links)"
  },
  {
    "objectID": "posts/2023-01-14_add_custom_family_bambi/index.html#footnotes",
    "href": "posts/2023-01-14_add_custom_family_bambi/index.html#footnotes",
    "title": "How to create a custom family in Bambi?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the PR #607‚Ü©Ô∏é\nhttps://en.wikipedia.org/wiki/Zero-inflated_model#Zero-inflated_Poisson‚Ü©Ô∏é\nCustom likelihood functions are also possible but we don‚Äôt cover them here‚Ü©Ô∏é\nIt‚Äôs possible to create custom link functions with bmb.Link as well‚Ü©Ô∏é\nIf you‚Äôre curious, check the docs. If you‚Äôre awesome, provide feedback üòÑ‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021-07-14_new-families-in-Bambi/index.html",
    "href": "posts/2021-07-14_new-families-in-Bambi/index.html",
    "title": "New families in Bambi",
    "section": "",
    "text": "I‚Äôm very happy I could contribute with many exciting changes to Bambi. Some changes, such as the reorganization of the default priors and built-in families, are not visible to the user but make the codebase more modular and easier to read. Other changes, such as the ones I‚Äôm going to describe here, have a direct impact on what you can do with Bambi.\nToday I‚Äôll describe two new built-in families that have been added to Bambi. The first one, already described in my previous post, is the \"t\" family. This can be used to make linear regressions more robust to outliers. The second one the \"beta\" family which can be used to model ratings and proportions.\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/2021-07-14_new-families-in-Bambi/index.html#introduction",
    "href": "posts/2021-07-14_new-families-in-Bambi/index.html#introduction",
    "title": "New families in Bambi",
    "section": "",
    "text": "I‚Äôm very happy I could contribute with many exciting changes to Bambi. Some changes, such as the reorganization of the default priors and built-in families, are not visible to the user but make the codebase more modular and easier to read. Other changes, such as the ones I‚Äôm going to describe here, have a direct impact on what you can do with Bambi.\nToday I‚Äôll describe two new built-in families that have been added to Bambi. The first one, already described in my previous post, is the \"t\" family. This can be used to make linear regressions more robust to outliers. The second one the \"beta\" family which can be used to model ratings and proportions.\n\nimport arviz as az\nimport bambi as bmb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/2021-07-14_new-families-in-Bambi/index.html#robust-linear-regression-with-the-t-family.",
    "href": "posts/2021-07-14_new-families-in-Bambi/index.html#robust-linear-regression-with-the-t-family.",
    "title": "New families in Bambi",
    "section": "Robust linear regression with the t family.",
    "text": "Robust linear regression with the t family.\nA Bayesian robust linear regression looks as follows\n\\[\ny_i \\sim \\text{StudentT}(\\mu_i, \\lambda, \\nu)\n\\]\nwhere \\(\\mu_i = \\beta_0 + \\beta_1 x_{1, i} + \\cdots + \\beta_p x_{p, i}\\), \\(\\lambda\\) is the precision parameter and \\(\\nu\\) is the degrees of freedom.\nThis wouldn‚Äôt be a Bayesian model without priors. Bambi uses the following priors by default:\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal}(\\mu_{\\beta_0}, \\sigma_{\\beta_0}) \\\\\n\\beta_j \\sim \\text{Normal}(\\mu_{\\beta_j}, \\sigma_{\\beta_j})  \\\\\n\\lambda \\sim \\text{HalfCauchy(1)}\n\\end{array}\n\\]\nwhere the \\(\\mu_{\\beta_j}\\) and \\(\\sigma_{\\beta_j}\\) are estimated from the data. By default, \\(\\nu=2\\), but it is also possible to assign it a probability distribution (as we‚Äôre going to see below).\nBefore seeing how this new family works, let‚Äôs simulate some data. On this opportunity, we‚Äôre using the same dataset than in the previous post. This is a toy dataset with one predictor x, one response y, and some outliers contaminating the beautiful linear relationship between the variables.\n\nsize = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ny = true_intercept + true_slope * x + np.random.normal(scale=0.5, size=size)\n\nx_out = np.append(x, [0.1, 0.15, 0.2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = pd.DataFrame(dict(x = x_out, y = y_out))\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\n\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7);\n\n\n\n\n\n\n\n\n\nModel specification and fit\nUsing this new family is extremely easy. It is almost as simple as running a default normal linear regression. The only difference is that we need to add the family=\"t\" argument to the Model() instantiation.\n\nmodel = bmb.Model(\"y ~ x\", data, family=\"t\")\nmodel\n\nFormula: y ~ x\nFamily name: T\nLink: identity\nObservations: 103\nPriors:\n  Common-level effects\n    Intercept ~ Normal(mu: 2.1234, sigma: 5.9491)\n    x ~ Normal(mu: 0.0, sigma: 10.4201)\n\n  Auxiliary parameters\n    sigma ~ HalfStudentT(nu: 4, sigma: 1.2227)\n    nu ~ Gamma(alpha: 2, beta: 0.1)\n\n\nThe output above shows information about the family being used and the parameters for the default priors. Next, we just do model.fit() to run the sampler.\n\nidata = model.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_nu, y_sigma, x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:02&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\n\n\n\n\nUse custom priors\nLet‚Äôs say we are not happy with having a fixed value for the degrees of freedom and we want to assign it a prior distribution. Is that a problem? Of course not!\n\n# Use a Gamma prior for the degrees of freedom\nmodel = bmb.Model(\"y ~ x\", data, family=\"t\")\nmodel.set_priors({\"nu\": bmb.Prior(\"Gamma\", alpha=3, beta=1)})\nmodel\n\nFormula: y ~ x\nFamily name: T\nLink: identity\nObservations: 103\nPriors:\n  Common-level effects\n    Intercept ~ Normal(mu: 2.1234, sigma: 5.9491)\n    x ~ Normal(mu: 0.0, sigma: 10.4201)\n\n  Auxiliary parameters\n    sigma ~ HalfStudentT(nu: 4, sigma: 1.2227)\n    nu ~ Gamma(alpha: 3, beta: 1)\n\n\nAnd hit the inference button\n\nidata = model.fit()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_nu, y_sigma, x, Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:04&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 5 seconds.\n\n\n\n\nExplore results\nFirst of all we can see the marginal posteriors for the parameters in the model and their respective traces\n\naz.plot_trace(idata)\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAnd it is also good to explore the posterior distribution of regression lines\n\n# Prepare data\nx = np.linspace(0, 1, num=200)\nposterior_stacked = idata.posterior.stack(samples=(\"chain\", \"draw\"))\nintercepts = posterior_stacked[\"Intercept\"].values\nslopes = posterior_stacked[\"x\"].values\n\n# Create plot\nfig, ax = plt.subplots(figsize=(10, 7))\n\n# Data points\nax.scatter(data[\"x\"], data[\"y\"], s=70, ec=\"black\", alpha=0.7)\n\n# Posterior regression lines\nfor a, b in zip(intercepts, slopes):\n    ax.plot(x, a + b * x, color =\"0.5\", alpha=0.3, zorder=-1)\n\n# True regression line\nax.plot(x, true_intercept + true_slope * x, color=\"k\", lw=2);\n\n\n\n\n\n\n\n\nwhere the line in black is the true regression line."
  },
  {
    "objectID": "posts/2021-07-14_new-families-in-Bambi/index.html#beta-regression-with-the-beta-family.",
    "href": "posts/2021-07-14_new-families-in-Bambi/index.html#beta-regression-with-the-beta-family.",
    "title": "New families in Bambi",
    "section": "Beta regression with the beta family.",
    "text": "Beta regression with the beta family.\nBeta regression is useful to model response variables that have values within the \\((0, 1)\\) interval. This type of regression is based on the assumption that the conditional distribution of the response variable follows a Beta distribution with its mean related to a set of regressors through a linear predictor with unknown coefficients and a link function.\nThe beta regression model is based on an alternative parameterization of the beta density in terms of the mean \\(\\mu\\) and a precision parameter \\(\\kappa\\).\n\\[\n\\begin{array}{lr}\n\\displaystyle f(y | \\mu, \\kappa) =\n  \\frac{\\Gamma(\\kappa)}{\\Gamma(\\mu\\kappa)\\Gamma((1-\\mu)\\kappa)}\n  y^{\\mu\\kappa -1}\n  y^{(1 - \\mu)\\kappa -1}, & 0 &lt; y &lt; 1\n\\end{array}\n\\]\nwith \\(0 &lt; \\mu &lt; 1\\) and \\(\\kappa &gt; 0\\).\nIf we use the same notation than for the robust linear regression, the beta regression model is defined as\n\\[\ny_i \\sim \\text{Beta}(g^{-1}(\\mu_i), \\kappa)\n\\]\nwhere \\(\\mu_i = \\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_p x_{p,i}\\), \\(\\kappa\\) is the precision parameter and \\(g\\) is a twice differentiable, strictly increasing, link function.\nBambi uses again the following priors by default:\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal}(\\mu_{\\beta_0}, \\sigma_{\\beta_0}) \\\\\n\\beta_j \\sim \\text{Normal}(\\mu_{\\beta_j}, \\sigma_{\\beta_j})  \\\\\n\\kappa \\sim \\text{HalfCauchy(1)}\n\\end{array}\n\\]\nwhere the \\(\\mu_{\\beta_j}\\) and \\(\\sigma_{\\beta_j}\\) are estimated from the data. By default, \\(g\\) is the logit function. Other options available are the identity, the probit, and the cloglog link functions.\nIt‚Äôs possible to resume all of this in a very simplistic way by seeing that the beta regression as a very close relative of the GLM family. This model presents all the characteristics of GLMs, with the exception that the beta distribution doesn‚Äôt belong to the exponential family.\n\nModel specification and fit\nHere we are going to use the GasolineYield dataset from the betareg R package. This dataset is about the proportion of crude oil converted to gasoline. The response variable is the proportion of crude oil after distillation and fractionation. In this example, we use the temperature at which gasoline has vaporized in Fahrenheit degrees (\"temp\") and a factor that indicates ten unique combinations of gravity, pressure and temperature (\"batch\").\nThe following is just a re-ordering of the categories in the \"batch\" variable so it matches the original contrasts used in the betareg package.\n\ndata = pd.read_csv(\"data/gasoline.csv\")\ndata[\"batch\"] = pd.Categorical(\n  data[\"batch\"], \n  [10, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n  ordered=True\n)\n\nNext, we define the model. The only difference is that we indicate family=\"beta\". Bambi handles all the rest for us.\n\n# Note this model does not include an intercept\nmodel = bmb.Model(\"yield ~ 0 + temp + batch\", data, family=\"beta\")\nmodel\n\nFormula: yield ~ 0 + temp + batch\nFamily name: Beta\nLink: logit\nObservations: 32\nPriors:\n  Common-level effects\n    temp ~ Normal(mu: 0.0, sigma: 0.0364)\n    batch ~ Normal(mu: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], sigma: [ 8.5769  7.5593  8.5769  8.5769  7.5593  8.5769  8.5769  7.5593  8.5769\n 10.328 ])\n\n  Auxiliary parameters\n    kappa ~ HalfCauchy(beta: 1)\n\n\nAnd model.fit() is all we need to ask the sampler to start running.\n\nidata = model.fit(draws=2000, target_accept=0.95)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [yield_kappa, batch, temp]\n\n\n\n\n\n\n\n    \n      \n      100.00% [6000/6000 00:19&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 20 seconds.\nThe number of effective samples is smaller than 10% for some parameters.\n\n\n\n\nExplore results\nOnce we got the posterior, we explore it. This time we‚Äôre going to plot highest density intervals for the marginal posteriors corresponding to the parameters in the model.\n\nsummary = az.summary(idata, kind=\"stats\")\nsummary\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\ntemp\n0.011\n0.001\n0.010\n0.012\n\n\nbatch[10]\n-6.111\n0.244\n-6.586\n-5.656\n\n\nbatch[1]\n-4.388\n0.201\n-4.766\n-4.010\n\n\nbatch[2]\n-4.799\n0.191\n-5.167\n-4.453\n\n\nbatch[3]\n-4.551\n0.185\n-4.892\n-4.207\n\n\nbatch[4]\n-5.055\n0.211\n-5.442\n-4.635\n\n\nbatch[5]\n-4.980\n0.213\n-5.415\n-4.591\n\n\nbatch[6]\n-5.073\n0.215\n-5.468\n-4.668\n\n\nbatch[7]\n-5.571\n0.212\n-5.973\n-5.170\n\n\nbatch[8]\n-5.617\n0.233\n-6.030\n-5.149\n\n\nbatch[9]\n-5.729\n0.250\n-6.201\n-5.264\n\n\nyield_kappa\n263.233\n86.097\n115.416\n425.489\n\n\n\n\n\n\n\n\nsummary[\"row\"] = list(range(12))\nsummary[\"panel\"] = [\"1-Temperature\"] + [\"2-Batch\"] * 10 + [\"3-Precision\"]\n\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 5.33), sharey=True, dpi=200)\nfig.subplots_adjust(left=0.12, right=0.975, wspace=0.1, bottom=0.12, top=0.925)\nfig.set_facecolor(\"w\")\n\nfor i, (ax, panel) in enumerate(zip(axes, [\"1-Temperature\", \"2-Batch\", \"3-Precision\"])):\n    plt_data = summary[summary[\"panel\"] == panel]\n    ax.scatter(plt_data[\"mean\"], plt_data[\"row\"], s=80)\n    ax.hlines(plt_data[\"row\"], plt_data[\"hdi_3%\"], plt_data[\"hdi_97%\"], lw=3)\n    ax.set_title(panel)\n    ax.tick_params(\"y\", length=0)\n\nax.set_yticks(range(len(summary.index)))\nax.set_yticklabels(list(summary.index))\n\nfig.text(0.5, 0.025, \"Marginal posterior\", size=12, ha=\"center\")\nfig.text(0.02, 0.5, \"Parameter\", size=12, va=\"center\", rotation=90)\n\nfig.savefig(\"imgs/plot.png\", dpi=200)"
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/es/index.html",
    "href": "posts/2025-09-25_freethrows-rookies/es/index.html",
    "title": "¬øLos rookies mejoran con cada lanzamiento? Jugando Bayes con datos de tiros libres de la NBA",
    "section": "",
    "text": "C√≥digo\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport polars as pl\nimport pymc as pm\nimport pytensor.tensor as pt\n\nc_orange = \"#f08533\"\nc_blue = \"#3b78b0\"\nc_red = \"#d1352c\"\nMientras scrapeaba datos sobre los tiros libres de los rookies en la NBA durante la temporada 2024-25, me encontr√© con este art√≠culo, donde se concluye que existe un efecto de ‚Äúprecalentamiento‚Äù: los jugadores de b√°squet tienden a fallar m√°s los primeros disparos que el resto. El hallazgo me pareci√≥ interesante y l√≥gico, y empec√© a preguntarme si val√≠a la pena realizar un an√°lisis similar con los datos que estaba recolectando. Y bueno, ac√° estamos."
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/es/index.html#los-datos",
    "href": "posts/2025-09-25_freethrows-rookies/es/index.html#los-datos",
    "title": "¬øLos rookies mejoran con cada lanzamiento? Jugando Bayes con datos de tiros libres de la NBA",
    "section": "Los datos",
    "text": "Los datos\nEl siguiente data frame contiene informaci√≥n sobre los tiros libres realizados por los rookies de la temporada 2024-25 de la NBA. En este caso, las columnas relevantes son la identificaci√≥n del jugador (player_id), el orden del disparo (description) y si el lanzamiento fue convertido (success).\n\ndf = (\n    pl.read_parquet(\"../data.parquet\")\n    .select(\"game_date\", \"matchup\", \"player_id\", \"player_name\", \"description\", \"success\")\n)\ndf\n\n\nshape: (3_722, 6)\n\n\n\ngame_date\nmatchup\nplayer_id\nplayer_name\ndescription\nsuccess\n\n\ndate\nstr\ni32\nstr\nstr\nbool\n\n\n\n\n2025-04-05\n\"MEM @ DET\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 1 of 2\"\nfalse\n\n\n2025-04-05\n\"MEM @ DET\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 2 of 2\"\nfalse\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 1 of 1\"\ntrue\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 1 of 2\"\nfalse\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 2 of 2\"\ntrue\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2024-11-06\n\"MEM vs. LAL\"\n1642530\n\"Kawamura, Yuki\"\n\"Free Throw 1 of 2\"\ntrue\n\n\n2024-11-06\n\"MEM vs. LAL\"\n1642530\n\"Kawamura, Yuki\"\n\"Free Throw 2 of 2\"\ntrue\n\n\n2025-03-12\n\"MEM vs. UTA\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 1 of 1\"\nfalse\n\n\n2024-11-22\n\"NOP vs. GSW\"\n1641810\n\"Reeves, Antonio\"\n\"Free Throw 1 of 2\"\ntrue\n\n\n2024-11-22\n\"NOP vs. GSW\"\n1641810\n\"Reeves, Antonio\"\n\"Free Throw 2 of 2\"\nfalse\n\n\n\n\n\n\nEn la NBA, los tiros libres pueden darse en series de 1, 2 o 3 intentos. Nuestra primera tarea es mapear los valores de description a un valor num√©rico que represente el orden del disparo dentro de su serie.\n\nthrow_order = {\n    \"Free Throw 1 of 1\": 1,\n    \"Free Throw 1 of 2\": 1,\n    \"Free Throw 1 of 3\": 1,\n    \"Free Throw 2 of 2\": 2,\n    \"Free Throw 2 of 3\": 2,\n    \"Free Throw 3 of 3\": 3,\n    \"Free Throw Clear Path 1 of 2\": 1,\n    \"Free Throw Clear Path 2 of 2\": 2,\n    \"Free Throw Flagrant 1 of 1\": 1,\n    \"Free Throw Flagrant 1 of 2\": 1,\n    \"Free Throw Flagrant 2 of 2\": 2,\n    \"Free Throw Technical\": 1,\n}\n\ndf = df.with_columns(\n    pl.col(\"description\").replace_strict(throw_order, return_dtype=pl.Int64).alias(\"order\")\n)\ndf\n\n\nshape: (3_722, 7)\n\n\n\ngame_date\nmatchup\nplayer_id\nplayer_name\ndescription\nsuccess\norder\n\n\ndate\nstr\ni32\nstr\nstr\nbool\ni64\n\n\n\n\n2025-04-05\n\"MEM @ DET\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 1 of 2\"\nfalse\n1\n\n\n2025-04-05\n\"MEM @ DET\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 2 of 2\"\nfalse\n2\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 1 of 1\"\ntrue\n1\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 1 of 2\"\nfalse\n1\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 2 of 2\"\ntrue\n2\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2024-11-06\n\"MEM vs. LAL\"\n1642530\n\"Kawamura, Yuki\"\n\"Free Throw 1 of 2\"\ntrue\n1\n\n\n2024-11-06\n\"MEM vs. LAL\"\n1642530\n\"Kawamura, Yuki\"\n\"Free Throw 2 of 2\"\ntrue\n2\n\n\n2025-03-12\n\"MEM vs. UTA\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 1 of 1\"\nfalse\n1\n\n\n2024-11-22\n\"NOP vs. GSW\"\n1641810\n\"Reeves, Antonio\"\n\"Free Throw 1 of 2\"\ntrue\n1\n\n\n2024-11-22\n\"NOP vs. GSW\"\n1641810\n\"Reeves, Antonio\"\n\"Free Throw 2 of 2\"\nfalse\n2\n\n\n\n\n\n\nPodemos observar que los primeros intentos suelen fallarse con mayor frecuencia que los segundos, y que estos, a su vez, se fallan m√°s que los terceros.\n\ndf_summary = (\n    df\n    .group_by(\"order\")\n    .agg(pl.col(\"success\").sum().alias(\"y\"), pl.len().alias(\"n\"))\n    .with_columns((pl.col(\"y\") / pl.col(\"n\")).alias(\"p\"))\n    .sort(\"order\")\n)\ndf_summary\n\n\nshape: (3, 4)\n\n\n\norder\ny\nn\np\n\n\ni64\nu32\nu32\nf64\n\n\n\n\n1\n1435\n2069\n0.693572\n\n\n2\n1227\n1625\n0.755077\n\n\n3\n23\n28\n0.821429"
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/es/index.html#modelo-agrupado",
    "href": "posts/2025-09-25_freethrows-rookies/es/index.html#modelo-agrupado",
    "title": "¬øLos rookies mejoran con cada lanzamiento? Jugando Bayes con datos de tiros libres de la NBA",
    "section": "Modelo agrupado",
    "text": "Modelo agrupado\nComo primer paso, utilizamos un modelo que agrupa los disparos de todos los jugadores, los considera equivalentes, m√°s de lo mismo. Definimos \\(Y_1\\) como la cantidad de aciertos en primeros disparos y \\(Y_2\\) como la cantidad de aciertos en segundos disparos. Luego, \\(\\pi_1\\) representa la probabilidad de acertar en un primer intento y \\(\\pi_2\\) en un segundo.\n\\[\n\\begin{aligned}\nY_1 &\\sim \\text{Binomial}(N_1, \\pi_1) \\\\\nY_2 &\\sim \\text{Binomial}(N_2, \\pi_2) \\\\\n\\pi_1 &\\sim \\text{Beta}(4, 2) \\\\\n\\pi_2 &\\sim \\text{Beta}(4, 2) \\\\\n\\end{aligned}\n\\]\nEn PyMC, tenemos:\n\ny = df_summary.head(2)[\"y\"].to_numpy()\nn = df_summary.head(2)[\"n\"].to_numpy()\n\nwith pm.Model(coords={\"order\": [1, 2]}) as model:\n    pi = pm.Beta(\"pi\", alpha=4, beta=2, dims=\"order\")\n    y = pm.Binomial(\"y\", p=pi, n=n, observed=y, dims=\"order\")\n    idata = pm.sample(chains=4, random_seed=1211, nuts_sampler=\"nutpie\", progressbar=False)\n\nmodel.to_graphviz()\n\n\n\n\n\n\n\n\nCalculamos \\(\\delta\\) como la diferencia entre \\(\\pi_2\\) y \\(\\pi_1\\), y analizamos las distribuciones marginales a posteriori.\nEn primer lugar, los diagn√≥sticos no indican problemas en el muestreo: \\(\\hat{R}\\) cercano a 1, tama√±os efectivos de muestra adecuados, etc.\n\nidata.posterior[\"delta\"] = idata.posterior[\"pi\"].sel(order=2) - idata.posterior[\"pi\"].sel(order=1)\naz.summary(idata, var_names=[\"pi\", \"delta\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\npi[1]\n0.694\n0.010\n0.673\n0.712\n0.0\n0.0\n4040.0\n2880.0\n1.0\n\n\npi[2]\n0.754\n0.011\n0.733\n0.773\n0.0\n0.0\n3996.0\n2737.0\n1.0\n\n\ndelta\n0.061\n0.015\n0.030\n0.088\n0.0\n0.0\n4193.0\n2918.0\n1.0\n\n\n\n\n\n\n\nSi nos enfocamos en la distribuci√≥n a posteriori de \\(\\delta\\), podemos ver que la probabilidad de que \\(\\delta\\) sea mayor a 0 es igual a 1.\nAs√≠, llegamos a nuestra primera gran conclusi√≥n: efectivamente, la probabilidad de acertar en el segundo intento es mayor que en el primero. De hecho, la probabilidad de acierto se incrementa, en promedio, un 6 %. Y, con un alto grado de certeza, podemos afirmar que esta mejora se encuentra entre el 3 % y el 9 %."
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/es/index.html#modelo-parcialmente-agrupado",
    "href": "posts/2025-09-25_freethrows-rookies/es/index.html#modelo-parcialmente-agrupado",
    "title": "¬øLos rookies mejoran con cada lanzamiento? Jugando Bayes con datos de tiros libres de la NBA",
    "section": "Modelo parcialmente agrupado",
    "text": "Modelo parcialmente agrupado\nEl modelo anterior solo nos permite concluir que es m√°s probable que un rookie acierte un segundo lanzamiento que un primero (y cuantificar la magnitud de la diferencia). Sin embargo, no podemos extraer conclusiones sobre jugadores individuales. Por ejemplo, no sabemos si este fen√≥meno de ‚Äúprecalentamiento‚Äù se presenta en todos ellos, ni si su intensidad var√≠a de un jugador a otro.\nUna alternativa ser√≠a ajustar pares de modelos beta-binomiales como los anteriores para cada jugador. Lamentablemente, dado que la cantidad de disparos var√≠a entre apenas uno y unas pocas centenas, las distribuciones a posteriori pueden resultar extremadamente inciertas en algunos casos o excesivamente confiadas en otros.\nAs√≠ llegamos a la alternativa bayesiana por excelencia: el modelo jer√°rquico. Bajo este enfoque, cada jugador tiene su propia probabilidad de acierto, pero dicha probabilidad proviene de una distribuci√≥n com√∫n compartida con el resto. De este modo, las estimaciones son m√°s estables y se evita el sobreajuste a los datos particulares de cada jugador, ya que la distribuci√≥n a posteriori de cada uno est√° influenciada, en cierta medida, por la informaci√≥n del resto.\nLos datos que necesitamos son la cantidad de disparos y aciertos en el primer y segundo intento para cada jugador. Por simplicidad, consideramos √∫nicamente a los jugadores que realizaron al menos una serie de dos tiros libres.\n\ndf_agg = (\n    df\n    .group_by(\"player_id\", \"order\")\n    .agg(\n        pl.col(\"success\").sum().alias(\"y\"),\n        pl.len().alias(\"n\")\n    )\n    .sort(\"player_id\", \"order\")\n)\nselected_ids = (\n    df_agg\n    .filter(pl.col(\"order\") == 2, pl.col(\"n\") &gt; 0)\n    .get_column(\"player_id\")\n    .unique()\n    .to_list()\n)\n\ndf_model = (\n    df_agg\n    .filter(pl.col(\"player_id\").is_in(selected_ids), pl.col(\"order\") &lt; 3)\n    .sort(\"player_id\", \"order\")\n)\ndf_model\n\n\nshape: (184, 4)\n\n\n\nplayer_id\norder\ny\nn\n\n\ni32\ni64\nu32\nu32\n\n\n\n\n1630283\n1\n1\n5\n\n\n1630283\n2\n5\n5\n\n\n1630542\n1\n4\n5\n\n\n1630542\n2\n4\n5\n\n\n1630545\n1\n9\n12\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n1642502\n2\n1\n2\n\n\n1642505\n1\n1\n1\n\n\n1642505\n2\n1\n1\n\n\n1642530\n1\n4\n5\n\n\n1642530\n2\n3\n4\n\n\n\n\n\n\nDe manera distribucional, podemos representar al modelo de la siguiente manera:\n\\[\\begin{aligned}\nY_{i1} &\\sim \\text{Binomial}(N_{i1}, \\pi_i) \\\\\nY_{i2} &\\sim \\text{Binomial}(N_{i2}, \\theta_i) \\\\ \\\\\n\n& \\text{--- P(Free throw 1 is made) ---} \\\\\n\n\\pi_i &\\sim \\text{Beta}(\\mu_\\pi, \\kappa_\\pi) \\\\\n\\mu_\\pi &\\sim \\text{Beta}(4, 2) \\\\\n\\kappa_\\pi &\\sim \\text{InverseGamma}(0.5 \\cdot 15, 0.5 \\cdot 15 \\cdot 10) \\\\ \\\\\n\n& \\text{--- P(Free throw 2 is made) --- } \\\\\n\n\\theta_i &= \\pi_i + \\delta_i \\\\\n\\delta_i &\\sim \\text{Normal}(\\mu_\\delta, \\sigma^2_\\delta) \\\\\n\\mu_\\delta &\\sim \\text{Normal}(0, 0.15^2) \\\\\n\\sigma^2_\\delta &\\sim \\text{InverseGamma}(0.5 \\cdot 30, 0.5 \\cdot 30 \\cdot 0.05) \\\\\n\\end{aligned}\\]\ndonde \\(i \\in \\{1, \\dots, 92\\}\\) indexa a los jugadores.\nEn otras palabras, modelizamos la probabilidad de acierto en el segundo intento como la suma de la probabilidad de acierto en el primero (\\(\\pi_i\\)) y un diferencial espec√≠fico de cada jugador (\\(\\delta_i\\)).\nLas distribuciones a priori para \\(\\mu_\\pi\\) y \\(\\mu_\\delta\\) son levemente y moderadamente informativas, respectivamente. Por su parte, los priors asignados a \\(\\kappa_\\pi\\) y \\(\\sigma^2_\\delta\\) tambi√©n son moderadamente informativos, aunque en este caso su funci√≥n principal es favorecer la estabilidad del muestreo del posterior.\n\n1y_1 = df_model.filter(pl.col(\"order\") == 1)[\"y\"].to_numpy()\ny_2 = df_model.filter(pl.col(\"order\") == 2)[\"y\"].to_numpy()\nn_1 = df_model.filter(pl.col(\"order\") == 1)[\"n\"].to_numpy()\nn_2 = df_model.filter(pl.col(\"order\") == 2)[\"n\"].to_numpy()\n\n2player_ids = df_model[\"player_id\"].unique(maintain_order=True)\nN = len(player_ids)\ncoords = {\"player\": player_ids}\n\nwith pm.Model(coords=coords) as model_h:\n3    pi_mu = pm.Beta(\"pi_mu\", alpha=4, beta=2)\n    pi_kappa = pm.InverseGamma(\"pi_kappa\", alpha=0.5 * 15, beta=0.5 * 15 * 10)\n    pi_alpha = pi_mu * pi_kappa\n    pi_beta = (1 - pi_mu) * pi_kappa\n    pi = pm.Beta(\"pi\", alpha=pi_alpha, beta=pi_beta, dims=\"player\")\n\n4    delta_mu = pm.Normal(\"delta_mu\", mu=0, sigma=0.15)\n    delta_sigma = pm.InverseGamma(\"delta_sigma^2\", 0.5 * 30, 0.5 * 30 * 0.05 ** 2) ** 0.5\n    delta = pm.Normal(\n5        \"delta\", mu=delta_mu, sigma=delta_sigma, dims=\"player\", initval=np.zeros(N)\n    )\n6    theta = pm.Deterministic(\"theta\", pt.clip(pi + delta, 0.0001, 0.9999), dims=\"player\")\n\n\n    pm.Binomial(\"y_1\", p=pi, n=n_1, observed=y_1, dims=\"player\")\n    pm.Binomial(\"y_2\", p=theta, n=n_2, observed=y_2, dims=\"player\")\n\n\n1\n\nPreparaci√≥n de datos: se usan vectores (se podr√≠an haber usado arreglos bidimensionales).\n\n2\n\nPreparaci√≥n de coordenadas: representan los diferentes jugadores.\n\n3\n\nConstrucci√≥n de \\(\\pi_i\\): se asignan priors para la media y la precisi√≥n, que luego se transforman en par√°metros de escala para usados directamente en pm.Beta.\n\n4\n\nConstrucci√≥n de \\(\\theta_i\\): se especifica, para cada jugador, un par√°metro de diferencia \\(\\delta_i\\).\n\n5\n\nUso de initval: para que el algoritmo de inferencia comience en un punto v√°lido del espacio param√©trico.\n\n6\n\nUso de pt.clip: para asegurarnos que \\(\\pi_i + \\delta_i\\) se encuentre entre 0 y 1. Esto no afecta al posterior, pero es necesario para inicializar correctamente el algoritmo de muestreo.\n\n\n\n\nFinalmente, as√≠ es como se ve una representaci√≥n gr√°fica del modelo:\n\n\n\n\n\n\n\n\n\nGracias a nutpie, es posible obtener muestras fiables del posterior en muy pocos segundos.\nSi bien los diagn√≥sticos no se ven tan bien como en el modelo agrupado (lo cual es esperable en modelos jer√°rquicos), los valores son aceptables y podemos continuar con nuestro an√°lisis.\n\nwith model_h:\n    idata_h = pm.sample(\n        chains=4,\n        target_accept=0.99,\n        nuts_sampler=\"nutpie\",\n        random_seed=1211,\n        progressbar=False,\n    )\n\naz.summary(idata_h, var_names=[\"pi_mu\", \"pi_kappa\", \"delta_mu\", \"delta_sigma^2\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\npi_mu\n0.698\n0.015\n0.670\n0.727\n0.000\n0.000\n1063.0\n2206.0\n1.00\n\n\npi_kappa\n28.903\n9.035\n14.499\n45.731\n0.286\n0.278\n1001.0\n1623.0\n1.00\n\n\ndelta_mu\n0.055\n0.016\n0.027\n0.086\n0.001\n0.000\n497.0\n910.0\n1.01\n\n\ndelta_sigma^2\n0.002\n0.001\n0.001\n0.003\n0.000\n0.000\n1262.0\n2006.0\n1.00\n\n\n\n\n\n\n\nLos posteriors marginales de los par√°metros poblacionales son los siguientes:\n\n\n\n\n\n\n\n\n\nA simple vista, se observa que, en promedio, la probabilidad de acertar un primer lanzamiento ronda el 70 %, y que los segundos intentos tienen, en promedio, cerca de un 6 % m√°s de probabilidad de ser convertidos. Aunque esto no resulte novedoso, es satisfactorio comprobar que las inferencias obtenidas con el modelo jer√°rquico son consistentes con los hallazgos anteriores.\nLo realmente interesante de este modelo jer√°rquico es que nos permite analizar las distribuciones a posteriori de \\(\\pi_i\\), \\(\\theta_i\\) y \\(\\delta_i\\) a nivel individual, es decir, para cada jugador.\nSin embargo, visualizar estas distribuciones para todos ellos puede resultar poco pr√°ctico. Por eso, seleccionamos un subconjunto representativo de jugadores en funci√≥n de sus valores de \\(N_1\\) (la cantidad de primeros lanzamientos). En concreto, conservamos a los 10 jugadores con mayor \\(N_1\\) y, entre el resto, los ordenamos de menor a mayor y elegimos uno cada dos posiciones para formar una muestra m√°s manejable.\n\n\n\nshape: (26, 6)\n\n\n\nplayer_id\nplayer_name\ny_1\ny_2\nn_1\nn_2\n\n\ni32\nstr\nu32\nu32\nu32\nu32\n\n\n\n\n1642264\n\"Castle, Stephon\"\n128\n121\n192\n152\n\n\n1642274\n\"Missi, Yves\"\n63\n64\n112\n92\n\n\n1642259\n\"Sarr, Alex\"\n62\n50\n88\n77\n\n\n1642268\n\"Collier, Isaiah\"\n56\n49\n85\n69\n\n\n1642258\n\"Risacher, Zaccharie\"\n55\n47\n82\n62\n\n\n1642271\n\"Filipowski, Kyle\"\n47\n46\n81\n62\n\n\n1641744\n\"Edey, Zach\"\n54\n36\n76\n51\n\n\n1642377\n\"Wells, Jaylen\"\n62\n44\n73\n56\n\n\n1641842\n\"Holland II, Ronald\"\n51\n41\n70\n52\n\n\n1642270\n\"Clingan, Donovan\"\n33\n29\n60\n44\n\n\n1641824\n\"Buzelis, Matas\"\n45\n43\n59\n49\n\n\n1642273\n\"George, Kyshawn\"\n39\n31\n51\n42\n\n\n1642266\n\"Walter, Ja'Kobe\"\n34\n32\n47\n36\n\n\n1642347\n\"Shead, Jamal\"\n32\n20\n42\n26\n\n\n1641783\n\"da Silva, Tristan\"\n29\n26\n35\n28\n\n\n1642272\n\"McCain, Jared\"\n23\n23\n28\n25\n\n\n1642348\n\"Edwards, Justin\"\n17\n15\n24\n22\n\n\n1641810\n\"Reeves, Antonio\"\n16\n12\n21\n14\n\n\n1631232\n\"Brooks Jr., Keion\"\n12\n10\n17\n13\n\n\n1642277\n\"Furphy, Johnny\"\n11\n7\n13\n9\n\n\n1641736\n\"Beekman, Reece\"\n8\n8\n11\n10\n\n\n1642265\n\"Dillingham, Rob\"\n4\n4\n9\n6\n\n\n1630574\n\"Hukporti, Ariel\"\n2\n4\n7\n6\n\n\n1630283\n\"Kelley, Kylor\"\n1\n5\n5\n5\n\n\n1641989\n\"Harkless, Elijah\"\n2\n1\n3\n3\n\n\n1630762\n\"Wheeler, Phillip\"\n1\n1\n1\n1\n\n\n\n\n\n\nLuego, podemos visualizar los posterior marginales de \\(\\pi_i\\), \\(\\theta_i\\) y \\(\\delta_i\\) para cada jugador seleccionado. Naturalmente, a medida que aumenta la cantidad de disparos observados, las distribuciones a posteriori se vuelven m√°s estrechas, reflejando un mayor nivel de certidumbre.\nEn todos los casos, incluso cuando \\(N_1 = 1\\) y \\(N_2 = 1\\), la media a posteriori de \\(\\theta_i\\) es mayor que la de \\(\\pi_i\\).\nVisto desde otro √°ngulo, en el panel inferior se observa que la media de \\(\\delta_i\\) es siempre mayor que 0. Sin embargo, solo en aquellos casos en los que se registran cientos de lanzamientos puede concluirse, con probabilidad cercana a 1, que los jugadores son efectivamente mejores en los segundos intentos que en los primeros.\n\n\n\n\n\n\n\n\n\nComo somos bayesianos y utilizamos m√©todos de Markov Chain Monte Carlo para obtener muestras del posterior, podemos calcular, para cada jugador, la probabilidad de que \\(\\delta_i\\) sea mayor que 0. Con estas probabilidades, es posible construir el siguiente histograma resumen:\n\n\n\n\n\n\n\n\n\nLuego, sin importar el rookie en el que nos fijemos, siempre concluiremos que tiene una probabilidad entre moderada y alta de ser m√°s efectivo en los segundos lanzamientos que en los primeros.\nPara finalizar, comparemos la distribuci√≥n a posteriori de \\(\\delta\\) del modelo agrupado con el posterior de \\(\\mu_\\delta\\) del modelo parcialmente agrupado, ya que ambas representan la misma cantidad: el diferencial promedio en la probabilidad de acierto entre un segundo y un primer lanzamiento.\n\n\n\n\n\n\n\n\n\nAfortunadamente, no tenemos que reportar sorpresas. Ambos modelos nos conducen hacia conclusiones pr√°cticamente similares sobre el \\(\\delta\\) promedio, aunque se puede mencionar que en el modelo jer√°rquico el posterior se encuentra levemente m√°s regularizado hacia el 0 que en el modelo agrupado. Sin embargo, no debemos olvidar que utilizar un modelo jer√°rquico nos permiti√≥ obtener distribuciones a posteriori para cada jugador individual."
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/es/index.html#comentarios-finales",
    "href": "posts/2025-09-25_freethrows-rookies/es/index.html#comentarios-finales",
    "title": "¬øLos rookies mejoran con cada lanzamiento? Jugando Bayes con datos de tiros libres de la NBA",
    "section": "Comentarios finales",
    "text": "Comentarios finales\nEjemplos como este son los que me recuerdan por qu√© el enfoque bayesiano para la modelizaci√≥n estad√≠stica me gusta tanto.\nSi bien podr√≠amos habernos conformado con una simple prueba de hip√≥tesis, como la que presentan en el art√≠culo mencionado, la flexibilidad que ofrecen herramientas como PyMC nos permite ir varios pasos m√°s all√°. Propusimos un modelo jer√°rquico (nada trivial), lo implementamos en PyMC, y luego este, junto con nutpie, se encargaron de proporcionarnos muestras del posterior.\nA partir de esas muestras, pudimos extraer varias conclusiones interesantes. No solo aquellas a nivel global, que confirman que los jugadores de b√°squet suelen fallar m√°s en los primeros disparos que en los segundos (en l√≠nea con los resultados del art√≠culo), sino tambi√©n conclusiones espec√≠ficas para cada jugador.\nY, por encima de todo, m√°s all√° de la utilidad pr√°ctica del modelo o de los insights que podamos obtener‚Ä¶ ¬øno es acaso super divertido jugar a la estad√≠stica bayesiana?"
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html",
    "title": "How to generate bingo cards in R",
    "section": "",
    "text": "Hello wor‚Ä¶ Well, my first hello world post appeared about a year ago, but this site had the same fate as many of my othe side-projects‚Ä¶ abandonment.\nUntil now."
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html#introduction",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html#introduction",
    "title": "How to generate bingo cards in R",
    "section": "Introduction",
    "text": "Introduction\nToday I‚Äôm going to show you how I came up with ‚Äúan algorithm‚Äù to generate random bingo cards and some utility functions to print them on a nice looking (?) .pdf file.\nFirst of all, what type of bingo card I‚Äôm referring to? As an Argentine, the only bingo cards I‚Äôve ever heard of are bingo cards like this one\n\n\n\n\nExample bingo card from bingo.es\n\n\n\nIt contains fifteen numbers from 1 to 90 that are divided in three rows and nine columns. The first column contains numbers between 1 and 9, the second column numbers between 10 and 20, and so on until the last column that contains numbers between 80 and 90. The type of bingo that you play with this bingo card is known as the 90-ball bingo game or British bingo. As I said, this is the only version I knew before this project 1 and I think it is the only bingo version you‚Äôll find here in Argentina (I also bet you‚Äôll find some fellow Argentine confirming this a national invention).\nSo, if you entered this post thinking you‚Äôll find how to print those bingo cards that are popular in places like United States, I‚Äôm sorry, this is not for you 2. Fortunately, other people have invented a tool for you even before I wondered how to generate bingo cards. If you are interested, have a look at this package and the Shiny app introduced there.\nNow, let‚Äôs go back to our business.\nAnyone who has gone to one of those events where people gather to play bingo 3 knows that bingo cards don‚Äôt usually come separated in individual pieces of paper. Sellers usually have strips of six bingo cards in their hands. In some events, you can buy bingo cards directly. In others, you have to buy the entire strip.\nSince this is a 90-ball bingo game and each card contains fifteen numbers, six bingo cards with no repeated numbers is all we need to have all the numbers of the game in a single strip. You see where it is going?. Yes, we won‚Äôt generate isolated cards, we‚Äôll generate entire strips. This is how a bingo strip looks like (just imagine them vertically stacked on a single strip)\n\n\n\n\nExample bingo strip from bingo.es"
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html#valid-cards-and-valid-strips",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html#valid-cards-and-valid-strips",
    "title": "How to generate bingo cards in R",
    "section": "Valid cards and valid strips",
    "text": "Valid cards and valid strips\nBingo cards are not just a bunch of numbers thrown at a piece of paper. All valid strips are composed of six valid cards each made of three valid rows. But not any combinations of three valid rows make up a valid card nor any combinations of six valid cards make up a valid strip. What a shame!\nBut what is a valid row, a valid card, a va‚Ä¶ whatever. Let‚Äôs just get to the point and list the rules that will govern how we generate bingo cards.\n\nValid row\nWe‚Äôre going to think that a row is a numeric vector of length nine where some elements are empty and some are filled with numbers.\n\nExactly five elements are numbers, and four are empty.\nThere can‚Äôt be more than two consecutive empty elements, which is equivalent to having at most three consecutive numbers.\n\nExample valid rows\n\n \n\nExample invalid rows\n\n \n\n\n\nValid card\nWe can think that a bingo card is a matrix of three rows and nine columns. Each row must be a valid row as specified in the previous point, plus\n\nNo column can be completely empty.\nNo column can be completely filled with numbers.\nNumbers are sorted in ascending order within columns.\n\nExample valid card\n\n\n\n\n\nValid strip\nA valid strip contains six valid cards that satisfy the following conditions\n\nThe first column must have nine numbers and nine empty slots.\nColumns 2 to 8 must have ten numbers and eight empty slots.\nColumn 9 must have eleven numbers and seven empty slots.\n\nIn total, we have \\(6\\times3\\times9 = 162\\) slots in a strip. 90 of them are filled with numbers, 72 are not."
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html#sample-this-sample-that-ive-got-no-need-to-compute-them-all",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html#sample-this-sample-that-ive-got-no-need-to-compute-them-all",
    "title": "How to generate bingo cards in R",
    "section": "Sample this, sample that, I‚Äôve got no need to compute them all4",
    "text": "Sample this, sample that, I‚Äôve got no need to compute them all4\nOne approach to generate bingo cards would be to get all possible combinations of row layouts, bingo layouts, number arrangements, etc. But the number of cards you could generate is huge and the task wouldn‚Äôt be easy at all.\nThe approach used here is one that mixes some simple combinatorics and random sampling. We use permutations to compute all the possible row layouts. Then, we sample rows to create cards and sample cards to create strips5.\nFirst of all, we are going to find valid layouts (i.e.¬†the skeleton of our bingo strips). Once we have them, we are going to fill them with numbers.\n\nFinding valid rows\nIf we represent empty slots with a 0 and filled slots with a 1, getting all permutations between four 0s and five 1s is as simple as calling combinat::permn(c(rep(0, 4), rep(1, 5))). However, this is not what we want because not all the returned layouts are valid rows. We need to select only those row layouts that are valid in a bingo card.\nThe following function, find_window(), receives a numeric vector x and looks for find windows of length width where all the elements are equal to what. If such a window is found, the function returns TRUE, otherwise it returns FALSE.\n\nfind_window &lt;- function(x, width, what) {\n    for (i in 1:(length(x) - width)) {\n        if (all(x[i:(i + width)] == what)) {\n            return(TRUE)\n        }\n    }\n    return(FALSE)\n}\n\nThen we write a function called get_rows() that generates all the possible row layouts and uses find_window() to select the layouts that satisfy our conditions.\n\nget_rows &lt;- function() {\n    # Get all row layouts\n    rows &lt;- combinat::permn(c(rep(0, 4), rep(1, 5)))\n    # Keep rows with at most two consecutive empty slots\n    rows &lt;- rows[!vapply(rows, find_window, logical(1), 2, 0)]\n    # Keep rows with at most three consecutive filled slots\n    rows &lt;- rows[!vapply(rows, find_window, logical(1), 3, 1)]\n    return(rows)\n}\n\n\n\nSampling valid cards\nWe noted that a valid card is made of three valid rows, but not all combinations of three valid rows make up a valid card. What if we sample three row layouts and keep/discard the combination based on whether they make up a valid card or not? We can repeat this until we have some desired number of card layours. The process is as follows\n\nLet \\(N\\) be the number of cards we want to generate.\nWhile the number of cards generated is smaller than \\(N\\), do:\n\nSample three rows and make up the card.\nCount the number of filled slots per column.\nIf all the counts are between 1 and 3, keep the card, else discard it.\n\n\nOnce we‚Äôre done, we end up with \\(N\\) bingo card layouts that are valid in terms of our requirements above.\nThis idea is implemented in a function called get_cards(). It receives the rows we generate with get_rows() and the number of card layouts we want to generate. Finally it returns a list whose elements are vectors of length 3 with the row indexes6.\n\nget_cards &lt;- function(rows, cards_n = 2000) {\n    rows_n &lt;- length(rows)\n    cards &lt;- vector(\"list\", cards_n)\n\n    attempts &lt;- 0\n    card_idx &lt;- 0\n\n    while (card_idx &lt; cards_n) {\n        attempts &lt;- attempts + 1\n        # Sample three rows\n        row_idxs &lt;- sample(rows_n, 3)\n        mm &lt;- matrix(unlist(rows[row_idxs]), ncol = 9, byrow = TRUE)\n        col_sums &lt;- colSums(mm)\n\n        # Select valid cards.\n        # These have between 1 and 3 numbers per column.\n        if (all(col_sums != 0) && all(col_sums != 3)) {\n            card_idx &lt;- card_idx + 1\n            cards[[card_idx]] &lt;- list(row_idxs, col_sums)\n        }\n        # Print message every 1000 attempts\n        if (attempts %% 1000 == 0) {\n            message(\"Attempt \", attempts, \" | Cards built:\", card_idx, \"\\n\")\n        }\n    }\n    # Check duplicates\n    dups &lt;- duplicated(lapply(cards, `[[`, 1))\n    message(\"There are \", sum(dups), \" duplicated cards.\")\n    return(cards)\n}\n\n\n\nSampling valid strips\nThis is the much like what we did above, with two differences. Instead of sampling three row layouts, we sample six card layouts. Instead of checking if the number of filled slots per column are between 1 and 3, we check if they match a number between 9 and 11 specific to each of them.\nThen, we have get_strips(). It receives a list called cards where each element contains the three row indexes corresponding to each card layout. rows is a list of row layouts and strips_n controls how many strip layouts we want to generate.\n\nget_strips &lt;- function(cards, rows, strips_n = 100) {\n    valid_counts &lt;- c(9, rep(10, 7), 11)\n    cards_n &lt;- length(cards)\n    strips &lt;- vector(\"list\", strips_n)\n\n    attempts &lt;- 0\n    strip_idx &lt;- 0\n\n    while (strip_idx &lt; strips_n) {\n        attempts &lt;- attempts + 1\n\n        # Sample 6 cards\n        cards_idxs &lt;- sample(cards_n, 6)\n        strip &lt;- cards[cards_idxs]\n\n        # Contains column counts by card\n        card_counts &lt;- matrix(\n            unlist(lapply(strip, `[[`, 2)),\n            ncol = 9, byrow = TRUE\n        )\n\n        # Check if strip column counts are valid\n        if (all(colSums(card_counts) == valid_counts)) {\n            strip_idx &lt;- strip_idx + 1\n            # Get row indexes contained in the selected card indexes\n            rows_idxs &lt;- unlist(lapply(cards[cards_idxs], `[[`, 1))\n            strips[[strip_idx]] &lt;- matrix(\n                unlist(rows[rows_idxs]),\n                ncol = 9, byrow = TRUE\n            )\n        }\n        # Print message every 1000 attempts\n        if (attempts %% 1000 == 0) {\n            message(\"Attempt \", attempts, \" | Strips built:\", strip_idx, \"\\n\")\n        }\n    }\n    dups &lt;- duplicated(strips)\n    message(\"There are \", sum(dups), \" duplicatd layouts.\\n\")\n    return(strips)\n}\n\n\n\nA last but not least step\nI‚Äôve never seen a bingo game where you are given empty layouts and are asked to put numbers yourself. So let‚Äôs wrap this up and fill our empty cards!\nfill_strips() receives the strip layouts we generated, randomly selects n of them, and, also randomly, fills the slots the cards with numbers. Of course, the first column contains numbers from 1 to 9, the second column contains numbers from 10 to 19‚Ä¶ and so on until the last column, that has numbers from 80 to 90.\n\nfill_strips &lt;- function(strips, n = 100) {\n    # Numbers that go in each column\n    numbers &lt;- list(1:9, 10:19, 20:29, 30:39, 40:49, 50:59, 60:69, 70:79, 80:90)\n    # Row indexes corresponding to each card in the strip\n    card_rows &lt;- list(1:3, 4:6, 7:9, 10:12, 13:15, 16:18)\n\n    fill_strip &lt;- function(strip) {\n        # Put numbers in the slots with a 1 (meaning they must contain a number)\n        strip[strip == 1] &lt;- unlist(\n            # This `sample()` reorders the numbers in each column randomly\n            mapply(sample, numbers, sapply(numbers, length))\n        )\n\n        for (i in seq_along(card_rows)) {\n            strip_ &lt;- strip[card_rows[[i]], ]\n            # Numbers in a given column are sorted in ascending order within cards\n            x &lt;- sort(strip_)\n            strip_[strip_ != 0] &lt;- x[x != 0]\n            strip[card_rows[[i]], ] &lt;- strip_\n        }\n        return(strip)\n    }\n    # Strip layouts can be repeated\n    strips &lt;- lapply(sample(strips, n, replace = TRUE), fill_strip)\n    message(\"There are \", sum(duplicated(strips)), \" duplicated strips.\\n\")\n    return(strips)\n}\n\nAnd we finally get our bingo strips :)\n\nset.seed(0303456)\nrows &lt;- get_rows()\ncards &lt;- get_cards(rows, 1000)\nstrips &lt;- get_strips(cards, rows, 20)\nstrips &lt;- fill_strips(strips, 50)\n# Output messages have been suppressed\n\nLet‚Äôs check some of them\n\nstrips[[1]]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n [1,]    0   11   20    0   48    0    0   74   80\n [2,]    8    0    0   31    0   51   60   78    0\n [3,]    0   19   27   39    0   54   62    0    0\n [4,]    1    0   26    0   42   55    0    0   84\n [5,]    2   14    0   34    0    0   65   77    0\n [6,]    0   17   29    0   43   59    0    0   89\n [7,]    0    0   22   33    0    0   64   75   88\n [8,]    0   15    0   35   45    0    0   79   90\n [9,]    9    0   25    0   49   50   66    0    0\n[10,]    3    0   28   30    0    0   61   71    0\n[11,]    7    0    0   36   40   58    0    0   81\n[12,]    0   10    0    0   44    0   63   76   87\n[13,]    0    0   21   37    0   52   68   70    0\n[14,]    5   16    0    0   41    0    0   72   82\n[15,]    0   18    0   38   47   57    0    0   86\n[16,]    0    0   23    0   46   53    0   73   83\n[17,]    4   12    0   32    0    0   67    0   85\n[18,]    6   13   24    0    0   56   69    0    0\n\n\n\nstrips[[30]]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n [1,]    0    0   25    0   43   50    0   74   80\n [2,]    0   16   26   34    0    0   65   79    0\n [3,]    6   17    0   38    0   58    0    0   86\n [4,]    3    0   27    0   40   51   61    0    0\n [5,]    4    0    0   32   49   59    0    0   81\n [6,]    0   19   29   35    0    0   68   71    0\n [7,]    1   14    0    0   47    0   60   75    0\n [8,]    2    0   20   31    0    0   66    0   83\n [9,]    0    0   24    0   48   55    0   77   89\n[10,]    0    0   28   33   42    0   64   76    0\n[11,]    5   12    0   39    0    0   67    0   84\n[12,]    9   15    0    0   45   54    0    0   87\n[13,]    0   13   21    0    0   52    0   73   85\n[14,]    0   18   22    0   44    0   63   78    0\n[15,]    8    0    0   37   46   56    0    0   90\n[16,]    0    0   23   30    0   53   62    0   82\n[17,]    7   10    0   36    0    0   69   70    0\n[18,]    0   11    0    0   41   57    0   72   88"
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html#are-we-going-to-play-on-r-consoles",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html#are-we-going-to-play-on-r-consoles",
    "title": "How to generate bingo cards in R",
    "section": "Are we going to play on R consoles?",
    "text": "Are we going to play on R consoles?\nAll we got so far are matrices that look like a bingo strip. But honestly, without any given context, they just look like a bunch of matrices of the same dimension filled with 0s and other integer numbers. Our last task is to generate a .pdf output where these matrices really look like bingo cards.\nIn this last part of the post we make use of the grid package. For those who haven‚Äôt heard of it, it is the low level plotting library behind ggplot2, for example.\nHere we have a little function, make_grid(), that given a number of rows and columns returns the natural parent coordinates of the borders the grid that defines the rectangles within each card.\n\nmake_grid &lt;- function(rows, cols) {\n    lines_rows &lt;- grid::unit((0:rows) / rows, \"npc\")\n    lines_cols &lt;- grid::unit((0:cols) / cols, \"npc\")\n    return(list(\"row\" = lines_rows, \"col\" = lines_cols))\n}\n\nAnd now we have the main function used to plot the bingo strips. Since the function is quite large, I prefer to explain how it works with comments in the body.\n\nplot_strips &lt;- function(strips, col = \"#8e44ad\", width_row = 0.925,\n                        width_col = 0.975) {\n\n    # `rows` and `cols` are the dimensions of each card\n    rows &lt;- 3\n    cols &lt;- 9\n    g &lt;- make_grid(rows, cols)\n    # Compute the center of each square in the card grid\n    centers_rows &lt;- g$row[-1] - grid::unit(1 / (rows * 2), \"npc\")\n    centers_cols &lt;- g$col[-1] - grid::unit(1 / (cols * 2), \"npc\")\n    # Sort the centers appropiately\n    # This is required because of how we loop over the values in each card\n    x_coords &lt;- rep(centers_cols, each = rows)\n    y_coords &lt;- rep(rev(centers_rows), cols)\n\n    # Create unique identifiers for the cards\n    cards_n &lt;- paste(\n        paste0(\"CARD N\", intToUtf8(176)),\n        seq_len(length(strips) * 6)\n    )\n    # Compute the number of sheets we're going to need.\n    # Each sheet contains two strips\n    sheets_n &lt;- ceiling(length(strips) / 2)\n\n    # Initial numbers\n    card_idx &lt;- 0\n    strip_idx &lt;- 0\n\n    # Loop over sheets\n    for (sheet_idx in seq_len(sheets_n)) {\n        # Each sheet is a grid of 6 rows and 3 columns.\n        # Columns 1 and 3 are where we place the strips.\n        # Column 2 just gives vertical separation.\n        l &lt;- grid::grid.layout(\n            nrow = 6, ncol = 3,\n            widths = c(48.75, 2.5 + 3.75, 48.75)\n        )\n        # Start a new page filled with white\n        grid::grid.newpage()\n        grid::grid.rect(gp = grid::gpar(col = NULL, fill = \"white\"))\n\n        vp_mid &lt;- grid::viewport(0.5, 0.5, width_row, width_col, layout = l)\n        grid::pushViewport(vp_mid)\n\n        # Loop over columns 1 and 3\n        for (j in c(1, 3)) {\n            # Select strip\n            strip_idx &lt;- strip_idx + 1\n            if (strip_idx &gt; length(strips)) break\n            strip &lt;- strips[[strip_idx]]\n\n            # Loop over rows (these rows represent the 6 rows assigned to cards)\n            for (i in 1L:l$nrow) {\n                card_idx &lt;- card_idx + 1\n                vp_inner &lt;- grid::viewport(layout.pos.row = i, layout.pos.col = j)\n                grid::pushViewport(vp_inner)\n\n                # Add card identification number on top-left\n                grid::grid.text(\n                    label = cards_n[card_idx],\n                    x = 0,\n                    y = 0.96,\n                    just = \"left\",\n                    gp = grid::gpar(fontsize = 9)\n                )\n\n                # Draw a grill that separates the slots in the card\n                vp_mid_inner &lt;- grid::viewport(0.5, 0.5, 1, 0.80)\n                grid::pushViewport(vp_mid_inner)\n                grid::grid.grill(h = g$row, v = g$col, gp = grid::gpar(col = col))\n\n                # Select the numbers that correspond to this card\n                numbers &lt;- as.vector(strip[(3 * i - 2):(3 * i), ])\n                # Logical vector that indicates which rectangles are filled\n                # with nunumbers and which rectangles are empty\n                lgl &lt;- ifelse(numbers == 0, FALSE, TRUE)\n\n                # Draw the numbers in positions given by the rectangle centers\n                grid::grid.text(\n                    label = numbers[lgl],\n                    x = x_coords[lgl],\n                    y = y_coords[lgl],\n                    gp = grid::gpar(fontsize = 18)\n                )\n\n                # Fill empty slots with color\n                grid::grid.rect(\n                    x = x_coords[!lgl],\n                    y = y_coords[!lgl],\n                    height = grid::unit(1 / rows, \"npc\"),\n                    width = grid::unit(1 / cols, \"npc\"),\n                    gp = grid::gpar(\n                        col = NA,\n                        fill = farver::encode_colour(farver::decode_colour(col), 0.7)\n                    )\n                )\n                # End\n                grid::popViewport()\n                grid::popViewport()\n            }\n        }\n        grid::popViewport()\n    }\n}\n\nNow, all we need is to pass the strips generated above to plot_strips() and wrap that call within grDevices::pdf() and grDevices::dev.off().\n\n# Height and width are in inches and here they correspond to legal paper size\ngrDevices::pdf(\"imgs/strips.pdf\", height = 14, width = 8.5)\nplot_strips(strips)\ngrDevices::dev.off()\n\nIf it works, you‚Äôll have a 25 pages pdf with bingo cards that look like this one\n\n\n\n\nFirst card in the output\n\n\n\nIf you can‚Äôt (or just don‚Äôt want to) run the code, here you have the generated pdf."
  },
  {
    "objectID": "posts/2020-11-03_bingo-cards-in-r/index.html#footnotes",
    "href": "posts/2020-11-03_bingo-cards-in-r/index.html#footnotes",
    "title": "How to generate bingo cards in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHow I dare to call this a project?‚Ü©Ô∏é\nBut you should try this bingo, you gonna like it!‚Ü©Ô∏é\nSome are also known as sobremesa‚Ü©Ô∏é\nHaven‚Äôt you heard Estallando desde el oc√©ano by Sumo?‚Ü©Ô∏é\nIf you‚Äôve heard of Sampford‚Äôs pps sampling, this is going to be familiar‚Ü©Ô∏é\nI know that returning row indexes is less intuitive than returning card layouts, but this approach requires less memory because it only stores 3 values per card, instead of 18.‚Ü©Ô∏é"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Who am I?",
    "section": "",
    "text": "I use data science to solve problems at PyMC Labs,  I teach statistics at Universidad Nacional de Rosario, and because I‚Äôm always learning, I‚Äôm still a (part-time) PhD student.  I‚Äôm also an Open Source Software developer, mostly focusing on Bambi.  I speak mainly Python and R, but I‚Äôm also experienced with other languages and tools as well."
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/index.html",
    "href": "posts/2025-09-25_freethrows-rookies/index.html",
    "title": "Do rookies get better with each throw? Playing Bayes with NBA free-throw data",
    "section": "",
    "text": "NOTE: You also have this versi√≥n en espa√±ol.\nCode\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport polars as pl\nimport pymc as pm\nimport pytensor.tensor as pt\n\nc_orange = \"#f08533\"\nc_blue = \"#3b78b0\"\nc_red = \"#d1352c\"\nWhile I was scraping data on NBA rookies‚Äô free throws during the 2024‚Äì25 season, I came across this article, which concludes that there‚Äôs a ‚Äúwarm-up‚Äù effect: basketball players tend to miss their first shots more often than the rest. I found the result both interesting and intuitive, and it made me wonder whether it would be worth running a similar analysis on the data I was collecting.\nAnd well‚Ä¶ here we are."
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/index.html#the-data",
    "href": "posts/2025-09-25_freethrows-rookies/index.html#the-data",
    "title": "Do rookies get better with each throw? Playing Bayes with NBA free-throw data",
    "section": "The data",
    "text": "The data\nThe following data frame contains information about the free throws taken by NBA rookies during the 2024‚Äì25 season. In this case, the relevant columns are the player‚Äôs identifier (player_id), the order of the attempt (description), and whether the shot was made (success).\n\ndf = (\n    pl.read_parquet(\"data.parquet\")\n    .select(\"game_date\", \"matchup\", \"player_id\", \"player_name\", \"description\", \"success\")\n)\ndf\n\n\nshape: (3_722, 6)\n\n\n\ngame_date\nmatchup\nplayer_id\nplayer_name\ndescription\nsuccess\n\n\ndate\nstr\ni32\nstr\nstr\nbool\n\n\n\n\n2025-04-05\n\"MEM @ DET\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 1 of 2\"\nfalse\n\n\n2025-04-05\n\"MEM @ DET\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 2 of 2\"\nfalse\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 1 of 1\"\ntrue\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 1 of 2\"\nfalse\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 2 of 2\"\ntrue\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2024-11-06\n\"MEM vs. LAL\"\n1642530\n\"Kawamura, Yuki\"\n\"Free Throw 1 of 2\"\ntrue\n\n\n2024-11-06\n\"MEM vs. LAL\"\n1642530\n\"Kawamura, Yuki\"\n\"Free Throw 2 of 2\"\ntrue\n\n\n2025-03-12\n\"MEM vs. UTA\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 1 of 1\"\nfalse\n\n\n2024-11-22\n\"NOP vs. GSW\"\n1641810\n\"Reeves, Antonio\"\n\"Free Throw 1 of 2\"\ntrue\n\n\n2024-11-22\n\"NOP vs. GSW\"\n1641810\n\"Reeves, Antonio\"\n\"Free Throw 2 of 2\"\nfalse\n\n\n\n\n\n\nIn the NBA, free throws can come in series of 1, 2, or 3 attempts. Our first task is to map the values of description to a numeric value representing the order of the shot within its series.\n\nthrow_order = {\n    \"Free Throw 1 of 1\": 1,\n    \"Free Throw 1 of 2\": 1,\n    \"Free Throw 1 of 3\": 1,\n    \"Free Throw 2 of 2\": 2,\n    \"Free Throw 2 of 3\": 2,\n    \"Free Throw 3 of 3\": 3,\n    \"Free Throw Clear Path 1 of 2\": 1,\n    \"Free Throw Clear Path 2 of 2\": 2,\n    \"Free Throw Flagrant 1 of 1\": 1,\n    \"Free Throw Flagrant 1 of 2\": 1,\n    \"Free Throw Flagrant 2 of 2\": 2,\n    \"Free Throw Technical\": 1,\n}\n\ndf = df.with_columns(\n    pl.col(\"description\").replace_strict(throw_order, return_dtype=pl.Int64).alias(\"order\")\n)\ndf\n\n\nshape: (3_722, 7)\n\n\n\ngame_date\nmatchup\nplayer_id\nplayer_name\ndescription\nsuccess\norder\n\n\ndate\nstr\ni32\nstr\nstr\nbool\ni64\n\n\n\n\n2025-04-05\n\"MEM @ DET\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 1 of 2\"\nfalse\n1\n\n\n2025-04-05\n\"MEM @ DET\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 2 of 2\"\nfalse\n2\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 1 of 1\"\ntrue\n1\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 1 of 2\"\nfalse\n1\n\n\n2025-01-18\n\"PHI @ IND\"\n1641737\n\"Bona, Adem\"\n\"Free Throw 2 of 2\"\ntrue\n2\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2024-11-06\n\"MEM vs. LAL\"\n1642530\n\"Kawamura, Yuki\"\n\"Free Throw 1 of 2\"\ntrue\n1\n\n\n2024-11-06\n\"MEM vs. LAL\"\n1642530\n\"Kawamura, Yuki\"\n\"Free Throw 2 of 2\"\ntrue\n2\n\n\n2025-03-12\n\"MEM vs. UTA\"\n1641744\n\"Edey, Zach\"\n\"Free Throw 1 of 1\"\nfalse\n1\n\n\n2024-11-22\n\"NOP vs. GSW\"\n1641810\n\"Reeves, Antonio\"\n\"Free Throw 1 of 2\"\ntrue\n1\n\n\n2024-11-22\n\"NOP vs. GSW\"\n1641810\n\"Reeves, Antonio\"\n\"Free Throw 2 of 2\"\nfalse\n2\n\n\n\n\n\n\nWe can see that first attempts tend to miss more often than second ones ‚Äî and second attempts, in turn, miss more often than third ones.\n\ndf_summary = (\n    df\n    .group_by(\"order\")\n    .agg(pl.col(\"success\").sum().alias(\"y\"), pl.len().alias(\"n\"))\n    .with_columns((pl.col(\"y\") / pl.col(\"n\")).alias(\"p\"))\n    .sort(\"order\")\n)\ndf_summary\n\n\nshape: (3, 4)\n\n\n\norder\ny\nn\np\n\n\ni64\nu32\nu32\nf64\n\n\n\n\n1\n1435\n2069\n0.693572\n\n\n2\n1227\n1625\n0.755077\n\n\n3\n23\n28\n0.821429"
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/index.html#pooled-model",
    "href": "posts/2025-09-25_freethrows-rookies/index.html#pooled-model",
    "title": "Do rookies get better with each throw? Playing Bayes with NBA free-throw data",
    "section": "Pooled model",
    "text": "Pooled model\nAs a first step, we use a model that groups all players‚Äô shots together and treats them as equivalent ‚Äî more of the same. We define \\(Y_1\\) as the number of made first attempts and \\(Y_2\\) as the number of made second attempts. Then, \\(\\pi_1\\) represents the probability of making a first attempt, and \\(\\pi_2\\) the probability of making a second one.\n\\[\n\\begin{aligned}\nY_1 &\\sim \\text{Binomial}(N_1, \\pi_1) \\\\\nY_2 &\\sim \\text{Binomial}(N_2, \\pi_2) \\\\\n\\pi_1 &\\sim \\text{Beta}(4, 2) \\\\\n\\pi_2 &\\sim \\text{Beta}(4, 2) \\\\\n\\end{aligned}\n\\]\nIn PyMC, we have:\n\ny = df_summary.head(2)[\"y\"].to_numpy()\nn = df_summary.head(2)[\"n\"].to_numpy()\n\nwith pm.Model(coords={\"order\": [1, 2]}) as model:\n    pi = pm.Beta(\"pi\", alpha=4, beta=2, dims=\"order\")\n    y = pm.Binomial(\"y\", p=pi, n=n, observed=y, dims=\"order\")\n    idata = pm.sample(chains=4, random_seed=1211, nuts_sampler=\"nutpie\", progressbar=False)\n\nmodel.to_graphviz()\n\n\n\n\n\n\n\n\nWe compute \\(\\delta\\) as the difference between \\(\\pi_2\\) and \\(\\pi_1\\), and then analyze the marginal posterior distributions.\nFirst, the diagnostics show no issues with sampling: \\(\\hat{R}\\) is close to 1, effective sample sizes are adequate, and so on.\n\nidata.posterior[\"delta\"] = idata.posterior[\"pi\"].sel(order=2) - idata.posterior[\"pi\"].sel(order=1)\naz.summary(idata, var_names=[\"pi\", \"delta\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\npi[1]\n0.694\n0.010\n0.673\n0.712\n0.0\n0.0\n4040.0\n2880.0\n1.0\n\n\npi[2]\n0.754\n0.011\n0.733\n0.773\n0.0\n0.0\n3996.0\n2737.0\n1.0\n\n\ndelta\n0.061\n0.015\n0.030\n0.088\n0.0\n0.0\n4193.0\n2918.0\n1.0\n\n\n\n\n\n\n\nIf we focus on the posterior distribution of \\(\\delta\\), we can see that the probability of \\(\\delta\\) being greater than 0 is equal to 1.\nThis leads us to our first major conclusion: indeed, the probability of making the second free throw is higher than that of the first. In fact, the success probability increases by an average of about 6%. And with a high degree of certainty, we can say that this improvement lies between 3% and 9%."
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/index.html#partially-pooled-model",
    "href": "posts/2025-09-25_freethrows-rookies/index.html#partially-pooled-model",
    "title": "Do rookies get better with each throw? Playing Bayes with NBA free-throw data",
    "section": "Partially pooled model",
    "text": "Partially pooled model\nThe previous model only allows us to conclude that a rookie is more likely to make a second free throw than a first one (and quantify the size of that difference). However, it doesn‚Äôt let us draw conclusions about individual players. For example, we don‚Äôt know whether this ‚Äúwarm-up‚Äù effect occurs for all of them, or whether its intensity varies from player to player.\nOne option would be to fit pairs of beta-binomial models like the ones above separately for each player. Unfortunately, since the number of attempts ranges from just one to a few hundred, the resulting posterior distributions can end up being extremely uncertain in some cases, or overly confident in others.\nThis brings us to the quintessential Bayesian alternative: the hierarchical model. Under this approach, each player has their own success probability, but that probability is drawn from a common distribution shared by all players. This makes the estimates more stable and helps prevent overfitting to individual data, since each player‚Äôs posterior distribution is influenced ‚Äî to some extent ‚Äî by the information from the rest.\nThe data we need are the number of attempts and the number of made shots on the first and second try for each player. For simplicity, we‚Äôll focus only on players who attempted at least one series of two free throws.\n\ndf_agg = (\n    df\n    .group_by(\"player_id\", \"order\")\n    .agg(\n        pl.col(\"success\").sum().alias(\"y\"),\n        pl.len().alias(\"n\")\n    )\n    .sort(\"player_id\", \"order\")\n)\nselected_ids = (\n    df_agg\n    .filter(pl.col(\"order\") == 2, pl.col(\"n\") &gt; 0)\n    .get_column(\"player_id\")\n    .unique()\n    .to_list()\n)\n\ndf_model = (\n    df_agg\n    .filter(pl.col(\"player_id\").is_in(selected_ids), pl.col(\"order\") &lt; 3)\n    .sort(\"player_id\", \"order\")\n)\ndf_model\n\n\nshape: (184, 4)\n\n\n\nplayer_id\norder\ny\nn\n\n\ni32\ni64\nu32\nu32\n\n\n\n\n1630283\n1\n1\n5\n\n\n1630283\n2\n5\n5\n\n\n1630542\n1\n4\n5\n\n\n1630542\n2\n4\n5\n\n\n1630545\n1\n9\n12\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n1642502\n2\n1\n2\n\n\n1642505\n1\n1\n1\n\n\n1642505\n2\n1\n1\n\n\n1642530\n1\n4\n5\n\n\n1642530\n2\n3\n4\n\n\n\n\n\n\nIn distributional form, we can write the model as follows:\n\\[\\begin{aligned}\nY_{i1} &\\sim \\text{Binomial}(N_{i1}, \\pi_i) \\\\\nY_{i2} &\\sim \\text{Binomial}(N_{i2}, \\theta_i) \\\\ \\\\\n\n& \\text{--- P(Free throw 1 is made) ---} \\\\\n\n\\pi_i &\\sim \\text{Beta}(\\mu_\\pi, \\kappa_\\pi) \\\\\n\\mu_\\pi &\\sim \\text{Beta}(4, 2) \\\\\n\\kappa_\\pi &\\sim \\text{InverseGamma}(0.5 \\cdot 15, 0.5 \\cdot 15 \\cdot 10) \\\\ \\\\\n\n& \\text{--- P(Free throw 2 is made) --- } \\\\\n\n\\theta_i &= \\pi_i + \\delta_i \\\\\n\\delta_i &\\sim \\text{Normal}(\\mu_\\delta, \\sigma^2_\\delta) \\\\\n\\mu_\\delta &\\sim \\text{Normal}(0, 0.15^2) \\\\\n\\sigma^2_\\delta &\\sim \\text{InverseGamma}(0.5 \\cdot 30, 0.5 \\cdot 30 \\cdot 0.05) \\\\\n\\end{aligned}\\]\nwhere \\(i \\in {1, \\dots, 92}\\) indexes the players.\nIn other words, we model the probability of making the second free throw as the sum of the probability of making the first one (\\(\\pi_i\\)) and a player-specific differential (\\(\\delta_i\\)).\nThe prior distributions for \\(\\mu_\\pi\\) and \\(\\mu_\\delta\\) are slightly and moderately informative, respectively. Meanwhile, the priors assigned to \\(\\kappa_\\pi\\) and \\(\\sigma^2_\\delta\\) are also moderately informative ‚Äî though in this case, their main role is to promote stability in the sampling of the posterior.\n\n1y_1 = df_model.filter(pl.col(\"order\") == 1)[\"y\"].to_numpy()\ny_2 = df_model.filter(pl.col(\"order\") == 2)[\"y\"].to_numpy()\nn_1 = df_model.filter(pl.col(\"order\") == 1)[\"n\"].to_numpy()\nn_2 = df_model.filter(pl.col(\"order\") == 2)[\"n\"].to_numpy()\n\n2player_ids = df_model[\"player_id\"].unique(maintain_order=True)\nN = len(player_ids)\ncoords = {\"player\": player_ids}\n\nwith pm.Model(coords=coords) as model_h:\n3    pi_mu = pm.Beta(\"pi_mu\", alpha=4, beta=2)\n    pi_kappa = pm.InverseGamma(\"pi_kappa\", alpha=0.5 * 15, beta=0.5 * 15 * 10)\n    pi_alpha = pi_mu * pi_kappa\n    pi_beta = (1 - pi_mu) * pi_kappa\n    pi = pm.Beta(\"pi\", alpha=pi_alpha, beta=pi_beta, dims=\"player\")\n\n4    delta_mu = pm.Normal(\"delta_mu\", mu=0, sigma=0.15)\n    delta_sigma = pm.InverseGamma(\"delta_sigma^2\", 0.5 * 30, 0.5 * 30 * 0.05 ** 2) ** 0.5\n    delta = pm.Normal(\n5        \"delta\", mu=delta_mu, sigma=delta_sigma, dims=\"player\", initval=np.zeros(N)\n    )\n6    theta = pm.Deterministic(\"theta\", pt.clip(pi + delta, 0.0001, 0.9999), dims=\"player\")\n\n\n    pm.Binomial(\"y_1\", p=pi, n=n_1, observed=y_1, dims=\"player\")\n    pm.Binomial(\"y_2\", p=theta, n=n_2, observed=y_2, dims=\"player\")\n\n\n1\n\nData preparation: vectors are used (although 2-dimensional arrays could have been used instead).\n\n2\n\nCoordinate preparation: these represent the different players.\n\n3\n\nConstruction of \\(\\pi_i\\): priors are assigned for the mean and precision, which are then transformed into scale parameters used directly in pm.Beta.\n\n4\n\nConstruction of \\(\\theta_i\\): for each player, a difference parameter \\(\\delta_i\\) is specified.\n\n5\n\nUse of initval: ensures that the inference algorithm starts from a valid point in the parameter space.\n\n6\n\nUse of pt.clip: guarantees that \\(\\pi_i + \\delta_i\\) stays between 0 and 1. This does not affect the posterior but is necessary to correctly initialize the sampling algorithm.\n\n\n\n\nFinally, here‚Äôs what a graphical representation of the model looks like:\n\n\n\n\n\n\n\n\n\nThanks to nutpie, we can obtain reliable samples from the posterior in just a few seconds.\nAlthough the diagnostics don‚Äôt look quite as good as in the pooled model (which is easy to happen in hierarchical models), the values are still acceptable, so we can proceed with our analysis.\n\nwith model_h:\n    idata_h = pm.sample(\n        chains=4,\n        target_accept=0.99,\n        nuts_sampler=\"nutpie\",\n        random_seed=1211,\n        progressbar=False,\n    )\n\naz.summary(idata_h, var_names=[\"pi_mu\", \"pi_kappa\", \"delta_mu\", \"delta_sigma^2\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\npi_mu\n0.698\n0.015\n0.670\n0.727\n0.000\n0.000\n1063.0\n2206.0\n1.00\n\n\npi_kappa\n28.903\n9.035\n14.499\n45.731\n0.286\n0.278\n1001.0\n1623.0\n1.00\n\n\ndelta_mu\n0.055\n0.016\n0.027\n0.086\n0.001\n0.000\n497.0\n910.0\n1.01\n\n\ndelta_sigma^2\n0.002\n0.001\n0.001\n0.003\n0.000\n0.000\n1262.0\n2006.0\n1.00\n\n\n\n\n\n\n\nThe marginal posteriors of the population-level parameters look as follows:\n\n\n\n\n\n\n\n\n\nAt first glance, we can see that, on average, the probability of making a first free throw is around 70%, and that second attempts are, on average, about 6% more likely to go in. While this isn‚Äôt a surprising result, it‚Äôs reassuring to see that the inferences from the hierarchical model are consistent with our previous findings.\nWhat‚Äôs truly interesting about this hierarchical approach is that it allows us to analyze the posterior distributions of \\(\\pi_i\\), \\(\\theta_i\\), and \\(\\delta_i\\) at the individual level ‚Äî that is, for each player.\nHowever, visualizing these distributions for all players can quickly become impractical. To make things more manageable, we selected a representative subset of players based on their \\(N_1\\) values (the number of first attempts). Specifically, we kept the 10 players with the highest \\(N_1\\), and from the remaining group, we sorted them from lowest to highest and chose every second player to form a more manageable sample.\n\n\n\nshape: (26, 6)\n\n\n\nplayer_id\nplayer_name\ny_1\ny_2\nn_1\nn_2\n\n\ni32\nstr\nu32\nu32\nu32\nu32\n\n\n\n\n1642264\n\"Castle, Stephon\"\n128\n121\n192\n152\n\n\n1642274\n\"Missi, Yves\"\n63\n64\n112\n92\n\n\n1642259\n\"Sarr, Alex\"\n62\n50\n88\n77\n\n\n1642268\n\"Collier, Isaiah\"\n56\n49\n85\n69\n\n\n1642258\n\"Risacher, Zaccharie\"\n55\n47\n82\n62\n\n\n1642271\n\"Filipowski, Kyle\"\n47\n46\n81\n62\n\n\n1641744\n\"Edey, Zach\"\n54\n36\n76\n51\n\n\n1642377\n\"Wells, Jaylen\"\n62\n44\n73\n56\n\n\n1641842\n\"Holland II, Ronald\"\n51\n41\n70\n52\n\n\n1642270\n\"Clingan, Donovan\"\n33\n29\n60\n44\n\n\n1641824\n\"Buzelis, Matas\"\n45\n43\n59\n49\n\n\n1642273\n\"George, Kyshawn\"\n39\n31\n51\n42\n\n\n1642266\n\"Walter, Ja'Kobe\"\n34\n32\n47\n36\n\n\n1642347\n\"Shead, Jamal\"\n32\n20\n42\n26\n\n\n1641783\n\"da Silva, Tristan\"\n29\n26\n35\n28\n\n\n1642272\n\"McCain, Jared\"\n23\n23\n28\n25\n\n\n1642348\n\"Edwards, Justin\"\n17\n15\n24\n22\n\n\n1641810\n\"Reeves, Antonio\"\n16\n12\n21\n14\n\n\n1631232\n\"Brooks Jr., Keion\"\n12\n10\n17\n13\n\n\n1642277\n\"Furphy, Johnny\"\n11\n7\n13\n9\n\n\n1641736\n\"Beekman, Reece\"\n8\n8\n11\n10\n\n\n1642265\n\"Dillingham, Rob\"\n4\n4\n9\n6\n\n\n1630574\n\"Hukporti, Ariel\"\n2\n4\n7\n6\n\n\n1630283\n\"Kelley, Kylor\"\n1\n5\n5\n5\n\n\n1641989\n\"Harkless, Elijah\"\n2\n1\n3\n3\n\n\n1630762\n\"Wheeler, Phillip\"\n1\n1\n1\n1\n\n\n\n\n\n\nWe can then visualize the marginal posteriors of \\(\\pi_i\\), \\(\\theta_i\\), and \\(\\delta_i\\) for each selected player. Naturally, as the number of observed shots increases, the posterior distributions become narrower, reflecting a higher level of certainty.\nIn every case, even when \\(N_1 = 1\\) and \\(N_2 = 1\\), the posterior mean of \\(\\theta_i\\) is greater than that of \\(\\pi_i\\).\nFrom another perspective, the bottom panel shows that the mean of \\(\\delta_i\\) is always greater than 0. However, it‚Äôs only in cases where we have hundreds of attempts that we can conclude, with probability close to 1, that players are indeed better on their second attempts than on their first.\n\n\n\n\n\n\n\n\n\nSince we‚Äôre Bayesians and use Markov Chain Monte Carlo methods to obtain samples from the posterior, we can compute, for each player, the probability that \\(\\delta_i\\) is greater than 0. With these probabilities, we can build the following summary histogram:\n\n\n\n\n\n\n\n\n\nThen, no matter which rookie we look at, we‚Äôll always conclude that they have a moderate to high probability of being more effective on their second free throws than on their first.\nFinally, let‚Äôs compare the posterior distribution of \\(\\delta\\) from the pooled model with the posterior of \\(\\mu_\\delta\\) from the partially pooled model, since both represent the same quantity: the average difference in success probability between a second and a first attempt.\n\n\n\n\n\n\n\n\n\nFortunately, there are no surprises to report. Both models lead us to practically the same conclusions about the average \\(\\delta\\), although it‚Äôs worth noting that in the hierarchical model, the posterior is slightly more regularized toward 0 compared to the pooled model.\nHowever, we shouldn‚Äôt forget that using a hierarchical model allowed us to obtain posterior distributions for each individual player."
  },
  {
    "objectID": "posts/2025-09-25_freethrows-rookies/index.html#final-comments",
    "href": "posts/2025-09-25_freethrows-rookies/index.html#final-comments",
    "title": "Do rookies get better with each throw? Playing Bayes with NBA free-throw data",
    "section": "Final comments",
    "text": "Final comments\nExamples like this are what remind me why I enjoy the Bayesian approach to statistical modeling so much.\nWhile we could have settled for a simple hypothesis test, like the one presented in the article mentioned above, the flexibility offered by tools like PyMC allows us to go several steps further.\nWe proposed a hierarchical model (not a trivial one), implemented it in PyMC, and then, together with nutpie, used it to obtain samples from the posterior.\nFrom those samples, we were able to draw several interesting conclusions. Not only the global ones, which confirm that basketball players tend to miss their first free throws more often than their second (consistent with the article‚Äôs findings), but also player-specific insights.\nAnd above all, beyond the practical usefulness of the model or the insights we can extract‚Ä¶ isn‚Äôt it just incredibly fun to play Bayes?"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "",
    "text": "A linear mixed model can be written as\n\\[\n\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} +\n                 \\boldsymbol{Z}\\boldsymbol{u} + \\boldsymbol{\\epsilon}\n\\]\nwhere \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) are the two design matrices we need to somehow construct when dealing with this type of model. \\(\\boldsymbol{X}\\) is the design matrix for the common (a.k.a. fixed) effects, and \\(\\boldsymbol{Z}\\) is the design matrix for the group-specific (a.k.a. random or varying) effects.\nIt is quite easy to obtain the design matrix \\(\\boldsymbol{X}\\) in R using its popular formula interface. In Python, patsy provides equivalent functionality. Unfortunately, there aren‚Äôt as many alternatives to compute the matrix \\(\\boldsymbol{Z}\\).\nIn R, there‚Äôs lme4, the statistical package par excellence for mixed models. It extends the base formula interface to include group-specific effects via the pipe operator (|) and internally computes both \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) without the user noticing. That‚Äôs great!\nIn Python, we are working on formulae, a library we use to handle mixed model formulas in Bambi. In this process, I‚Äôve found Fitting Linear Mixed-Effects Models Using lme4 vignette extremely useful when figuring out how to compute the design matrix for the group-specific effects.\nToday, I was adding tests to make sure we are constructing \\(\\boldsymbol{Z}\\) appropriately and found myself comparing the matrices obtained with formulae with matrices obtained with lme4. Then I was like ‚Ä¶ why not making this a blog post? ü§î\n‚Ä¶ and so here we are! But before we get started, just note this post mixes both R and Python code. I will try to be explicit when I‚Äôm using one language or the other. But if you‚Äôre reading a chunk and it looks like Python, it‚Äôs Python. And if it looks like R‚Ä¶ you guessed! It‚Äôs R."
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#introduction",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#introduction",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "",
    "text": "A linear mixed model can be written as\n\\[\n\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} +\n                 \\boldsymbol{Z}\\boldsymbol{u} + \\boldsymbol{\\epsilon}\n\\]\nwhere \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) are the two design matrices we need to somehow construct when dealing with this type of model. \\(\\boldsymbol{X}\\) is the design matrix for the common (a.k.a. fixed) effects, and \\(\\boldsymbol{Z}\\) is the design matrix for the group-specific (a.k.a. random or varying) effects.\nIt is quite easy to obtain the design matrix \\(\\boldsymbol{X}\\) in R using its popular formula interface. In Python, patsy provides equivalent functionality. Unfortunately, there aren‚Äôt as many alternatives to compute the matrix \\(\\boldsymbol{Z}\\).\nIn R, there‚Äôs lme4, the statistical package par excellence for mixed models. It extends the base formula interface to include group-specific effects via the pipe operator (|) and internally computes both \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) without the user noticing. That‚Äôs great!\nIn Python, we are working on formulae, a library we use to handle mixed model formulas in Bambi. In this process, I‚Äôve found Fitting Linear Mixed-Effects Models Using lme4 vignette extremely useful when figuring out how to compute the design matrix for the group-specific effects.\nToday, I was adding tests to make sure we are constructing \\(\\boldsymbol{Z}\\) appropriately and found myself comparing the matrices obtained with formulae with matrices obtained with lme4. Then I was like ‚Ä¶ why not making this a blog post? ü§î\n‚Ä¶ and so here we are! But before we get started, just note this post mixes both R and Python code. I will try to be explicit when I‚Äôm using one language or the other. But if you‚Äôre reading a chunk and it looks like Python, it‚Äôs Python. And if it looks like R‚Ä¶ you guessed! It‚Äôs R."
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#setup",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#setup",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Setup",
    "text": "Setup\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lme4)\nlibrary(patchwork)\n\n\nfrom formulae import design_matrices"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#problem",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#problem",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Problem",
    "text": "Problem\nHere we will be comparing design matrices for the group-specific terms in a mixed-effects model obtained with both lme4 and formulae. We‚Äôre using the dataset Pixel that comes with the R package nlme.\n\ndata(\"Pixel\", package = \"nlme\")\nhead(Pixel)\n\nGrouped Data: pixel ~ day | Dog/Side\n  Dog Side day  pixel\n1   1    R   0 1045.8\n2   1    R   1 1044.5\n3   1    R   2 1042.9\n4   1    R   4 1050.4\n5   1    R   6 1045.2\n6   1    R  10 1038.9\n\n\nWe‚Äôre not interested in how to fit a certain model here. We‚Äôre interested in constructing the design matrix for group-specific effects with different characteristics. We use the following formula\n\nf1 = ~ (0 + day | Dog) + (1 | Side / Dog)\n\nwhere each part can be interpreted as follows\n\n(0 + day | Dog) means that day has a group-specific slope for each Dog. This is usually known as a random slope. The 0 indicates not to add the default group-specific intercept (because it‚Äôs added next).\n(1 | Side / Dog) is equivalent to (1 | Side) + (1 | Dog:Side). This means there‚Äôs a varying intercept for each Side and a varying intercept for each combination of Dog and Side. In other words, we have a nested group-specific intercept, where Dog is nested within Side.\n\n\nlme4_terms = mkReTrms(findbars(f1), model.frame(subbars(f1), data = Pixel))\n\nlme4_terms contains much more information than what we need for this post. We mostly use lme4_terms$Ztlist, which is a list that contains the transpose of the group-specific effects model matrix, separated by term. These matrices are stored as sparse matrices of dgCMatrix class. If we want to have the sub-matrix for a given group-specific term as a base R matrix, we have to do as.matrix(t(lme4_terms$Ztlist$[[\"term\"]])).\n\nnames(lme4_terms$Ztlist)\n\n[1] \"1 | Dog:Side\"  \"0 + day | Dog\" \"1 | Side\"     \n\n\nWe have three group-specific terms. The first and the last ones are the group-specific intercepts we mentioned. These are the result of the nested group-specific intercept (1 | Side / Dog). Dog is nested within Side and consequently there‚Äôs an intercept varying among Side and another varying among Dog within Side. The second term, 0 + day | Dog, represents varying slope of day for each level of Dog.\nWe finally store the sub-matrix for each term in different objects that we‚Äôll later use when comparing results with those obtained with formulae.\n\nday_by_dog = as.matrix(t(lme4_terms$Ztlist$`0 + day | Dog`))\nintercept_by_side = as.matrix(t(lme4_terms$Ztlist$`1 | Side`))\nintercept_by_side_dog = as.matrix(t(lme4_terms$Ztlist$`1 | Dog:Side`))\n\nOn the other hand, in Python, we use design_matrices() from the formulae library to obtain a DesignMatrices object. All the information associated with the group-specific terms is contained in the .group attribute and the sub-matrix corresponding to a particular term is accessed with .group[term_name].\n\ndm = design_matrices(\"(0 + day | Dog) + (1 | Side / Dog)\", r.Pixel)\n\nThere‚Äôs a dictionary called terms_info within dm.group. To see the names of the group-specific effects we just retrieve the keys.\n\ndm.group.terms.keys()\n\ndict_keys(['day|Dog', '1|Side', '1|Side:Dog'])\n\n\nNames differ a little with the ones from lme4, but they represent the same thing.\n\nday_by_dog = dm.group['day|Dog']\nintercept_by_side = dm.group['1|Side']\nintercept_by_side_dog = dm.group['1|Side:Dog']\n\nNow let‚Äôs compare those matrices!"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#design-matrices-for-daydog",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#design-matrices-for-daydog",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Design matrices for (day|Dog)",
    "text": "Design matrices for (day|Dog)\nRectangles in the following plot correspond to the cells in the matrix. The lowest value for day is 0, represented by violet, and the highest value is 21, represented by yellow. The 10 columns represent the 10 groups in Dog, and the rows represent the observations in Pixel. Here, and also in the other cases, the left panel contains the matrix obtained with lme4 and the right panel the one produced with formulae.\n\n\n\n\n\n\n\n\n\nIn this first case, both panels are representing the same data so we can happily conclude the result obtained with formulae matches the one from lme4. Yay!!\nBut we‚Äôre humans and our eyes can fail so it‚Äôs better to always check appropiately with\n\nall(py$day_by_dog == day_by_dog)\n\n[1] TRUE\n\n\n\nDesign matrices for (1|Side)\nHere the first column represents Side == \"L\" and the second column represents Side == \"R\". Since we‚Äôre dealing with an intercept, violet means 0 and yellow means 1. In this case it is much easier to see both results match.\n\n\n\n\n\n\n\n\n\n\nall(py$intercept_by_side == intercept_by_side)\n\n[1] TRUE\n\n\n\n\nDesign matrices for (1|Side:Dog)\nBut things are not always as one wishes. It‚Äôs clear from the following plot that both matrices aren‚Äôt equal here.\n\n\n\n\n\n\n\n\n\nBut don‚Äôt worry. We‚Äôre not giving up. We still have things to do1. We can check what are the groups being represented in the columns of the matrices we‚Äôre plotting.\n\ncolnames(intercept_by_side_dog)\n\n [1] \"1:L\"  \"1:R\"  \"10:L\" \"10:R\" \"2:L\"  \"2:R\"  \"3:L\"  \"3:R\"  \"4:L\"  \"4:R\" \n[11] \"5:L\"  \"5:R\"  \"6:L\"  \"6:R\"  \"7:L\"  \"7:R\"  \"8:L\"  \"8:R\"  \"9:L\"  \"9:R\" \n\n\n\ndm.group.terms[\"1|Side:Dog\"].labels\n\n['1|Side[L]:Dog[1]', '1|Side[L]:Dog[10]', '1|Side[L]:Dog[2]', '1|Side[L]:Dog[3]', '1|Side[L]:Dog[4]', '1|Side[L]:Dog[5]', '1|Side[L]:Dog[6]', '1|Side[L]:Dog[7]', '1|Side[L]:Dog[8]', '1|Side[L]:Dog[9]', '1|Side[R]:Dog[1]', '1|Side[R]:Dog[10]', '1|Side[R]:Dog[2]', '1|Side[R]:Dog[3]', '1|Side[R]:Dog[4]', '1|Side[R]:Dog[5]', '1|Side[R]:Dog[6]', '1|Side[R]:Dog[7]', '1|Side[R]:Dog[8]', '1|Side[R]:Dog[9]']\n\n\nAnd there it is! Matrices differ because columns are representing different groups. In lme4, groups are looping first along Dog and then along Side, while in formulae it is the other way around.\nWe can simply re-order the columns of one of the matrices and generate and check whether they match or not.\n\nintercept_by_side_dog_f = as.data.frame(py$intercept_by_side_dog)\ncolnames(intercept_by_side_dog_f) = py$dm$group$terms[[\"1|Side:Dog\"]]$groups\nnames_lme4_order = paste(\n  rep(c(\"L\", \"R\"), 10),\n  rep(c(1, 10, 2, 3, 4, 5, 6, 7, 8, 9), each = 2),\n  sep = \":\"\n)\nintercept_by_side_dog_f = intercept_by_side_dog_f[names_lme4_order] %&gt;%\n  as.matrix() %&gt;%\n  unname()\n\n\n\n\n\n\n\n\n\n\n\nall(intercept_by_side_dog_f == intercept_by_side_dog)\n\n[1] TRUE\n\n\nAnd there it is! Results match ü§©"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#another-formula",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#another-formula",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Another formula",
    "text": "Another formula\nThis other formula contains an interaction between categorical variables as the expression of the group-specific term, which is something we‚Äôre not covering above. In this case, we are going to subset the data so the design matrices are smaller and we can understand what‚Äôs going on with more ease.\n\n# Subset data\nPixel2 = Pixel %&gt;%\n  filter(Dog %in% c(1, 2, 3), day %in% c(2, 4, 6)) %&gt;%\n  mutate(Dog = forcats::fct_drop(Dog))\n# Create terms with lme4\nf2 = ~ day +  (0 + Dog:Side | day)\nlme4_terms = mkReTrms(findbars(f2), model.frame(subbars(f2), data = Pixel2))\ndog_and_side_by_day = as.matrix(t(lme4_terms$Ztlist$`0 + Dog:Side | day`))\n\nAnd now with design_matrices() in Python.\n\n# Create terms with\ndm = design_matrices(\"(0 + Dog:Side|day)\", r.Pixel2)\ndog_and_side_by_day = dm.group[\"Dog:Side|day\"]\n\n\nDesign matrix for (Dog:Side|day)\nAlthough this term is called slope, it is not actually a slope like the one for (day|Dog). Since both Dog and Side are categorical, the entries of this matrix consist of zeros and ones.\n\n\n\n\n\n\n\n\n\nWe have the same problem than above, matrices don‚Äôt match. So we know what to do: look at the groups represented in the columns.\n\ncolnames(dog_and_side_by_day)\n\n [1] \"2\" \"2\" \"2\" \"2\" \"2\" \"2\" \"4\" \"4\" \"4\" \"4\" \"4\" \"4\" \"6\" \"6\" \"6\" \"6\" \"6\" \"6\"\n\n\n\ndm.group.terms[\"Dog:Side|day\"].labels\n\n['Dog[1]:Side[L]|day[2.0]', 'Dog[1]:Side[R]|day[2.0]', 'Dog[2]:Side[L]|day[2.0]', 'Dog[2]:Side[R]|day[2.0]', 'Dog[3]:Side[L]|day[2.0]', 'Dog[3]:Side[R]|day[2.0]', 'Dog[1]:Side[L]|day[4.0]', 'Dog[1]:Side[R]|day[4.0]', 'Dog[2]:Side[L]|day[4.0]', 'Dog[2]:Side[R]|day[4.0]', 'Dog[3]:Side[L]|day[4.0]', 'Dog[3]:Side[R]|day[4.0]', 'Dog[1]:Side[L]|day[6.0]', 'Dog[1]:Side[R]|day[6.0]', 'Dog[2]:Side[L]|day[6.0]', 'Dog[2]:Side[R]|day[6.0]', 'Dog[3]:Side[L]|day[6.0]', 'Dog[3]:Side[R]|day[6.0]']\n\n\nBut this they represent the same groups2. We can look if there‚Äôs a difference in how the interactions are ordered within each group.\n\nlme4_terms$cnms\n\n$day\n[1] \"Dog1:SideL\" \"Dog2:SideL\" \"Dog3:SideL\" \"Dog1:SideR\" \"Dog2:SideR\"\n[6] \"Dog3:SideR\"\n\n\nAnd again, thankfully, we see there‚Äôs a difference in how columns are being ordered. Let‚Äôs see if matrices match after we reorder the one obtained with formulae.\n\ndog_and_side_by_day_f = as.data.frame(py$dog_and_side_by_day)\ncolnames(dog_and_side_by_day_f) = py$dm$group$terms[[\"Dog:Side|day\"]]$labels\nside = rep(rep(c(\"L\", \"R\"), each = 3), 3)\ndog = rep(1:3, 6)\nday = rep(c(\"2.0\", \"4.0\", \"6.0\"), each = 6)\nnames_lme4_order = glue::glue(\"Dog[{dog}]:Side[{side}]|day[{day}]\")\ndog_and_side_by_day_f = dog_and_side_by_day_f[names_lme4_order] %&gt;%\n  as.matrix() %&gt;%\n  unname()\n\n\n\n\n\n\n\n\n\n\n\nall(dog_and_side_by_day_f == dog_and_side_by_day)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#conclusion",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#conclusion",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough formulae works differently than lme4, and has different goals, we showed that formulae produces the same design matrices as lme4 for the variety of examples we covered. While case-based comparisons like these are not what one should rely on when writing software, the examples here were really helpful when working on the implementation in formulae and writing the corresponding tests. And if this post helps someone to better understand what‚Äôs going on when working with design matrices associated with group-specific effects, it will have been even more worth it!"
  },
  {
    "objectID": "posts/2021-06-08_group-specific-effects-matrix/index.html#footnotes",
    "href": "posts/2021-06-08_group-specific-effects-matrix/index.html#footnotes",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI was undoubtedly talking to myself was quite disappointed at this time, wondering what I did wrong. Suffering the consequences of mistakes I wasn‚Äôt even aware I made. Well, not that dramatic. But now I‚Äôm happy the problem wasn‚Äôt real üòÖ‚Ü©Ô∏é\nWe have six 2s, six 4s and six 6s in both cases‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021-06-28_first-weeks-of-gsoc/index.html",
    "href": "posts/2021-06-28_first-weeks-of-gsoc/index.html",
    "title": "First weeks of GSoC",
    "section": "",
    "text": "I am really happy to participate in this Google Summer of Code season with NumFOCUS to contribute to the Bambi library. The coding period ranges from June 7 to August 16, with an intermediate evaluation taking place between July 12 and July 16."
  },
  {
    "objectID": "posts/2021-06-28_first-weeks-of-gsoc/index.html#overview",
    "href": "posts/2021-06-28_first-weeks-of-gsoc/index.html#overview",
    "title": "First weeks of GSoC",
    "section": "Overview",
    "text": "Overview\nMy project is called Extend available models and default priors in Bambi. The main goal of this project is to add new families of generalized linear models, such as beta regression, robust linear regression (i.e.¬†linear model with error following a T-Student distribution)1 as well as multinomial regression. However, this raises a second problem, which is about default priors distributions.\nDefault priors in Bambi are limited to the families implemented in the GLM module instatsmodels, which does not include the families mentioned above. For this reason, it is first necessary to incorporate alternative automatic priors so new families work without requiring the user to manually specify priors.\nTherefore, these first weeks of the coding period were centered around understanding how default priors work on other high-level modeling packages such as brms and rstanarm, how to translate their ideas into PyMC3 code, and finally how to implement everything within Bambi."
  },
  {
    "objectID": "posts/2021-06-28_first-weeks-of-gsoc/index.html#alternative-default-priors",
    "href": "posts/2021-06-28_first-weeks-of-gsoc/index.html#alternative-default-priors",
    "title": "First weeks of GSoC",
    "section": "Alternative default priors",
    "text": "Alternative default priors\nCurrently, Bambi uses maximum likelihood estimates in the construction of its default priors. There are two limitations associated with this approach. First, current default priors don‚Äôt exist whenever uniquely identifiable maximum likelihood estimates don‚Äôt exist (e.g.¬†\\(p &gt; n\\) or complete separation scenarios). Secondly, these estimates are obtained via the GLM module in statsmodels, which means default priors can only be obtained for families made available in statsmodels.\nBased on the available documentation and simulations I‚Äôve done, I decided to implement alternative default priors that are much like the default priors in rstanarm. These priors aim to be weakly-informative in most scenarios and do not depend on maximum likelihood estimates. Their documentation is excellent and it was a great guide for my implementation.\nThis is the PR where I implement alternative default priors inspired on rstanarm default priors. In addition, I also implement LKJ prior for the correlation matrices of group-specific effects.\n\nHow to invoke alternative default priors\nThe Model() class has gained one new argument, automatic_priors, that can be equal to \"default\" to use Bambi‚Äôs default method, or \"rstanarm\" to use the alternative implementation2.\nmodel = bmb.Model(\"y ~ x + z\", data, automatic_priors=\"rstanarm\")\n\n\nHow to use LKJ priors for correlation matrices of group-specific effects\nGroup-specific effects can now have non-independent priors. Instead of using independent normal distributions, we can use a multivariate normal distribution whose correlation matrix has an LKJ prior distribution. This distribution depends on a parameter \\(\\eta &gt; 0\\). If \\(\\eta=1\\), the LJK prior is jointly uniform over all correlation matrices of the same dimension. If \\(\\eta &gt;1\\) increases, the mode of the distribution is the identity matrix. The larger the value of \\(\\eta\\) the more sharply peaked the density is at the identity matrix.\nModel has an argument priors_cor where we can pass a dictionary to indicate which groups are going to have a LKJ prior. The keys of the dictionary are the names of the groups, and the values are the values for \\(\\eta\\).\nIn the following model, we have a varying intercept and varying slope for the groups given by group. These varying effects have a multivariate normal prior whose covariance matrix depends on a correlation matrix that has a LKJ hyperprior with \\(\\eta=1\\).\nmodel = bmb.Model(\"y ~ x + (x|group)\", data, priors_cor={\"group\": 1})"
  },
  {
    "objectID": "posts/2021-06-28_first-weeks-of-gsoc/index.html#footnotes",
    "href": "posts/2021-06-28_first-weeks-of-gsoc/index.html#footnotes",
    "title": "First weeks of GSoC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese two distributions are not members of the exponential family so using them as the distribution of the random component does not result in a generalized linear model in a strict sense. But I would usually refer to them as GLMs since the linear predictor, link function, and random component properties are still present.‚Ü©Ô∏é\nBoth the argument name and the options may change‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021-08-17_gsoc-2021-final-evaluation/index.html",
    "href": "posts/2021-08-17_gsoc-2021-final-evaluation/index.html",
    "title": "GSOC 2021: Final evaluation",
    "section": "",
    "text": "In this short blogpost, I‚Äôm going to summarize my contributions to the Bambi library during this Google Summer of Code.\nGSoC has been great. I‚Äôve learnt so much during the past weeks. And I‚Äôm obviously eager to keep learning and doing stuff with Bambi in the future. The following is a summary of what we were able to achieve during these time of code:\n\nImplemented new default priors #360, #385.\nAdded new Student-T family #367.\nAdded new Beta family #368.\nImplemented predictions #372.\nImproved internal model specification by splitting it into smaller and composable classes #366.\nAdded the new Binomial family #386. This also implied some changes in its sibling project, formulae.\n\nThis, with many other smaller changes or improvements that you can find here were included in Bambi 0.6.0.\nOn the other hand, the items on my original proposal that left to do are multinomial regression and ordered categorical terms. I‚Äôve started to do some work on the formulae side, but these features require a more involved work in Bambi, and thus it is left for future contributions.\nTo conclude, I want to thank Google for having such an amazing program and everyone who contributed or helped me to contribute to Bambi. Specially, I want to recognize the the work of my mentors Ravin Kumar and Thomas Wiecki, and my director Osvaldo Martin for all the support, feedback, and work during this program."
  },
  {
    "objectID": "posts/2025-08-12_regression-with-fixed-intercept/index.html",
    "href": "posts/2025-08-12_regression-with-fixed-intercept/index.html",
    "title": "When to fix the intercept",
    "section": "",
    "text": "I was working on a model where I knew the value of \\(Y\\) when \\(X = 0\\). This left me in the uncomfortable position of deciding whether to give the intercept a strong prior or simply fix it to a constant value. Since I was too lazy to work out the math by hand, I decided to go for a computationally assisted approach to determine whether that choice made sense.\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm"
  },
  {
    "objectID": "posts/2025-08-12_regression-with-fixed-intercept/index.html#where-it-fails",
    "href": "posts/2025-08-12_regression-with-fixed-intercept/index.html#where-it-fails",
    "title": "When to fix the intercept",
    "section": "Where it fails",
    "text": "Where it fails\nIn the past, I‚Äôve used the heights dataset from Karl Pearson and Alice Lee to teach linear regression. It‚Äôs a classic dataset where the goal is to predict a daughter‚Äôs height (\\(Y\\)) from her mother‚Äôs height (\\(X\\)). Let‚Äôs have a quick look at it.\n\n\nCode\nurl_heights = \"https://raw.githubusercontent.com/tomicapretto/introduccion_pymc/main/datos/heights.txt\"\ndf_heights = pd.read_table(url_heights, sep=\" \")\ndf_heights\n\n\n\n\n\n\n\n\n\ndaughter_height\nmother_height\n\n\n\n\n0\n52.5\n59.5\n\n\n1\n52.5\n59.5\n\n\n2\n53.5\n59.5\n\n\n3\n53.5\n59.5\n\n\n4\n55.5\n59.5\n\n\n...\n...\n...\n\n\n5519\n71.5\n70.5\n\n\n5520\n73.5\n63.5\n\n\n5521\n73.5\n63.5\n\n\n5522\n73.5\n63.5\n\n\n5523\n73.5\n63.5\n\n\n\n\n5524 rows √ó 2 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel with intercept\nA simple linear regression model with both an intercept and a slope is an obvious choice.\n\\[\nY_i \\mid X_i = x_i \\sim \\text{Normal}(\\alpha + \\beta x_i, \\sigma^2)\n\\]\nAnd in Python it just takes us a few lines of code:\n\ny = df_heights[\"daughter_height\"].to_numpy()\nX = np.column_stack([np.ones_like(y), df_heights[\"mother_height\"]])\nmodel = sm.OLS(y, X).fit()\n\nThe parameter summary is the following:\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n29.7984\n0.790\n37.703\n0.000\n28.249\n31.348\n\n\nx1\n0.5449\n0.013\n43.125\n0.000\n0.520\n0.570\n\n\n\n\n\nIf you‚Äôve ever had to interpret the coefficients of a regression model, you might have said: ‚ÄúThe average height of daughters whose mothers are 0 inches tall is 29.8 inches‚Äù. Of course, it makes no sense.\n\n\nModel without intercept\nA natural first reaction might be to question whether you need an intercept at all. In that case, you might go ahead and fit the following model without one:\n\\[\nY_i \\mid X_i = x_i \\sim \\text{Normal}(\\beta x_i, \\sigma^2)\n\\]\n\nmodel_no_intercept = sm.OLS(y, X[:, 1]).fit()\n\nNow, the parameter summary contains a single record for the slope:\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nx1\n1.0210\n0.001\n1871.328\n0.000\n1.020\n1.022\n\n\n\n\n\n\n\nComparison\nInstead of focusing on the exact values of the estimated slopes, let‚Äôs look at the fitted regression lines overlaid on the original scatterplot.\n\n\nCode\nfig, ax = plt.subplots()\nax.scatter(df_heights[\"mother_height\"], df_heights[\"daughter_height\"], s=10)\nax.set(xlabel=\"Mother height (in)\", ylabel=\"Daughter height (in)\", xlim=(50, 75), ylim=(50, 75));\n\nax.axline((0, model.params[0]), slope=model.params[1], color=\"C3\", lw=2, label=\"Model with unknown $\\\\alpha$ ($\\\\hat{\\\\alpha}=29.8$)\")\nax.axline((0, 0), slope=model_no_intercept.params[0], color=\"0.3\", lw=2, label=\"Model with fixed $\\\\alpha$ ($\\\\alpha=0$)\")\nax.legend();\n\n\n\n\n\n\n\n\n\nAt first glance, the two models produce remarkably different regression lines, but it is not immediately clear which one best describes the association between the heights. To shed some light on this, let‚Äôs look at the RMSE for each model:\n\ndef compute_rmse(y, y_hat):\n    return np.sqrt(np.mean((y - y_hat) ** 2))\n\nrmse_1 = compute_rmse(y, model.predict(X))\nrmse_2 = compute_rmse(y, model_no_intercept.predict(X[:, 1]))\n\nprint(f\"Model with intercept RMSE: {rmse_1:.3f}\")\nprint(f\"Model without intercept RMSE: {rmse_2:.3f}\")\n\nModel with intercept RMSE: 2.262\nModel without intercept RMSE: 2.536\n\n\n\n\nConclusion\nThe model without an intercept, though initially appealing in theory, ultimately produces larger errors. This is something we could have expected: forcing the intercept to a fixed value, especially when \\(X = 0\\) lies outside the range of the observed data, can bias the slope estimate.\nAs George Box famously remarked, all models are wrong, but some are useful. Usefulness often depends on the range of the explanatory variables, and a model that performs well within a given range may fail to do so outside it."
  },
  {
    "objectID": "posts/2025-08-12_regression-with-fixed-intercept/index.html#where-it-works",
    "href": "posts/2025-08-12_regression-with-fixed-intercept/index.html#where-it-works",
    "title": "When to fix the intercept",
    "section": "Where it works",
    "text": "Where it works\nThe previous example made it clear that haphazardly fixing the intercept to a value can have unintended consequences.\nHowever, in the problem that motivated this blog post, I do have a solid basis for fixing the intercept to a specific value.\nLet‚Äôs now run a simulation in which the model we use matches the underlying data generating process and \\(X=0\\) lies within the range of observed data.\nThis turn, we will simulate data using a linear regression model with \\(\\alpha=2\\), \\(\\beta=1\\), and \\(\\sigma=2\\).\nWe will run \\(S=1000\\) iterations where we observe \\(N=50\\) data points with \\(X \\in [0, 10]\\). In the first scenario, we will estimate both \\(\\alpha\\) and \\(\\beta\\). In the second, we will fix \\(\\alpha\\) to its true value, and only estimate \\(\\beta\\). In both cases, we will record the point estimates as well as the 95% confidence interval for the slope as reported by statsmodels.\nTo illustrate, here‚Äôs what the simulated data will look like in any given iteration:\n\n\nCode\nN = 50  # Number of data points\nS = 1000 # Number of simulations\n\na_true = 2.0 # True intercept\nb_true = 1.0 # True slope\nsigma_true = 2.0 # True residual standard deviation\n\nrng = np.random.default_rng(1234)\n\nx = np.linspace(0, 10, num=N)\ny = rng.normal(loc=a_true + b_true * x, scale=sigma_true)\n\nfig, ax = plt.subplots()\nax.scatter(x, y, label=\"Simulated data\")\nax.axline((0, a_true), slope=b_true, color=\"C3\", lw=2, label=\"True regression line\")\nax.set(xlabel=\"x\", ylabel=\"y\")\nax.legend();\n\n\n\n\n\n\n\n\n\n\nScenario 1: Unknown intercept\nHere we just simulate datasets and estimate \\(\\alpha\\) and \\(\\beta\\) via classic OLS.\nLet‚Äôs run the simulation and explore the sampling distribution of \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\).\n\n\nCode\nunknown_intercept_output = {\n    \"intercept\": [],\n    \"slope\": [],\n    \"slope_ci\": []\n}\n\nX = np.column_stack([np.ones_like(x),x])\n\nfor s in range(S):\n    y = rng.normal(loc=a_true + b_true * x, scale=sigma_true)\n    model = sm.OLS(y, X).fit()\n    conf_int = model.conf_int()\n\n    unknown_intercept_output[\"intercept\"].append(model.params[0].item())\n    unknown_intercept_output[\"slope\"].append(model.params[1].item())\n    unknown_intercept_output[\"slope_ci\"].append(conf_int[1])\n\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4), sharey=True)\nfig.subplots_adjust(wspace=0.1)\n\naxes[0].hist(unknown_intercept_output[\"intercept\"], bins=30)\naxes[1].hist(unknown_intercept_output[\"slope\"], bins=30)\n\naxes[0].set(xlabel=\"$\\\\hat{\\\\alpha}$\", yticks=[])\naxes[1].set(xlabel=\"$\\\\hat{\\\\beta}$\");\n\naxes[0].axvline(a_true, color=\"C3\", lw=2.5);\naxes[1].axvline(b_true, color=\"C3\", lw=2.5);\n\n\n\n\n\n\n\n\n\nThere are no surprises here: both distributions are centered around the true values of \\(\\alpha\\) and \\(\\beta\\).\n\n\nScenaro 2: Fixed intercept\nIn this other scenario, we again simulate datasets and estimate parameters via OLS, but this time we fix the value of the intercept to \\(\\alpha=2\\).\n\n\nCode\nknown_intercept_output = {\n    \"slope\": [],\n    \"slope_ci\": []\n}\n\nfor s in range(S):\n    y = rng.normal(loc=a_true + b_true * x, scale=sigma_true)\n    model = sm.OLS(y - a_true, x).fit()\n    known_intercept_output[\"slope\"].append(model.params[0].item())\n    known_intercept_output[\"slope_ci\"].append(model.conf_int()[0])\n\nfig, ax = plt.subplots()\nax.hist(known_intercept_output[\"slope\"], bins=30)\nax.set(xlabel=\"$\\\\hat{\\\\beta}$\", yticks=[])\nax.axvline(b_true, color=\"red\", lw=2);\n\n\n\n\n\n\n\n\n\nThe sampling distribution of \\(\\hat{\\beta}\\) is again centered on its true value, but it is now more concentrated around that center than before.\nTo finalize, we can also compare the sampling distribution of the length of the 95% CI for \\(\\hat{\\beta}\\) between the two scenarios.\n\n\nCode\nslope_cis_1 = np.vstack(unknown_intercept_output[\"slope_ci\"])\nslope_cis_2 = np.vstack(known_intercept_output[\"slope_ci\"])\n\nbins = np.linspace(0.1, 0.6, num=30)\n\nfig, ax = plt.subplots()\nax.hist(\n    np.diff(slope_cis_1, axis=-1).flatten(),\n    density=True,\n    color=\"C0\",\n    bins=30,\n    alpha=0.8,\n    label=\"Model with unknown $\\\\alpha$\",\n)\n\nax.hist(\n    np.diff(slope_cis_2, axis=-1).flatten(),\n    density=True,\n    color=\"C1\",\n    bins=30,\n    alpha=0.8,\n    label=\"Model with fixed $\\\\alpha$\",\n)\n\nax.set(xlabel=\"95% CI length\", yticks=[])\nax.legend();\n\n\n\n\n\n\n\n\n\nNot only does the sampling distribution of \\(\\hat{\\beta}\\) have lower uncertainty, but the confidence intervals in each individual iteration are also more concentrated around the estimate."
  },
  {
    "objectID": "posts/2025-08-12_regression-with-fixed-intercept/index.html#summary",
    "href": "posts/2025-08-12_regression-with-fixed-intercept/index.html#summary",
    "title": "When to fix the intercept",
    "section": "Summary",
    "text": "Summary\nEven if the intercept‚Äôs meaning doesn‚Äôt make sense, leaving it out of a regression model is usually a bad idea.\nThat said, there are scenarios where fixing the intercept to zero or to another meaningful value can be justified. And in such cases, if we are right, doing so may actually improve the estimation of other unknown parameters."
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html",
    "href": "posts/2022-06-12_lkj-prior/index.html",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "Throughout this blogpost, I will be working with the famous sleepstudy dataset. I‚Äôm going to estimate a hierarchical linear regression with both varying intercepts and varying slopes. The goal is to understand how to place non-independent priors for the group-specific effects in PyMC as efficiently as possible.\nThe sleepstudy dataset is derived from the study described in Belenky et al.¬†(2003) and popularized in the lme4 R package. This dataset contains the average reaction time per day (in milliseconds) on a series of tests for the most sleep-deprived group in a sleep deprivation study. The first two days of the study are considered as adaptation and training, the third day is a baseline, and sleep deprivation started after day 3. The subjects in this group were restricted to 3 hours of sleep per night.\nWith that said, let‚Äôs get into the code!\n\nimport arviz as az\nimport aesara.tensor as at\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\n\n\n%matplotlib inline\naz.style.use(\"arviz-darkgrid\")\nmpl.rcParams[\"figure.facecolor\"] = \"white\"\n\nLet‚Äôs get started by downloading and exploring sleepstudy dataset.\n\nurl = \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/lme4/sleepstudy.csv\"\ndata = pd.read_csv(url, index_col = 0)\n\nThe following is a description of the variables we have in the dataset.\n\nReaction: Average of the reaction time measurements on a given subject for a given day.\nDays: Number of days of sleep deprivation.\nSubject: The subject ID.\n\n\nprint(f\"There are {len(data)} observations.\")\ndata.head()\n\nThere are 180 observations.\n\n\n\n\n\n\n\n\n\nReaction\nDays\nSubject\n\n\n\n\n1\n249.5600\n0\n308\n\n\n2\n258.7047\n1\n308\n\n\n3\n250.8006\n2\n308\n\n\n4\n321.4398\n3\n308\n\n\n5\n356.8519\n4\n308\n\n\n\n\n\n\n\n\nprint(f\"Days range from {data['Days'].min()} to {data['Days'].max()}.\")\nprint(f\"There are J={data['Subject'].unique().size} subjects.\")\n\nDays range from 0 to 9.\nThere are J=18 subjects.\n\n\nLet‚Äôs explore the evolution of the reaction times through the days for every subject.\n\ndef plot_data(data, figsize=(16, 7.5)):\n    fig, axes = plt.subplots(2, 9, figsize=figsize, sharey=True, sharex=True)\n    fig.subplots_adjust(left=0.075, right=0.975, bottom=0.075, top=0.925, wspace=0.03)\n\n    for i, (subject, ax) in enumerate(zip(data[\"Subject\"].unique(), axes.ravel())):\n        idx = data.index[data[\"Subject\"] == subject].tolist()\n        days = data.loc[idx, \"Days\"].values\n        reaction = data.loc[idx, \"Reaction\"].values\n\n        # Plot observed data points\n        ax.scatter(days, reaction, color=\"C0\", ec=\"black\", alpha=0.7)\n\n        # Add a title\n        ax.set_title(f\"Subject: {subject}\", fontsize=14)\n\n    ax.xaxis.set_ticks([0, 2, 4, 6, 8])\n    fig.text(0.5, 0.02, \"Days\", fontsize=14)\n    fig.text(0.03, 0.5, \"Reaction time (ms)\", rotation=90, fontsize=14, va=\"center\")\n\n    return fig, axes\n\n\nplot_data(data);\n\n\n\n\n\n\n\n\nFor most of the subjects, there‚Äôs a clear positive association between Days and Reaction time. Reaction times increase as people accumulate more days of sleep deprivation. Participants differ in the initial reaction times as well as in the association between sleep deprivation and reaction time. Reaction times increase faster for some subjects and slower for others. Finally, the relationship between Days and Reaction time presents some deviations from linearity within the panels, but these are neither substantial nor systematic.\n\n\nThe model we‚Äôre going to build today is a hierarchical linear regression, with a Gaussian likelihood. In the following description, I use the greek letter \\(\\beta\\) to refer to common effects and the roman letter \\(u\\) to refer to group-specific (or varying) effects.\n\\[\ny_{ij} = \\beta_0 + u_{0j} + \\left( \\beta_1 + u_{1j} \\right) \\cdot {\\text{Days}} + \\epsilon_i\n\\]\nwhere\n\\[\n\\begin{aligned}\ny_{ij} &= \\text{Reaction time for the subject } j \\text{ on day } i \\\\\n\\beta_0 &= \\text{Intercept common to all subjects} \\\\\n\\beta_1 &= \\text{Days slope common to all subjects} \\\\\nu_{0j} &= \\text{Intercept deviation of the subject } j \\\\\nu_{1j} &= \\text{Days slope deviation of the subject } j \\\\\n\\epsilon_i &= \\text{Residual random error}\n\\end{aligned}\n\\]\nwe also have\n\\[\n\\begin{aligned}\ni = 1, \\cdots, 10 \\\\\nj = 1, \\cdots, 18\n\\end{aligned}\n\\]\nwhere \\(i\\) indexes Days and \\(j\\) indexes subjects.\nFrom the mathematical description we notice both the intercept and the slope are made of two components. The intercept is made of a common or global intercept \\(\\beta_0\\) and subject-specific deviations \\(u_{0j}\\). The same logic applies for the slope with both \\(\\beta_1\\) and \\(u_{1j}\\).\n\n\n\n\n\nFor the common effects, we Guassian independent priors.\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal}\\left(\\bar{y}, \\sigma_{\\beta_0}\\right) \\\\\n\\beta_1 \\sim \\text{Normal}\\left(0, \\sigma_{\\beta_1}\\right)\n\\end{array}\n\\]\nBambi centers the prior for the intercept at \\(\\bar{y}\\), so do we. For \\(\\sigma_{\\beta_0}\\) and \\(\\sigma_{\\beta_1}\\) I‚Äôm going to use 50 and 10 respectively. We‚Äôll use these same priors for all the variants of the model above.\n\n\n\n\\[\n\\begin{aligned}\n\\epsilon_i &\\sim \\text{Normal}(0, \\sigma) \\\\\n\\sigma &\\sim \\text{HalfStudentT}(\\nu, \\sigma_\\epsilon)\n\\end{aligned}\n\\]\nWhere \\(\\nu\\) and \\(\\sigma_\\epsilon\\), both positive constants, represent the degrees of freedom and the scale parameter, respectively.\n\n\n\nThroughout this post we‚Äôll propose the following variants for the priors of the group-specific effects.\n\nIndependent priors.\nCorrelated priors.\n\nUsing LKJCholeskyCov.\nUsing LKJCorr.\nUsign LKJCorr with non-trivial standard deviation.\n\n\nEach of them will be described in more detail in its own section.\nThen we create subjects and subjects_idx. These represent the subject IDs and their indexes. These are used with the distribution of the group-specific coefficients. We also have the coords that we pass to the model and the mean of the prior for the intercept\n\n# Subjects and subjects index\nsubjects, subjects_idx = np.unique(data[\"Subject\"], return_inverse=True)\n\n# Coordinates to handle dimensions of PyMC distributions and use better labels\ncoords = {\"subject\": subjects}\n\n# Response mean -- Used in the prior for the intercept\ny_mean = data[\"Reaction\"].mean()\n\n# Days variable\ndays = data[\"Days\"].values\n\n\n\n\n\n\n\n\\[\n\\begin{array}{lr}\nu_{0j} \\sim \\text{Normal} \\left(0, \\sigma_{u_0}\\right) & \\text{for all } j:1,..., 18 \\\\\nu_{1j} \\sim \\text{Normal} \\left(0, \\sigma_{u_1}\\right) & \\text{for all } j:1,..., 18\n\\end{array}\n\\]\nwhere the hyperpriors are\n\\[\n\\begin{array}{c}\n\\sigma_{u_0} \\sim \\text{HalfNormal} \\left(\\tau_0\\right) \\\\\n\\sigma_{u_1} \\sim \\text{HalfNormal} \\left(\\tau_1\\right)\n\\end{array}\n\\]\nwhere \\(\\tau_0\\) and \\(\\tau_1\\) represent the standard deviations of the hyperpriors. These are fixed positive constants. We set them to the same values than \\(\\sigma_{\\beta_0}\\) and \\(\\sigma_{\\beta_1}\\) respectively.\n\nwith pm.Model(coords=coords) as model_independent:\n    # Common effects\n    Œ≤0 = pm.Normal(\"Œ≤0\", mu=y_mean, sigma=50)\n    Œ≤1 = pm.Normal(\"Œ≤1\", mu=0, sigma=10)\n    \n    # Group-specific effects\n    # Intercept\n    œÉ_u0 = pm.HalfNormal(\"œÉ_u0\", sigma=50)\n    u0 = pm.Normal(\"u0\", mu=0, sigma=œÉ_u0, dims=\"subject\")\n    \n    # Slope\n    œÉ_u1 = pm.HalfNormal(\"œÉ_u1\", sigma=10)   \n    u1 = pm.Normal(\"u1\", mu=0, sigma=œÉ_u1, dims=\"subject\")\n   \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", Œ≤0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (Œ≤1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    Œº = pm.Deterministic(\"Œº\", intercept + slope)\n    \n    # Residual standard deviation\n    œÉ = pm.HalfStudentT(\"œÉ\", nu=4, sigma=50)\n    \n    # Response\n    y = pm.Normal(\"y\", mu=Œº, sigma=œÉ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_independent)\n\n\n\n\n\n\n\n\n\nwith model_independent:\n    idata_independent = pm.sample(draws=1000, chains=4, random_seed=1234)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [Œ≤0, Œ≤1, œÉ_u0, u0, œÉ_u1, u1, œÉ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:31&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 33 seconds.\n\n\n\naz.plot_trace(\n    idata_independent, \n    var_names=[\"Œ≤0\", \"Œ≤1\", \"u0\", \"u1\", \"œÉ\", \"œÉ_u0\", \"œÉ_u1\"],\n    combined=True, \n    chain_prop={\"ls\": \"-\"}\n);\n\n\n\n\n\n\n\n\n\ndef plot_predictions(data, idata, figsize=(16, 7.5)):\n    # Plot the data\n    fig, axes = plot_data(data, figsize=figsize)\n    \n    # Extract predicted mean\n    reaction_mean = idata.posterior[\"Œº\"]\n\n    for subject, ax in zip(subjects, axes.ravel()):\n        idx = (data[\"Subject\"]== subject).values\n        days = data.loc[idx, \"Days\"].values\n\n        # Plot highest density interval / credibility interval\n        az.plot_hdi(days, reaction_mean[..., idx], color=\"C0\", ax=ax)\n\n        # Plot mean regression line\n        ax.plot(days, reaction_mean[..., idx].mean((\"chain\", \"draw\")), color=\"C0\")\n    \n    return fig ,axes\n\n\nplot_predictions(data, idata_independent);\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of placing independent priors on \\(u_{0j}\\) and \\(u_{1j}\\), we place a multivariate normal prior on \\([u_{0j}, u_{1j}]^T\\), which allows for correlated group-specific effects.\n\\[\n\\begin{array}{lr}\n    \\left[\n        \\begin{array}{c}\n            u_{0j} \\\\\n            u_{0j}\n        \\end{array}\n    \\right]\n    \\sim \\text{MvNormal}\n    \\left(\n        \\left[\n            \\begin{array}{c}\n                0 \\\\\n                0\n            \\end{array}\n        \\right],\n        \\Sigma =\n        \\left[\n            \\begin{array}{cc}\n                \\sigma_{u_0}^2 & \\text{cov}(u_0, u_1) \\\\\n                \\text{cov}(u_0, u_1) & \\sigma_{u_1}^2\n            \\end{array}\n        \\right]\n    \\right)\n    &\n    \\text{for all } j:1,..., 18\n\\end{array}\n\\]\nand we use the same hyperpriors\n\\[\n\\begin{array}{c}\n\\sigma_{u_0} \\sim \\text{HalfNormal} \\left(\\tau_0\\right) \\\\\n\\sigma_{u_1} \\sim \\text{HalfNormal} \\left(\\tau_1\\right)\n\\end{array}\n\\]\n\n\nIn practice, we do not set a prior for the covariance matrix \\(\\Sigma\\). Instead, we set a prior on the correlation matrix \\(\\Omega\\) and use the following decomposition to recover the covariance matrix\n\\[\n\\begin{split}\n    \\Sigma & =\n    \\begin{pmatrix}\n        \\sigma_{u_0} & 0 \\\\\n        0 & \\sigma_{u_1}\n    \\end{pmatrix}\n    \\begin{pmatrix} 1 & \\rho_{u_0, u_1} \\\\ \\rho_{u_0, u_1} & 1 \\end{pmatrix}\n    \\begin{pmatrix}\n        \\sigma_{u_0} & 0 \\\\\n        0 & \\sigma_{u_1}\n    \\end{pmatrix} \\\\\n      & = \\text{diag}(\\sigma_u) \\ \\Omega \\ \\text{diag}(\\sigma_u)\n\\end{split}\n\\]\nA very popular and flexible alternative is to place an LKJ prior on the correlation matrix.\n\\[\n\\Omega \\sim \\text{LKJ}(\\eta), \\ \\  \\eta &gt; 0\n\\]\nLKJ stands for the Lewandowski-Kurowicka-Joe distribution. If \\(\\eta = 1\\) (our default choice), the prior is jointly uniform over all correlation matrices of the same dimension as \\(\\Omega\\). If \\(\\eta &gt; 1\\), then the mode of the distribution is the identity matrix. The larger the value of \\(\\eta\\) the more sharply peaked the density is at the identity matrix\n\n\n\nFor efficiency and numerical stability, the correlation matrix \\(\\Omega\\) can be parametrized by its (lower-triangular) Cholesky factor \\(L\\), which can be seen as the square root of \\(\\Omega\\)\n\\[\n\\boldsymbol{L}\\boldsymbol{L^T} = \\Omega = \\begin{pmatrix} 1 & \\rho_{u_0, u_1} \\\\ \\rho_{u_0, u_1} & 1 \\end{pmatrix}\n\\]\nIf \\(\\boldsymbol{Z}_{\\text{uncorr}}\\) is a \\(2\\times n\\) matrix where the rows are 2 samples from uncorrelated random variables, then\n\\[\n\\begin{split}\n    \\boldsymbol{Z}_{\\text{corr}} & =\n    \\begin{pmatrix} \\sigma_{u_0} & 0 \\\\ 0 & \\sigma_{u_1} \\end{pmatrix}\n    \\boldsymbol{L}\n    \\boldsymbol{Z}_{\\text{uncorr}} \\\\\n    & = \\text{diag}(\\sigma_u) \\boldsymbol{L} \\boldsymbol{Z}_{\\text{uncorr}}     \n\\end{split}\n\\]\nThen \\(\\boldsymbol{Z}_{\\text{corr}}\\), of shape \\((2, n)\\), contains a sample of size \\(n\\) of two correlated variables with the desired variances \\(\\sigma_{u_0}^2\\), \\(\\sigma_{u_1}^2\\), and correlation \\(\\rho_{u_0, u_1}\\).\n\n\n\n\nPyMC conveniently implements a distribution called LKJCholeskyCov. Here, n represents the dimension of the correlation matrix. eta is the parameter of the LKJ distribution. sd_dist is the prior distribution for the standard deviations of the group-specific effects. compute_corr=True means we want it to also return the correlation between the group-specific parameters and their standard deviations. store_in_trace=False means we don‚Äôt want to store the correlation and the standard deviations in the trace.\nBefore seeing the code, we note that sd_dist is not a random variable, but a stateless distribution (i.e.¬†the result of pm.Distribution.dist()).\n\ncoords.update({\"effect\": [\"intercept\", \"slope\"]})\n\nwith pm.Model(coords=coords) as model_lkj_cov:\n    ## Common effects\n    Œ≤0 = pm.Normal(\"Œ≤0\", mu=y_mean, sigma=50)\n    Œ≤1 = pm.Normal(\"Œ≤1\", mu=0, sigma=10)\n   \n    ## Group-specific effects\n    # Hyper prior for the standard deviations\n    u_œÉ = pm.HalfNormal.dist(sigma=np.array([50, 10]), shape=2)\n    \n    # Obtain Cholesky factor for the covariance\n    L, œÅ_u, œÉ_u = pm.LKJCholeskyCov(\n        \"L\", n=2, eta=1, sd_dist=u_œÉ, compute_corr=True, store_in_trace=False\n    )\n    \n    # Parameters\n    u_raw = pm.Normal(\"u_raw\", mu=0, sigma=1, dims=(\"effect\", \"subject\")) \n    u = pm.Deterministic(\"u\", at.dot(L, u_raw).T, dims=(\"subject\", \"effect\"))\n   \n    ## Separate group-specific terms \n    # Intercept\n    u0 = pm.Deterministic(\"u0\", u[:, 0], dims=\"subject\")\n    œÉ_u0 = pm.Deterministic(\"œÉ_u0\", œÉ_u[0])\n    \n    # Slope\n    u1 = pm.Deterministic(\"u1\", u[:, 1], dims=\"subject\")\n    œÉ_u1 = pm.Deterministic(\"œÉ_u1\", œÉ_u[1])\n\n    # Correlation\n    œÅ_u = pm.Deterministic(\"œÅ_u\", œÅ_u[0, 1])\n    \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", Œ≤0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (Œ≤1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    Œº = pm.Deterministic(\"Œº\", intercept + slope)\n    \n    ## Residual standard deviation\n    œÉ = pm.HalfStudentT(\"œÉ\", nu=4, sigma=56)\n    \n    ## Response\n    y = pm.Normal(\"y\", mu=Œº, sigma=œÉ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_lkj_cov)\n\n\n\n\n\n\n\n\n\nwith model_lkj_cov:\n    idata_lkj_cov = pm.sample(draws=1000, chains=4, random_seed=1234, target_accept=0.9)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [Œ≤0, Œ≤1, L, u_raw, œÉ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 01:04&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 66 seconds.\n\n\n\naz.plot_trace(\n    idata_lkj_cov, \n    var_names=[\"Œ≤0\", \"Œ≤1\", \"u0\", \"u1\", \"œÉ\", \"œÉ_u0\", \"œÉ_u1\", \"œÅ_u\"],\n    combined=True, \n    chain_prop={\"ls\": \"-\"}\n);\n\n\n\n\n\n\n\n\nFrom the traceplot of the correlation coefficient œÅ_u it looks like the group-specific intercept and slope are not related since the distribution is centered around zero.\nBut there‚Äôs another question we haven‚Äôt answered yet: Are the initial reaction times associated with how much the sleep deprivation affects the evolution of reaction times? Let‚Äôs create a scatterplot to visualize the joint posterior of the subject-specific intercepts and slopes. This chart uses different colors for the individuals.\n\nposterior_u0 = idata_lkj_cov.posterior[\"u0\"].values\nposterior_u1 = idata_lkj_cov.posterior[\"u1\"].values\n\nfig, ax = plt.subplots()\nfor subject in range(18):\n    # Not all the samples are drawn\n    x = posterior_u0[::10, :, subject]\n    y = posterior_u1[::10, :, subject]\n    ax.scatter(x, y, alpha=0.5)\n    \nax.axhline(c=\"k\", ls=\"--\", alpha=0.5)\nax.axvline(c=\"k\", ls=\"--\", alpha=0.5)\nax.set(xlabel=\"Subject-specific intercept\", ylabel=\"Subject-specific slope\");\n\n\n\n\n\n\n\n\nIf we look at the bigger picture, i.e omitting the groups, we can conclude there‚Äôs no association between the intercept and slope. In other words, having lower or higher intial reaction times does not say anything about how much sleep deprivation affects the average reaction time on a given subject.\nOn the other hand, if we look at the joint posterior for a given individual, we can see a negative correlation between the intercept and the slope. This is telling that, conditional on a given subject, the intercept and slope posteriors are not independent. However, it doesn‚Äôt imply anything about the overall relationship between the intercept and the slope, which is what we need if we want to know whether the initial time is associated with how much sleep deprivation affects the reaction time.\nLet‚Äôs check predictions\n\nplot_predictions(data, idata_lkj_cov);\n\n\n\n\n\n\n\n\n\n\n\nOne problem with LKJCholeskyCov is that its sd_dist argument must be a stateless distribution and thus we cannot use a customized distribution for the standard deviations of the group-specific effects.\nIf we want to use a random variable instead of a stateless distribution for the standard deviation of the group-specific effects, then we need to perform many steps manually. Let‚Äôs see how we can implement it!\n\nwith pm.Model(coords=coords) as model_lkj_corr:\n    # Common part\n    Œ≤0 = pm.Normal(\"Œ≤0\", mu=y_mean, sigma=50)\n    Œ≤1 = pm.Normal(\"Œ≤1\", mu=0, sigma=10)\n    \n    # Group-specific part   \n    œÉ_u = pm.HalfNormal(\"u_œÉ\", sigma=np.array([50, 10]), dims=\"effect\")\n    \n    # Triangular upper part of the correlation matrix\n    Œ©_triu = pm.LKJCorr(\"Œ©_triu\", eta=1, n=2)\n\n    # Construct correlation matrix\n    Œ© = pm.Deterministic(\n        \"Œ©\", \n        at.fill_diagonal(Œ©_triu[np.zeros((2, 2), dtype=np.int64)], 1)\n    )\n    \n    # Construct diagonal matrix of standard deviation\n    œÉ_diagonal = pm.Deterministic(\"œÉ_diagonal\", at.eye(2) * œÉ_u)\n    \n    # Compute covariance matrix\n    Œ£ = at.nlinalg.matrix_dot(œÉ_diagonal, Œ©, œÉ_diagonal)\n    \n    # Cholesky decomposition of covariance matrix\n    L = pm.Deterministic(\"L\", at.slinalg.cholesky(Œ£))\n    \n    # And finally get group-specific coefficients\n    u_raw = pm.Normal(\"u_raw\", mu=0, sigma=1, dims=(\"effect\", \"subject\")) \n    u = pm.Deterministic(\"u\", at.dot(L, u_raw).T, dims=(\"subject\", \"effect\"))\n\n    u0 = pm.Deterministic(\"u0\", u[:, 0], dims=\"subject\")\n    œÉ_u0 = pm.Deterministic(\"œÉ_u0\", œÉ_u[0])\n    \n    u1 = pm.Deterministic(\"u1\", u[:, 1], dims=\"subject\")\n    œÉ_u1 = pm.Deterministic(\"œÉ_u1\", œÉ_u[1])\n    \n    # Correlation\n    œÅ_u = pm.Deterministic(\"œÅ_u\", Œ©[0, 1])\n         \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", Œ≤0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (Œ≤1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    Œº = pm.Deterministic(\"Œº\", intercept + slope)\n    \n    œÉ = pm.HalfStudentT(\"œÉ\", nu=4, sigma=50)\n    y = pm.Normal(\"y\", mu=Œº, sigma=œÉ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_lkj_corr)\n\n\n\n\n\n\n\n\n\nwith model_lkj_corr:\n    idata_lkj_corr = pm.sample(draws=1000, chains=2, random_seed=1234, target_accept=0.9)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [Œ≤0, Œ≤1, u_œÉ, Œ©_triu, u_raw, œÉ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 01:15&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 76 seconds.\n\n\n\naz.plot_trace(\n    idata_lkj_corr, \n    var_names=[\"Œ≤0\", \"Œ≤1\", \"u0\", \"u1\", \"œÉ\", \"œÉ_u0\", \"œÉ_u1\", \"œÅ_u\"],\n    combined=True, \n    chain_prop={\"ls\": \"-\"}\n);\n\n\n\n\n\n\n\n\n\nplot_predictions(data, idata_lkj_corr);\n\n\n\n\n\n\n\n\n\n\n\nThis model is like the previous one, but œÉ_u is the result of multiplying several random variables. Rstanarm prior is introduced here\nNOTE: œÉ_u is what I would like to be able to pass to the sd_dist argument in pm.LKJCholeskyCov. Since I can only pass a stateless distribution, I need to perform all the steps manually.\n\nThe vector of variances is set equal to the product of a simplex vector \\(\\pi\\) ‚Äî which is non-negative and sums to 1 ‚Äî and the scalar trace: \\(J\\tau^2\\pi\\).\n[‚Ä¶]\nFor the simplex vector \\(\\pi\\) we use a symmetric Dirichlet prior which has a single concentration parameter \\(\\gamma &gt; 0\\).\n\nOn top of that, the \\(J\\tau^2\\pi\\) is scaled by the residual standard deviation as explained in this comment.\n\nJ = 2 # Order of covariance matrix\n\nwith pm.Model(coords=coords) as model_lkj_corr_2:\n    # Common part\n    Œ≤0 = pm.Normal(\"Œ≤0\", mu=y_mean, sigma=50)\n    Œ≤1 = pm.Normal(\"Œ≤1\", mu=0, sigma=10)\n    \n    # Residual SD \n    œÉ = pm.HalfStudentT(\"œÉ\", nu=4, sigma=50)\n    \n    # Group-specific part\n    # Begin of rstanarm approach ----------------------------------\n    œÑ = pm.Gamma(\"œÑ\", alpha=1, beta=1)\n    Œ£_trace = J * œÑ ** 2\n    œÄ = pm.Dirichlet(\"œÄ\", a=np.ones(J), dims=\"effect\")\n    œÉ_u = pm.Deterministic(\"b_œÉ\", œÉ * œÄ * (Œ£_trace) ** 0.5)\n    # End of rstanarm approach ------------------------------------\n    \n    # Triangular upper part of the correlation matrix\n    Œ©_triu = pm.LKJCorr(\"Œ©_triu\", eta=1, n=2)\n     \n    # Correlation matrix\n    Œ© = pm.Deterministic(\n        \"Œ©\", at.fill_diagonal(Œ©_triu[np.zeros((2, 2), dtype=np.int64)], 1.)\n    )\n\n    # Construct diagonal matrix of standard deviation\n    œÉ_u_diagonal = pm.Deterministic(\"œÉ_u_diagonal\", at.eye(2) * œÉ_u)\n    \n    # Covariance matrix\n    Œ£ = at.nlinalg.matrix_dot(œÉ_u_diagonal, Œ©, œÉ_u_diagonal)\n    \n    # Cholesky decomposition, lower triangular matrix.\n    L = pm.Deterministic(\"L\", at.slinalg.cholesky(Œ£))\n    u_raw = pm.Normal(\"u_raw\", mu=0, sigma=1, dims=(\"effect\", \"subject\")) \n    \n    u = pm.Deterministic(\"u\", at.dot(L, u_raw).T, dims=(\"subject\", \"effect\"))\n                         \n    u0 = pm.Deterministic(\"u0\", u[:, 0], dims=\"subject\")\n    œÉ_u0 = pm.Deterministic(\"œÉ_u0\", œÉ_u[0])\n    \n    u1 = pm.Deterministic(\"u1\", u[:, 1], dims=\"subject\")\n    œÉ_u1 = pm.Deterministic(\"œÉ_u1\", œÉ_u[1])\n    \n    # Correlation\n    œÅ_u = pm.Deterministic(\"œÅ_u\", Œ©[0, 1])\n         \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", Œ≤0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (Œ≤1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    Œº = pm.Deterministic(\"Œº\", intercept + slope)\n       \n    y = pm.Normal(\"y\", mu=Œº, sigma=œÉ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_lkj_corr_2)\n\n\n\n\n\n\n\n\n\nwith model_lkj_corr_2:\n    idata_lkj_corr_2 = pm.sample(draws=1000, chains=4, random_seed=1234, target_accept=0.9)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [Œ≤0, Œ≤1, œÉ, œÑ, œÄ, Œ©_triu, u_raw]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 03:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 181 seconds.\n\n\n\naz.plot_trace(\n    idata_lkj_corr_2,\n    var_names=[\"Œ≤0\", \"Œ≤1\", \"u0\", \"u1\", \"œÉ\", \"œÉ_u0\", \"œÉ_u1\", \"œÅ_u\", \"œÑ\", \"œÄ\"],\n);\n\n\n\n\n\n\n\n\n\nplot_predictions(data, idata_lkj_corr_2);\n\n\n\n\n\n\n\n\n\n\n\n\ngroups = [\n    [\"Œ≤0\", \"Œ≤1\", \"œÉ\"],\n    [\"u0\", \"œÉ_u0\"],\n    [\"u1\", \"œÉ_u1\"],\n]\n\nmodel_names = [\"Indepentent\", \"LKJCholeskyCov\", \"LKJCorr\", \"LKJCorr + rstanarm\"]\nfig, ax = plt.subplots(1, 3, figsize = (20, 10))\n\nfor idx, group in enumerate(groups):\n    az.plot_forest(\n        [idata_independent, idata_lkj_cov, idata_lkj_corr, idata_lkj_corr_2],\n        model_names=model_names,\n        var_names=group,\n        combined=True,\n        ax=ax[idx],\n    )\n\n\n\n\n\n\n\n\n\naz.plot_forest(\n    [idata_lkj_cov, idata_lkj_corr, idata_lkj_corr_2],\n    model_names=model_names[1:],\n    var_names=[\"œÅ_u\"],\n    combined=True\n);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe showed how to use correlated priors for group-specific coefficients.\nThe posteriors resulted to be the same for the in all the cases.\nThe correlated priors didn‚Äôt imply any benefit to our sampling process. However, this could be beneficial for more complex hierarchical models.\nWhat‚Äôs more, the model with the correlated priors took more time to sample than the one with independent priors.\nAttempting to replicate rstanarm approach takes even longer because we are forced to compute many things manually.\n\n\n\n\n\nSometimes, the models with correlated priors based on pm.LKJCorr resulted in divergences. We needed to increase target_accept.\nIt would be good to be able to pass a random variable to sd_dist in pm.LKJCholeskyCov, and not just a stateless distribution. This forced me to use pm.LKJCorr and perform many manipulations manually, which was more error-prone and inefficient.\nIt would be good to check if there‚Äôs something in the LKJCorr/LKJCholeskyCov that could be improved. I plan to use LKJCholeskyCov within Bambi in the future and I would like it to work as better as possible.\n\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Sun Jun 12 2022\n\nPython implementation: CPython\nPython version       : 3.10.4\nIPython version      : 8.3.0\n\nnumpy     : 1.21.6\npandas    : 1.4.2\nmatplotlib: 3.5.2\narviz     : 0.12.1\naesara    : 2.6.6\npymc      : 4.0.0\nsys       : 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html#the-model",
    "href": "posts/2022-06-12_lkj-prior/index.html#the-model",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "The model we‚Äôre going to build today is a hierarchical linear regression, with a Gaussian likelihood. In the following description, I use the greek letter \\(\\beta\\) to refer to common effects and the roman letter \\(u\\) to refer to group-specific (or varying) effects.\n\\[\ny_{ij} = \\beta_0 + u_{0j} + \\left( \\beta_1 + u_{1j} \\right) \\cdot {\\text{Days}} + \\epsilon_i\n\\]\nwhere\n\\[\n\\begin{aligned}\ny_{ij} &= \\text{Reaction time for the subject } j \\text{ on day } i \\\\\n\\beta_0 &= \\text{Intercept common to all subjects} \\\\\n\\beta_1 &= \\text{Days slope common to all subjects} \\\\\nu_{0j} &= \\text{Intercept deviation of the subject } j \\\\\nu_{1j} &= \\text{Days slope deviation of the subject } j \\\\\n\\epsilon_i &= \\text{Residual random error}\n\\end{aligned}\n\\]\nwe also have\n\\[\n\\begin{aligned}\ni = 1, \\cdots, 10 \\\\\nj = 1, \\cdots, 18\n\\end{aligned}\n\\]\nwhere \\(i\\) indexes Days and \\(j\\) indexes subjects.\nFrom the mathematical description we notice both the intercept and the slope are made of two components. The intercept is made of a common or global intercept \\(\\beta_0\\) and subject-specific deviations \\(u_{0j}\\). The same logic applies for the slope with both \\(\\beta_1\\) and \\(u_{1j}\\)."
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html#priors",
    "href": "posts/2022-06-12_lkj-prior/index.html#priors",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "For the common effects, we Guassian independent priors.\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal}\\left(\\bar{y}, \\sigma_{\\beta_0}\\right) \\\\\n\\beta_1 \\sim \\text{Normal}\\left(0, \\sigma_{\\beta_1}\\right)\n\\end{array}\n\\]\nBambi centers the prior for the intercept at \\(\\bar{y}\\), so do we. For \\(\\sigma_{\\beta_0}\\) and \\(\\sigma_{\\beta_1}\\) I‚Äôm going to use 50 and 10 respectively. We‚Äôll use these same priors for all the variants of the model above.\n\n\n\n\\[\n\\begin{aligned}\n\\epsilon_i &\\sim \\text{Normal}(0, \\sigma) \\\\\n\\sigma &\\sim \\text{HalfStudentT}(\\nu, \\sigma_\\epsilon)\n\\end{aligned}\n\\]\nWhere \\(\\nu\\) and \\(\\sigma_\\epsilon\\), both positive constants, represent the degrees of freedom and the scale parameter, respectively.\n\n\n\nThroughout this post we‚Äôll propose the following variants for the priors of the group-specific effects.\n\nIndependent priors.\nCorrelated priors.\n\nUsing LKJCholeskyCov.\nUsing LKJCorr.\nUsign LKJCorr with non-trivial standard deviation.\n\n\nEach of them will be described in more detail in its own section.\nThen we create subjects and subjects_idx. These represent the subject IDs and their indexes. These are used with the distribution of the group-specific coefficients. We also have the coords that we pass to the model and the mean of the prior for the intercept\n\n# Subjects and subjects index\nsubjects, subjects_idx = np.unique(data[\"Subject\"], return_inverse=True)\n\n# Coordinates to handle dimensions of PyMC distributions and use better labels\ncoords = {\"subject\": subjects}\n\n# Response mean -- Used in the prior for the intercept\ny_mean = data[\"Reaction\"].mean()\n\n# Days variable\ndays = data[\"Days\"].values"
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html#model-1-independent-priors",
    "href": "posts/2022-06-12_lkj-prior/index.html#model-1-independent-priors",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "\\[\n\\begin{array}{lr}\nu_{0j} \\sim \\text{Normal} \\left(0, \\sigma_{u_0}\\right) & \\text{for all } j:1,..., 18 \\\\\nu_{1j} \\sim \\text{Normal} \\left(0, \\sigma_{u_1}\\right) & \\text{for all } j:1,..., 18\n\\end{array}\n\\]\nwhere the hyperpriors are\n\\[\n\\begin{array}{c}\n\\sigma_{u_0} \\sim \\text{HalfNormal} \\left(\\tau_0\\right) \\\\\n\\sigma_{u_1} \\sim \\text{HalfNormal} \\left(\\tau_1\\right)\n\\end{array}\n\\]\nwhere \\(\\tau_0\\) and \\(\\tau_1\\) represent the standard deviations of the hyperpriors. These are fixed positive constants. We set them to the same values than \\(\\sigma_{\\beta_0}\\) and \\(\\sigma_{\\beta_1}\\) respectively.\n\nwith pm.Model(coords=coords) as model_independent:\n    # Common effects\n    Œ≤0 = pm.Normal(\"Œ≤0\", mu=y_mean, sigma=50)\n    Œ≤1 = pm.Normal(\"Œ≤1\", mu=0, sigma=10)\n    \n    # Group-specific effects\n    # Intercept\n    œÉ_u0 = pm.HalfNormal(\"œÉ_u0\", sigma=50)\n    u0 = pm.Normal(\"u0\", mu=0, sigma=œÉ_u0, dims=\"subject\")\n    \n    # Slope\n    œÉ_u1 = pm.HalfNormal(\"œÉ_u1\", sigma=10)   \n    u1 = pm.Normal(\"u1\", mu=0, sigma=œÉ_u1, dims=\"subject\")\n   \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", Œ≤0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (Œ≤1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    Œº = pm.Deterministic(\"Œº\", intercept + slope)\n    \n    # Residual standard deviation\n    œÉ = pm.HalfStudentT(\"œÉ\", nu=4, sigma=50)\n    \n    # Response\n    y = pm.Normal(\"y\", mu=Œº, sigma=œÉ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_independent)\n\n\n\n\n\n\n\n\n\nwith model_independent:\n    idata_independent = pm.sample(draws=1000, chains=4, random_seed=1234)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [Œ≤0, Œ≤1, œÉ_u0, u0, œÉ_u1, u1, œÉ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:31&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 33 seconds.\n\n\n\naz.plot_trace(\n    idata_independent, \n    var_names=[\"Œ≤0\", \"Œ≤1\", \"u0\", \"u1\", \"œÉ\", \"œÉ_u0\", \"œÉ_u1\"],\n    combined=True, \n    chain_prop={\"ls\": \"-\"}\n);\n\n\n\n\n\n\n\n\n\ndef plot_predictions(data, idata, figsize=(16, 7.5)):\n    # Plot the data\n    fig, axes = plot_data(data, figsize=figsize)\n    \n    # Extract predicted mean\n    reaction_mean = idata.posterior[\"Œº\"]\n\n    for subject, ax in zip(subjects, axes.ravel()):\n        idx = (data[\"Subject\"]== subject).values\n        days = data.loc[idx, \"Days\"].values\n\n        # Plot highest density interval / credibility interval\n        az.plot_hdi(days, reaction_mean[..., idx], color=\"C0\", ax=ax)\n\n        # Plot mean regression line\n        ax.plot(days, reaction_mean[..., idx].mean((\"chain\", \"draw\")), color=\"C0\")\n    \n    return fig ,axes\n\n\nplot_predictions(data, idata_independent);"
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html#correlated-priors",
    "href": "posts/2022-06-12_lkj-prior/index.html#correlated-priors",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "Instead of placing independent priors on \\(u_{0j}\\) and \\(u_{1j}\\), we place a multivariate normal prior on \\([u_{0j}, u_{1j}]^T\\), which allows for correlated group-specific effects.\n\\[\n\\begin{array}{lr}\n    \\left[\n        \\begin{array}{c}\n            u_{0j} \\\\\n            u_{0j}\n        \\end{array}\n    \\right]\n    \\sim \\text{MvNormal}\n    \\left(\n        \\left[\n            \\begin{array}{c}\n                0 \\\\\n                0\n            \\end{array}\n        \\right],\n        \\Sigma =\n        \\left[\n            \\begin{array}{cc}\n                \\sigma_{u_0}^2 & \\text{cov}(u_0, u_1) \\\\\n                \\text{cov}(u_0, u_1) & \\sigma_{u_1}^2\n            \\end{array}\n        \\right]\n    \\right)\n    &\n    \\text{for all } j:1,..., 18\n\\end{array}\n\\]\nand we use the same hyperpriors\n\\[\n\\begin{array}{c}\n\\sigma_{u_0} \\sim \\text{HalfNormal} \\left(\\tau_0\\right) \\\\\n\\sigma_{u_1} \\sim \\text{HalfNormal} \\left(\\tau_1\\right)\n\\end{array}\n\\]\n\n\nIn practice, we do not set a prior for the covariance matrix \\(\\Sigma\\). Instead, we set a prior on the correlation matrix \\(\\Omega\\) and use the following decomposition to recover the covariance matrix\n\\[\n\\begin{split}\n    \\Sigma & =\n    \\begin{pmatrix}\n        \\sigma_{u_0} & 0 \\\\\n        0 & \\sigma_{u_1}\n    \\end{pmatrix}\n    \\begin{pmatrix} 1 & \\rho_{u_0, u_1} \\\\ \\rho_{u_0, u_1} & 1 \\end{pmatrix}\n    \\begin{pmatrix}\n        \\sigma_{u_0} & 0 \\\\\n        0 & \\sigma_{u_1}\n    \\end{pmatrix} \\\\\n      & = \\text{diag}(\\sigma_u) \\ \\Omega \\ \\text{diag}(\\sigma_u)\n\\end{split}\n\\]\nA very popular and flexible alternative is to place an LKJ prior on the correlation matrix.\n\\[\n\\Omega \\sim \\text{LKJ}(\\eta), \\ \\  \\eta &gt; 0\n\\]\nLKJ stands for the Lewandowski-Kurowicka-Joe distribution. If \\(\\eta = 1\\) (our default choice), the prior is jointly uniform over all correlation matrices of the same dimension as \\(\\Omega\\). If \\(\\eta &gt; 1\\), then the mode of the distribution is the identity matrix. The larger the value of \\(\\eta\\) the more sharply peaked the density is at the identity matrix\n\n\n\nFor efficiency and numerical stability, the correlation matrix \\(\\Omega\\) can be parametrized by its (lower-triangular) Cholesky factor \\(L\\), which can be seen as the square root of \\(\\Omega\\)\n\\[\n\\boldsymbol{L}\\boldsymbol{L^T} = \\Omega = \\begin{pmatrix} 1 & \\rho_{u_0, u_1} \\\\ \\rho_{u_0, u_1} & 1 \\end{pmatrix}\n\\]\nIf \\(\\boldsymbol{Z}_{\\text{uncorr}}\\) is a \\(2\\times n\\) matrix where the rows are 2 samples from uncorrelated random variables, then\n\\[\n\\begin{split}\n    \\boldsymbol{Z}_{\\text{corr}} & =\n    \\begin{pmatrix} \\sigma_{u_0} & 0 \\\\ 0 & \\sigma_{u_1} \\end{pmatrix}\n    \\boldsymbol{L}\n    \\boldsymbol{Z}_{\\text{uncorr}} \\\\\n    & = \\text{diag}(\\sigma_u) \\boldsymbol{L} \\boldsymbol{Z}_{\\text{uncorr}}     \n\\end{split}\n\\]\nThen \\(\\boldsymbol{Z}_{\\text{corr}}\\), of shape \\((2, n)\\), contains a sample of size \\(n\\) of two correlated variables with the desired variances \\(\\sigma_{u_0}^2\\), \\(\\sigma_{u_1}^2\\), and correlation \\(\\rho_{u_0, u_1}\\)."
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html#model-2-correlated-priors-with-lkjcholeskycov",
    "href": "posts/2022-06-12_lkj-prior/index.html#model-2-correlated-priors-with-lkjcholeskycov",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "PyMC conveniently implements a distribution called LKJCholeskyCov. Here, n represents the dimension of the correlation matrix. eta is the parameter of the LKJ distribution. sd_dist is the prior distribution for the standard deviations of the group-specific effects. compute_corr=True means we want it to also return the correlation between the group-specific parameters and their standard deviations. store_in_trace=False means we don‚Äôt want to store the correlation and the standard deviations in the trace.\nBefore seeing the code, we note that sd_dist is not a random variable, but a stateless distribution (i.e.¬†the result of pm.Distribution.dist()).\n\ncoords.update({\"effect\": [\"intercept\", \"slope\"]})\n\nwith pm.Model(coords=coords) as model_lkj_cov:\n    ## Common effects\n    Œ≤0 = pm.Normal(\"Œ≤0\", mu=y_mean, sigma=50)\n    Œ≤1 = pm.Normal(\"Œ≤1\", mu=0, sigma=10)\n   \n    ## Group-specific effects\n    # Hyper prior for the standard deviations\n    u_œÉ = pm.HalfNormal.dist(sigma=np.array([50, 10]), shape=2)\n    \n    # Obtain Cholesky factor for the covariance\n    L, œÅ_u, œÉ_u = pm.LKJCholeskyCov(\n        \"L\", n=2, eta=1, sd_dist=u_œÉ, compute_corr=True, store_in_trace=False\n    )\n    \n    # Parameters\n    u_raw = pm.Normal(\"u_raw\", mu=0, sigma=1, dims=(\"effect\", \"subject\")) \n    u = pm.Deterministic(\"u\", at.dot(L, u_raw).T, dims=(\"subject\", \"effect\"))\n   \n    ## Separate group-specific terms \n    # Intercept\n    u0 = pm.Deterministic(\"u0\", u[:, 0], dims=\"subject\")\n    œÉ_u0 = pm.Deterministic(\"œÉ_u0\", œÉ_u[0])\n    \n    # Slope\n    u1 = pm.Deterministic(\"u1\", u[:, 1], dims=\"subject\")\n    œÉ_u1 = pm.Deterministic(\"œÉ_u1\", œÉ_u[1])\n\n    # Correlation\n    œÅ_u = pm.Deterministic(\"œÅ_u\", œÅ_u[0, 1])\n    \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", Œ≤0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (Œ≤1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    Œº = pm.Deterministic(\"Œº\", intercept + slope)\n    \n    ## Residual standard deviation\n    œÉ = pm.HalfStudentT(\"œÉ\", nu=4, sigma=56)\n    \n    ## Response\n    y = pm.Normal(\"y\", mu=Œº, sigma=œÉ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_lkj_cov)\n\n\n\n\n\n\n\n\n\nwith model_lkj_cov:\n    idata_lkj_cov = pm.sample(draws=1000, chains=4, random_seed=1234, target_accept=0.9)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [Œ≤0, Œ≤1, L, u_raw, œÉ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 01:04&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 66 seconds.\n\n\n\naz.plot_trace(\n    idata_lkj_cov, \n    var_names=[\"Œ≤0\", \"Œ≤1\", \"u0\", \"u1\", \"œÉ\", \"œÉ_u0\", \"œÉ_u1\", \"œÅ_u\"],\n    combined=True, \n    chain_prop={\"ls\": \"-\"}\n);\n\n\n\n\n\n\n\n\nFrom the traceplot of the correlation coefficient œÅ_u it looks like the group-specific intercept and slope are not related since the distribution is centered around zero.\nBut there‚Äôs another question we haven‚Äôt answered yet: Are the initial reaction times associated with how much the sleep deprivation affects the evolution of reaction times? Let‚Äôs create a scatterplot to visualize the joint posterior of the subject-specific intercepts and slopes. This chart uses different colors for the individuals.\n\nposterior_u0 = idata_lkj_cov.posterior[\"u0\"].values\nposterior_u1 = idata_lkj_cov.posterior[\"u1\"].values\n\nfig, ax = plt.subplots()\nfor subject in range(18):\n    # Not all the samples are drawn\n    x = posterior_u0[::10, :, subject]\n    y = posterior_u1[::10, :, subject]\n    ax.scatter(x, y, alpha=0.5)\n    \nax.axhline(c=\"k\", ls=\"--\", alpha=0.5)\nax.axvline(c=\"k\", ls=\"--\", alpha=0.5)\nax.set(xlabel=\"Subject-specific intercept\", ylabel=\"Subject-specific slope\");\n\n\n\n\n\n\n\n\nIf we look at the bigger picture, i.e omitting the groups, we can conclude there‚Äôs no association between the intercept and slope. In other words, having lower or higher intial reaction times does not say anything about how much sleep deprivation affects the average reaction time on a given subject.\nOn the other hand, if we look at the joint posterior for a given individual, we can see a negative correlation between the intercept and the slope. This is telling that, conditional on a given subject, the intercept and slope posteriors are not independent. However, it doesn‚Äôt imply anything about the overall relationship between the intercept and the slope, which is what we need if we want to know whether the initial time is associated with how much sleep deprivation affects the reaction time.\nLet‚Äôs check predictions\n\nplot_predictions(data, idata_lkj_cov);"
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html#model-3-correlated-priors-with-lkjcorr.",
    "href": "posts/2022-06-12_lkj-prior/index.html#model-3-correlated-priors-with-lkjcorr.",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "One problem with LKJCholeskyCov is that its sd_dist argument must be a stateless distribution and thus we cannot use a customized distribution for the standard deviations of the group-specific effects.\nIf we want to use a random variable instead of a stateless distribution for the standard deviation of the group-specific effects, then we need to perform many steps manually. Let‚Äôs see how we can implement it!\n\nwith pm.Model(coords=coords) as model_lkj_corr:\n    # Common part\n    Œ≤0 = pm.Normal(\"Œ≤0\", mu=y_mean, sigma=50)\n    Œ≤1 = pm.Normal(\"Œ≤1\", mu=0, sigma=10)\n    \n    # Group-specific part   \n    œÉ_u = pm.HalfNormal(\"u_œÉ\", sigma=np.array([50, 10]), dims=\"effect\")\n    \n    # Triangular upper part of the correlation matrix\n    Œ©_triu = pm.LKJCorr(\"Œ©_triu\", eta=1, n=2)\n\n    # Construct correlation matrix\n    Œ© = pm.Deterministic(\n        \"Œ©\", \n        at.fill_diagonal(Œ©_triu[np.zeros((2, 2), dtype=np.int64)], 1)\n    )\n    \n    # Construct diagonal matrix of standard deviation\n    œÉ_diagonal = pm.Deterministic(\"œÉ_diagonal\", at.eye(2) * œÉ_u)\n    \n    # Compute covariance matrix\n    Œ£ = at.nlinalg.matrix_dot(œÉ_diagonal, Œ©, œÉ_diagonal)\n    \n    # Cholesky decomposition of covariance matrix\n    L = pm.Deterministic(\"L\", at.slinalg.cholesky(Œ£))\n    \n    # And finally get group-specific coefficients\n    u_raw = pm.Normal(\"u_raw\", mu=0, sigma=1, dims=(\"effect\", \"subject\")) \n    u = pm.Deterministic(\"u\", at.dot(L, u_raw).T, dims=(\"subject\", \"effect\"))\n\n    u0 = pm.Deterministic(\"u0\", u[:, 0], dims=\"subject\")\n    œÉ_u0 = pm.Deterministic(\"œÉ_u0\", œÉ_u[0])\n    \n    u1 = pm.Deterministic(\"u1\", u[:, 1], dims=\"subject\")\n    œÉ_u1 = pm.Deterministic(\"œÉ_u1\", œÉ_u[1])\n    \n    # Correlation\n    œÅ_u = pm.Deterministic(\"œÅ_u\", Œ©[0, 1])\n         \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", Œ≤0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (Œ≤1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    Œº = pm.Deterministic(\"Œº\", intercept + slope)\n    \n    œÉ = pm.HalfStudentT(\"œÉ\", nu=4, sigma=50)\n    y = pm.Normal(\"y\", mu=Œº, sigma=œÉ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_lkj_corr)\n\n\n\n\n\n\n\n\n\nwith model_lkj_corr:\n    idata_lkj_corr = pm.sample(draws=1000, chains=2, random_seed=1234, target_accept=0.9)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [Œ≤0, Œ≤1, u_œÉ, Œ©_triu, u_raw, œÉ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 01:15&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 76 seconds.\n\n\n\naz.plot_trace(\n    idata_lkj_corr, \n    var_names=[\"Œ≤0\", \"Œ≤1\", \"u0\", \"u1\", \"œÉ\", \"œÉ_u0\", \"œÉ_u1\", \"œÅ_u\"],\n    combined=True, \n    chain_prop={\"ls\": \"-\"}\n);\n\n\n\n\n\n\n\n\n\nplot_predictions(data, idata_lkj_corr);"
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html#model-4-correlated-priors-with-lkjcorr.-replicate-rstanarm-prior",
    "href": "posts/2022-06-12_lkj-prior/index.html#model-4-correlated-priors-with-lkjcorr.-replicate-rstanarm-prior",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "This model is like the previous one, but œÉ_u is the result of multiplying several random variables. Rstanarm prior is introduced here\nNOTE: œÉ_u is what I would like to be able to pass to the sd_dist argument in pm.LKJCholeskyCov. Since I can only pass a stateless distribution, I need to perform all the steps manually.\n\nThe vector of variances is set equal to the product of a simplex vector \\(\\pi\\) ‚Äî which is non-negative and sums to 1 ‚Äî and the scalar trace: \\(J\\tau^2\\pi\\).\n[‚Ä¶]\nFor the simplex vector \\(\\pi\\) we use a symmetric Dirichlet prior which has a single concentration parameter \\(\\gamma &gt; 0\\).\n\nOn top of that, the \\(J\\tau^2\\pi\\) is scaled by the residual standard deviation as explained in this comment.\n\nJ = 2 # Order of covariance matrix\n\nwith pm.Model(coords=coords) as model_lkj_corr_2:\n    # Common part\n    Œ≤0 = pm.Normal(\"Œ≤0\", mu=y_mean, sigma=50)\n    Œ≤1 = pm.Normal(\"Œ≤1\", mu=0, sigma=10)\n    \n    # Residual SD \n    œÉ = pm.HalfStudentT(\"œÉ\", nu=4, sigma=50)\n    \n    # Group-specific part\n    # Begin of rstanarm approach ----------------------------------\n    œÑ = pm.Gamma(\"œÑ\", alpha=1, beta=1)\n    Œ£_trace = J * œÑ ** 2\n    œÄ = pm.Dirichlet(\"œÄ\", a=np.ones(J), dims=\"effect\")\n    œÉ_u = pm.Deterministic(\"b_œÉ\", œÉ * œÄ * (Œ£_trace) ** 0.5)\n    # End of rstanarm approach ------------------------------------\n    \n    # Triangular upper part of the correlation matrix\n    Œ©_triu = pm.LKJCorr(\"Œ©_triu\", eta=1, n=2)\n     \n    # Correlation matrix\n    Œ© = pm.Deterministic(\n        \"Œ©\", at.fill_diagonal(Œ©_triu[np.zeros((2, 2), dtype=np.int64)], 1.)\n    )\n\n    # Construct diagonal matrix of standard deviation\n    œÉ_u_diagonal = pm.Deterministic(\"œÉ_u_diagonal\", at.eye(2) * œÉ_u)\n    \n    # Covariance matrix\n    Œ£ = at.nlinalg.matrix_dot(œÉ_u_diagonal, Œ©, œÉ_u_diagonal)\n    \n    # Cholesky decomposition, lower triangular matrix.\n    L = pm.Deterministic(\"L\", at.slinalg.cholesky(Œ£))\n    u_raw = pm.Normal(\"u_raw\", mu=0, sigma=1, dims=(\"effect\", \"subject\")) \n    \n    u = pm.Deterministic(\"u\", at.dot(L, u_raw).T, dims=(\"subject\", \"effect\"))\n                         \n    u0 = pm.Deterministic(\"u0\", u[:, 0], dims=\"subject\")\n    œÉ_u0 = pm.Deterministic(\"œÉ_u0\", œÉ_u[0])\n    \n    u1 = pm.Deterministic(\"u1\", u[:, 1], dims=\"subject\")\n    œÉ_u1 = pm.Deterministic(\"œÉ_u1\", œÉ_u[1])\n    \n    # Correlation\n    œÅ_u = pm.Deterministic(\"œÅ_u\", Œ©[0, 1])\n         \n    # Construct intercept and slope\n    intercept = pm.Deterministic(\"intercept\", Œ≤0 + u0[subjects_idx]) \n    slope = pm.Deterministic(\"slope\", (Œ≤1 + u1[subjects_idx]) * days) \n    \n    # Conditional mean\n    Œº = pm.Deterministic(\"Œº\", intercept + slope)\n       \n    y = pm.Normal(\"y\", mu=Œº, sigma=œÉ, observed=data[\"Reaction\"])\n\n\npm.model_to_graphviz(model_lkj_corr_2)\n\n\n\n\n\n\n\n\n\nwith model_lkj_corr_2:\n    idata_lkj_corr_2 = pm.sample(draws=1000, chains=4, random_seed=1234, target_accept=0.9)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [Œ≤0, Œ≤1, œÉ, œÑ, œÄ, Œ©_triu, u_raw]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 03:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 181 seconds.\n\n\n\naz.plot_trace(\n    idata_lkj_corr_2,\n    var_names=[\"Œ≤0\", \"Œ≤1\", \"u0\", \"u1\", \"œÉ\", \"œÉ_u0\", \"œÉ_u1\", \"œÅ_u\", \"œÑ\", \"œÄ\"],\n);\n\n\n\n\n\n\n\n\n\nplot_predictions(data, idata_lkj_corr_2);"
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html#compare-inferences",
    "href": "posts/2022-06-12_lkj-prior/index.html#compare-inferences",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "groups = [\n    [\"Œ≤0\", \"Œ≤1\", \"œÉ\"],\n    [\"u0\", \"œÉ_u0\"],\n    [\"u1\", \"œÉ_u1\"],\n]\n\nmodel_names = [\"Indepentent\", \"LKJCholeskyCov\", \"LKJCorr\", \"LKJCorr + rstanarm\"]\nfig, ax = plt.subplots(1, 3, figsize = (20, 10))\n\nfor idx, group in enumerate(groups):\n    az.plot_forest(\n        [idata_independent, idata_lkj_cov, idata_lkj_corr, idata_lkj_corr_2],\n        model_names=model_names,\n        var_names=group,\n        combined=True,\n        ax=ax[idx],\n    )\n\n\n\n\n\n\n\n\n\naz.plot_forest(\n    [idata_lkj_cov, idata_lkj_corr, idata_lkj_corr_2],\n    model_names=model_names[1:],\n    var_names=[\"œÅ_u\"],\n    combined=True\n);"
  },
  {
    "objectID": "posts/2022-06-12_lkj-prior/index.html#conclusions",
    "href": "posts/2022-06-12_lkj-prior/index.html#conclusions",
    "title": "Hierarchical modeling with the LKJ prior in PyMC",
    "section": "",
    "text": "We showed how to use correlated priors for group-specific coefficients.\nThe posteriors resulted to be the same for the in all the cases.\nThe correlated priors didn‚Äôt imply any benefit to our sampling process. However, this could be beneficial for more complex hierarchical models.\nWhat‚Äôs more, the model with the correlated priors took more time to sample than the one with independent priors.\nAttempting to replicate rstanarm approach takes even longer because we are forced to compute many things manually.\n\n\n\n\n\nSometimes, the models with correlated priors based on pm.LKJCorr resulted in divergences. We needed to increase target_accept.\nIt would be good to be able to pass a random variable to sd_dist in pm.LKJCholeskyCov, and not just a stateless distribution. This forced me to use pm.LKJCorr and perform many manipulations manually, which was more error-prone and inefficient.\nIt would be good to check if there‚Äôs something in the LKJCorr/LKJCholeskyCov that could be improved. I plan to use LKJCholeskyCov within Bambi in the future and I would like it to work as better as possible.\n\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Sun Jun 12 2022\n\nPython implementation: CPython\nPython version       : 3.10.4\nIPython version      : 8.3.0\n\nnumpy     : 1.21.6\npandas    : 1.4.2\nmatplotlib: 3.5.2\narviz     : 0.12.1\naesara    : 2.6.6\npymc      : 4.0.0\nsys       : 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:39:04) [GCC 10.3.0]\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2024-11-01_pymc-data-simulation/index.html",
    "href": "posts/2024-11-01_pymc-data-simulation/index.html",
    "title": "Data simulation with PyMC",
    "section": "",
    "text": "To simulate data in Python, people generally use NumPy or SciPy. Their interfaces are simple and easy to use. For example, to simulate values from a normal distribution, np.random.normal() is all we need.\nBut not all the scenarios where one may want to simulate data are as simple as our example above. In statistics, quite often we want to perform parameter recovery studies. These tell us if our model and estimation procedure are able to recover the true value of the parameters of the assumed data generating process.\nTo perform a parameter recovery study, we roughly follow the steps below:\nIf we wanted to do this using NumPy, we would need to write the model from scratch using functions in its random module. In general, when our goal is to perform estimation and prediction, we don‚Äôt write models from scratch in NumPy. Rather, we use the interface provided by another library that usually comes with the tools needed to do estimation, criticism, prediction, etc.\nWhat the previous paragraph highlights is that, at the end of the day, the model is implemented twice. One implementation is used to carry out simulation and parameter recovery, the other is used to estimate the parameters. As the complexity of the model increases, it becomes increasingly harder to avoid mistakes in the implementation of our model in a pure NumPy or SciPy approach.\nFortunately, for those of us following a Bayesian approach, there‚Äôs PyMC. The model representation in PyMC gives users the possibility to do both simulation and estimation using the same model object.\nThrough a series of examples of increasing complexity, I‚Äôm going to show how to simulate data with PyMC in the context of a parameter recovery study. In our journey, we‚Äôll learn about some PyMC specific terminologies and functions including:\nHopefully, the groupings will make sense by the end of this post.\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc as pm\nseed = sum(map(ord, \"happy data simulation\"))  # A fancy way to determine a seed\nrng = np.random.default_rng(seed)              # Random number generator"
  },
  {
    "objectID": "posts/2024-11-01_pymc-data-simulation/index.html#a-simple-normal-model",
    "href": "posts/2024-11-01_pymc-data-simulation/index.html#a-simple-normal-model",
    "title": "Data simulation with PyMC",
    "section": "A simple normal model",
    "text": "A simple normal model\nIt all starts with a very simple model. There‚Äôs an outcome variable \\(Y\\) which we model with a normal distribution, and we‚Äôre interested in the estimation of the mean and standard deviation parameters, \\(\\mu\\) and \\(\\sigma\\).\n\\[\n\\begin{aligned}\nY_i \\mid \\mu, \\sigma    &\\sim \\text{Normal}(\\mu, \\sigma^2) \\\\\n\\mu &\\sim \\text{Normal}(0, 1^2) \\\\\n\\sigma  &\\sim \\text{Gamma}(2, 2)\n\\end{aligned}\n\\]\nwith \\(i = 1, \\dots, N\\). In this case, as we‚Äôre starting small, we ill use \\(N = 10\\).\n\nN = 10\n\nwith pm.Model() as model:\n1    mu = pm.Normal(\"mu\", mu=0, sigma=1)\n2    sigma = pm.Gamma(\"sigma\", alpha=2, beta=2)\n3    pm.Normal(\"y\", mu=mu, sigma=sigma, shape=N)\n\nmodel.to_graphviz()\n\n\n1\n\nPrior for \\(\\mu\\)\n\n2\n\nPrior for \\(\\sigma\\)\n\n3\n\nThe observational model. shape=N indicates there are N values of y.\n\n\n\n\n\n\n\n\n\n\n\nNote that we have created a PyMC model without any observed data. If you‚Äôre already familiar with PyMC, don‚Äôt look for the observed keyword in the distribution for \\(Y\\), you won‚Äôt find it.\nTo simulate values from a PyMC distribution, there‚Äôs the very handy pm.draw() function. It does what its name says: drawing samples from a probability distribution1.\nLet‚Äôs draw a value from the distribution of \\(\\mu\\):\n\npm.draw(mu)\n\narray(0.02122677)\n\n\nYou want many values? No problem:\n\npm.draw(mu, draws=10)\n\narray([ 0.10972394, -0.46840241,  2.81970857,  0.1552543 ,  0.89596717,\n       -0.86260537,  0.31740876, -0.95548771,  0.68387801,  2.33809632])\n\n\nHow about reproducibility? It has you covered with random_seed. You can pass an integer seed or a random number generator. I prefer the latter and I will use it throughout this blog post.\n\npm.draw(sigma, random_seed=rng)\n\narray(0.66709727)\n\n\nFinally, the last thing I‚Äôm going to say about pm.draw() is that it accepts a sequence of random variables. Below, we get a draw for both \\(\\mu\\) and \\(\\sigma\\).\n\nmu_value, sigma_value = pm.draw([mu, sigma], random_seed=rng)\nmu_value, sigma_value\n\n(array(-0.64551614), array(0.29948268))\n\n\nThe next step is to set these parameter values in the model. This is where the pm.do() function comes into play. The name of the function comes from the causal inference literature and it is used to perform ‚Äúinterventions‚Äù in a model graph. For our purposes, we can think of it as a way to assign specific values to model parameters. If you‚Äôre curious about it you can have a look at Causal analysis with PyMC: Answering ‚ÄúWhat If?‚Äù with the new do operator and Interventional distributions and graph mutation with the do-operator.\nLet‚Äôs return to our topic. To assign values to model parameters using pm.do(), we pass it a model instance and a dictionary that maps variable names to values. The output we obtain is a new PyMC model instance.\n\nmodel_fixed_parameters = pm.do(model, {\"mu\": mu_value, \"sigma\": sigma_value})\nmodel_fixed_parameters.to_graphviz()\n\n\n\n\n\n\n\n\nIn this model new instance, the nodes for mu and sigma got updated. They don‚Äôt represent random variables anymore, they are now constant values.\nNow that the parameter values are fixed, we need to simulate values from the outcome variable y. To do so, we use pm.draw() again. Note that we access the random variable y from the model object, as there‚Äôs no global variable that represents it.\n\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\ny_values\n\narray([-0.93216552, -0.37497902,  0.02807845, -0.53793148, -0.99724182,\n       -0.77754811, -0.60105837, -0.34428665, -0.63623636, -0.66625698])\n\n\nThe next step is to set the ‚Äúobserved‚Äù values of the outcome variable in the original model, where mu and sigma are random variables. This is where pm.observe() comes into play. Unlike pm.do(), pm.observe() doesn‚Äôt convert random variables into fixed quantities. Instead, it attaches observed values ‚Äîrealizations of these random variables‚Äî that are later used to condition inference.\nThe usage is analog to pm.do(). We pass a model, a dictionary mapping variables to values and it returns a new PyMC model.\n\nmodel_observed_data = pm.observe(model, {\"y\": y_values})\nmodel_observed_data.to_graphviz()\n\n\n\n\n\n\n\n\nFinally, we now can perform inference and evaluate whether the posterior concentrates around the parameter value used to simulate the values of the outcome.\n\nwith model_observed_data:\n    idata = pm.sample(random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\naz.plot_posterior(idata, ref_val=[mu_value, sigma_value]);\n\n\n\n\n\n\n\n\nAnd everything worked just as expected.\n\nSummary\nSince we‚Äôre going to follow the same steps in all the examples below, let‚Äôs summarise them here for reference.\n\nDefine model (input variables, parameters, and output variables)\n\nN = 10\n\nwith pm.Model() as model:\n    mu = pm.Normal(\"mu\", mu=0, sigma=1)\n    sigma = pm.Gamma(\"sigma\", alpha=2, beta=2)\n    pm.Normal(\"y\", mu=mu, sigma=sigma, shape=N)\n\nSet model parameters to some plausible values\n\nmu_value, sigma_value = pm.draw([mu, sigma], random_seed=rng)\nmodel_fixed_parameters = pm.do(model, {\"mu\": mu_value, \"sigma\": sigma_value})\n\nSimulate values of the outcome variable\n\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\n\nEstimate the parameters\n\nmodel_observed_data = pm.observe(model, {\"y\": y_values})\nwith model_observed_data:\n    idata = pm.sample(random_seed=rng)\n\nCompare the parameter estimates against the true values\n\naz.plot_posterior(idata, ref_val=[mu_value, sigma_value]);\nBefore moving on to the next example, some clarification notes:\n\nYou don‚Äôt need to use the prior to simulate parameter values. You could do pm.do() with arbitrary values, as long as they are within the parameter domain.\nThis is not a post about thorough parameter recovery, just the basics to get the idea across. Don‚Äôt take it as a gold standard."
  },
  {
    "objectID": "posts/2024-11-01_pymc-data-simulation/index.html#simple-linear-regression",
    "href": "posts/2024-11-01_pymc-data-simulation/index.html#simple-linear-regression",
    "title": "Data simulation with PyMC",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nLet‚Äôs work with the following simple linear regression model:\n\\[\n\\begin{aligned}\nY_i \\mid \\mu_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\alpha + \\beta x_i \\\\\n\\alpha &\\sim \\text{Normal}(0, 1^2) \\\\\n\\beta &\\sim \\text{Normal}(0, 1^2) \\\\\n\\sigma  &\\sim \\text{Gamma}(2, 2)\n\\end{aligned}\n\\]\nwith \\(i = 1, \\dots, N\\)\n\nKnown covariate values\nUsually, the values of the covariate are assumed to be fixed and known. We start with a scenario where we have the values for it, and later on, we show how to simulate data for both covariate and outcome variables.\n\nx_values = np.array(\n    [\n        0.786, -0.399,  1.018,  0.657, -0.195, -0.083,  0.651, -0.476, -0.584, -0.194,\n        0.282,  0.176, -0.309,  2.022,  0.341, -0.982, -0.904,  0.491, -2.07 , -0.568\n    ]\n)\n\n\nN = 20\n\nwith pm.Model() as model:\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=1)\n    beta = pm.Normal(\"beta\", mu=0, sigma=1)\n\n    mu = alpha + beta * x_values\n    sigma = pm.Gamma(\"sigma\", alpha=2, beta=2)\n\n    pm.Normal(\"y\", mu=mu, sigma=sigma, shape=N)\n\nmodel.to_graphviz()\n\n\n\n\n\n\n\n\nThe steps below are analogous to the ones above. We only change the random variables from which we sample.\n\n# Get plausible values for the parameters\nalpha_value, beta_value, sigma_value = pm.draw([alpha, beta, sigma], random_seed=rng)\n\nprint(\"alpha:\", alpha_value)\nprint(\"beta:\", beta_value)\nprint(\"sigma:\", sigma_value)\n\n# Set parameters to the sampled values\nmodel_fixed_parameters = pm.do(\n    model,\n    {\"alpha\": alpha_value, \"beta\": beta_value, \"sigma\": sigma_value}\n)\n\nmodel_fixed_parameters.to_graphviz()\n\nalpha: 0.6559553890005595\nbeta: -0.2818584784669223\nsigma: 2.9596230424561605\n\n\n\n\n\n\n\n\n\nGiven the fixed parameter values and the values of the covariate, we simulate values of the response y.\n\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\ny_values\n\narray([ 3.12686173, -0.11044308, -4.05742728,  2.9712478 ,  1.03231454,\n       -0.77103352,  0.42986971,  2.32298661,  2.22334492,  4.25858143,\n       -1.75237172, -0.51776157,  3.27740227,  1.12482262, -4.2394528 ,\n        0.77089266,  4.45871827,  5.20353471,  0.61391209,  3.22912347])\n\n\n\nfig, ax = plt.subplots()\nax.scatter(x_values, y_values, alpha=0.7)\nax.axline((0, alpha_value), slope=beta_value, color=\"0.3\") # theoretical line curve\nax.set(xlabel=\"x\", ylabel=\"y\");\n\n\n\n\n\n\n\n\nObserve the simulated values of y in the original model:\n\nmodel_observed_data = pm.observe(model, {\"y\": y_values})\nmodel_observed_data.to_graphviz()\n\n\n\n\n\n\n\n\nand perform inference:\n\nwith model_observed_data:\n    idata = pm.sample(random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, beta, sigma]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\nAgain, the true parameter value is fairly well contained in the posterior distribution.\n\naz.plot_posterior(idata, ref_val=[alpha_value, beta_value, sigma_value]);\n\n\n\n\n\n\n\n\nWe can also simulate some draws of the posterior regression line.\n\nposterior_draws = az.extract(\n    idata, group=\"posterior\", var_names=[\"alpha\", \"beta\"], num_samples=15\n)\n\nfig, ax = plt.subplots()\nax.scatter(x_values, y_values, alpha=0.7)\nax.axline((0, alpha_value), slope=beta_value, color=\"0.3\")\n\nfor a, b in zip(posterior_draws[\"alpha\"].to_numpy(), posterior_draws[\"beta\"].to_numpy()):\n    ax.axline((0, a), slope=b, color=\"C1\", alpha=0.8, lw=1, zorder=-1)\n\nax.set(xlabel=\"x\", ylabel=\"y\");\n\n\n\n\n\n\n\n\n\n\nUnknown covariate values\nWe may need to simulate values for x either because we don‚Äôt yet have values for the covariate or because we want to evaluate different scenarios.\nThe process is again similar to what we‚Äôve done so far, but now we need to specify a distribution for x to determine how its values are generated. There is a plethora of choices. Here, I‚Äôm faling back on a standard normal distribution.\n\nN = 20\n\nwith pm.Model() as model:\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=1)\n    beta = pm.Normal(\"beta\", mu=0, sigma=1)\n    x = pm.Normal(\"x\", shape=N) # there are as many 'x' values as observations\n\n    mu = alpha + beta * x\n    sigma = pm.Gamma(\"sigma\", alpha=2, beta=2)\n\n    pm.Normal(\"y\", mu=mu, sigma=sigma, shape=N)\n\nmodel.to_graphviz()\n\n\n\n\n\n\n\n\nEverything else stays the same:\n\n# Get plausible values for the parameters\nalpha_value, beta_value, sigma_value, x_values = pm.draw([alpha, beta, sigma, x], random_seed=rng)\n\nprint(\"alpha:\", alpha_value)\nprint(\"beta:\", beta_value)\nprint(\"sigma:\", sigma_value)\n\nprint(\"\\nx values:\", x_values, sep=\"\\n\")\n\n# Set parameters to the sampled values\nmodel_fixed_parameters = pm.do(\n    model,\n    {\"alpha\": alpha_value, \"beta\": beta_value, \"sigma\": sigma_value, \"x\": x_values}\n)\n\nmodel_fixed_parameters.to_graphviz()\n\nalpha: 0.1719973000588151\nbeta: 0.6008524215503052\nsigma: 1.1351657143542704\n\nx values:\n[-0.07292669 -0.54826599 -0.46954908  0.26994204  1.00053203  0.65220785\n -1.52515351  0.51970338 -0.51429822 -0.26137883 -0.47653569  0.16923646\n -0.93059417  0.45502837 -1.05738506 -0.61663942 -0.52632527  0.34543959\n -0.04647066  1.32000584]\n\n\n\n\n\n\n\n\n\n\n# Simulate values for the outcome variable\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\nprint(\"y values:\", y_values, sep=\"\\n\")\n\ny values:\n[ 1.71790428 -0.0682522  -1.26126753 -0.9484205  -0.06096033  0.28471004\n -0.40986102  0.38968251 -2.03112939  1.44298618  0.99584892 -0.46963706\n  0.99470417  0.2107016   0.50460528 -0.72376308  1.58806132  0.28525278\n  0.71371689  2.99055351]\n\n\n\nfig, ax = plt.subplots()\nax.scatter(x_values, y_values, alpha=0.7)\nax.axline((0, alpha_value), slope=beta_value, color=\"0.3\");\nax.set(xlabel=\"x\", ylabel=\"y\");\n\n\n\n\n\n\n\n\nAnd observe the simulated values of y in the original model.\n\nmodel_observed_data = pm.observe(pm.do(model, {\"x\": x_values}), {\"y\": y_values})\nmodel_observed_data.to_graphviz()\n\n\n\n\n\n\n\n\nHold on for a second and take a second look at the code above. See that we‚Äôre using both pm.do() and pm.observe()? We first need to use pm.do() to set the values of x in the original model, and then we use pm.observe() to attach the observed values to the outcome variable y. We can‚Äôt pass the values of x through pm.observe() because that would mean they are not fixed values, but realizations from a random variable.\nTo conclude, let‚Äôs explore the posterior.\n\nwith model_observed_data:\n    idata = pm.sample(random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, beta, sigma]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\naz.plot_posterior(idata, ref_val=[alpha_value, beta_value, sigma_value]);\n\n\n\n\n\n\n\n\n\nposterior_draws = az.extract(\n    idata, group=\"posterior\", var_names=[\"alpha\", \"beta\"], num_samples=15\n)\n\nfig, ax = plt.subplots()\nax.scatter(x_values, y_values, alpha=0.7)\nax.axline((0, alpha_value), slope=beta_value, color=\"0.3\")\n\nfor a, b in zip(posterior_draws[\"alpha\"].to_numpy(), posterior_draws[\"beta\"].to_numpy()):\n    ax.axline((0, a), slope=b, color=\"C1\", alpha=0.8, lw=1, zorder=-1)\n\nax.set(xlabel=\"x\", ylabel=\"y\");"
  },
  {
    "objectID": "posts/2024-11-01_pymc-data-simulation/index.html#normal-model-for-multiple-groups",
    "href": "posts/2024-11-01_pymc-data-simulation/index.html#normal-model-for-multiple-groups",
    "title": "Data simulation with PyMC",
    "section": "Normal model for multiple groups",
    "text": "Normal model for multiple groups\nThe following model is a simple extension of the first one, where we have multiple groups with different population means.\n\\[\n\\begin{aligned}\nY_i     &\\sim \\text{Normal}(\\mu_{j[i]}, \\sigma^2) \\\\\n\\mu_j   &\\sim \\text{Normal}(0, 3^2) & \\text{for all } j \\\\\n\\sigma  &\\sim \\text{Gamma}(2, 2)\n\\end{aligned}\n\\]\nwith \\(i = 1, \\dots, N\\) and \\(j = 1, \\dots, J\\).\nThe indexing notation in \\(\\mu_{j[i]}\\) is read as ‚Äúthe value of \\(j\\) for the \\(i\\)-th observation‚Äù. You can also see \\(j[i]\\) as a function call, where you pass the index of an observation and it gives you the value of the group it belongs to.\n\nKnown group membership\nLet‚Äôs start with a simple example, where:\n\nAll groups have the same, pre-defined, sample size.\nGroup memberships are known.\nObservations are sorted by group.\n\n\nJ = 4        # Number of groups\nn_j = 20     # Number of observations per group\nN = J * n_j  # Total number of observations\n\n# Generate group indexes\ngroup_idx = np.repeat(np.arange(J), n_j)\nprint(\"Group indexes:\")\nprint(group_idx)\n\nGroup indexes:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3 3 3 3 3]\n\n\nThe first 20 observations are from the first group, the second 20 observations are from the second group, and so on.\nLet‚Äôs write the model in PyMC. Since we have as many means as groups, we need to pass shape=J when we define the prior for \\(\\mu\\). Note that we create an intermediate variable mu_indexed that contains the value of mu for each observation, according to their group.\n\nwith pm.Model() as model:\n    mu = pm.Normal(\"mu\", mu=0, sigma=3, shape=J)\n    sigma = pm.Gamma(\"sigma\", alpha=2, beta=2)\n    mu_indexed = mu[group_idx]\n    pm.Normal(\"y\", mu=mu_indexed, sigma=sigma, shape=N)\n\nmodel.to_graphviz()\n\n\n\n\n\n\n\n\n\nmu_values, mu_indexed_values, sigma_value = pm.draw([mu, mu_indexed, sigma], random_seed=rng)\nprint(\"mu:\", mu_values)\nprint(\"sigma:\", sigma_value)\n\nmu: [6.05496819 1.16678048 2.30070048 4.72798215]\nsigma: 0.5873507289564738\n\n\nAs expected, there are 4 values for mu. Let‚Äôs have a look at the values for mu_indexed_values.\n\nmu_indexed_values\n\narray([6.05496819, 6.05496819, 6.05496819, 6.05496819, 6.05496819,\n       6.05496819, 6.05496819, 6.05496819, 6.05496819, 6.05496819,\n       6.05496819, 6.05496819, 6.05496819, 6.05496819, 6.05496819,\n       6.05496819, 6.05496819, 6.05496819, 6.05496819, 6.05496819,\n       1.16678048, 1.16678048, 1.16678048, 1.16678048, 1.16678048,\n       1.16678048, 1.16678048, 1.16678048, 1.16678048, 1.16678048,\n       1.16678048, 1.16678048, 1.16678048, 1.16678048, 1.16678048,\n       1.16678048, 1.16678048, 1.16678048, 1.16678048, 1.16678048,\n       2.30070048, 2.30070048, 2.30070048, 2.30070048, 2.30070048,\n       2.30070048, 2.30070048, 2.30070048, 2.30070048, 2.30070048,\n       2.30070048, 2.30070048, 2.30070048, 2.30070048, 2.30070048,\n       2.30070048, 2.30070048, 2.30070048, 2.30070048, 2.30070048,\n       4.72798215, 4.72798215, 4.72798215, 4.72798215, 4.72798215,\n       4.72798215, 4.72798215, 4.72798215, 4.72798215, 4.72798215,\n       4.72798215, 4.72798215, 4.72798215, 4.72798215, 4.72798215,\n       4.72798215, 4.72798215, 4.72798215, 4.72798215, 4.72798215])\n\n\nThe result is not a coincidence. The first 20 values are the first value of mu, the second 20 values are the second value of mu, and so on. This is because observations are sorted by group.\nThe usage of pm.do() is the same as always.\n\nmodel_fixed_parameters = pm.do(model, {\"mu\": mu_values, \"sigma\": sigma_value})\nmodel_fixed_parameters.to_graphviz()\n\n\n\n\n\n\n\n\nAnd the same logic applies to the simulation of the values of y.\n\n# Simulate values\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\n\n# Plot empirical distributions\nfig, ax = plt.subplots()\n\nfor j in range(J):\n    ax.hist(y_values[group_idx == j], bins=6, alpha=0.6, label=f\"Group {j}\")\nax.legend()\nax.set(xlabel=\"y\");\n\n\n\n\n\n\n\n\nNow let‚Äôs set the observed values, sample the posterior, and, subsequently, explore it.\n\nmodel_observed_data = pm.observe(model, {\"y\": y_values})\n\nwith model_observed_data:\n    idata = pm.sample(random_seed=rng)\n\naz.plot_posterior(idata, var_names=[\"mu\"], ref_val=mu_values.tolist());\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\n\n\n\n\n\n\nWe can also make use of pm.sample_posterior_predictive() to get draws of the posterior predictive distribution:\n\nwith model_observed_data:\n    idata.extend(pm.sample_posterior_predictive(idata))\n\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\nThis allows us to plot the predictive distribution for each group, where we can see they differ in location but not in dispersion.\n\nfig, ax = plt.subplots(figsize=(10, 4))\n\nfor j in range(J):\n    predictive_values = idata.posterior_predictive[\"y\"].to_numpy()[..., group_idx == j].flatten()\n    ax.hist(predictive_values, bins=50, alpha=0.6, label=f\"Group {j}\", density=True)\nax.legend()\nax.set(xlabel=\"y\");\n\n\n\n\n\n\n\n\n\n\nUnknown group membership\n\\[\n\\begin{aligned}\nY_i     &\\sim \\text{Normal}(\\mu_{j[i]}, \\sigma^2) \\\\\n\\mu_j &\\sim \\text{Normal}(0, 3^2) & \\text{for all } j \\\\\n\\sigma  &\\sim \\text{Gamma}(2, 2)\n\\end{aligned}\n\\]\nwith \\(i = 1, \\dots, N\\) and \\(j = 1, \\dots, J\\).\nThe model is exactly the same as the one in the previous section. The difference is that we don‚Äôt know how many observations belong to each group ‚Äì we also want to simulate that.\nTo simulate group memberships we need a distribution that gives us integers between \\(0\\) and \\(J-1\\). One such distribution is the discrete uniform distribution.\n\nJ = 4    # Number of groups\nN = 48   # Total number of observations\n\nwith pm.Model() as model:\n    group_idx = pm.DiscreteUniform(\"group_idx\", lower=0, upper=J-1, shape=N)\n    mu = pm.Normal(\"mu\", mu=0, sigma=3, shape=J)\n    sigma = pm.Gamma(\"sigma\", alpha=2, beta=2)\n    pm.Normal(\"y\", mu=mu[group_idx], sigma=sigma, shape=N)\n\nmodel.to_graphviz()\n\n\n\n\n\n\n\n\n\nmu_values, sigma_value, group_idx_values = pm.draw([mu, sigma, group_idx], random_seed=rng)\n\nprint(\"mu:\", mu_values)\nprint(\"sigma:\", sigma_value)\nprint(\"group indexes:\", group_idx_values, sep=\"\\n\")\n\nmu: [ 4.39222186  3.06006526 -1.82575653  3.42869414]\nsigma: 0.36418924500311767\ngroup indexes:\n[2 0 0 2 1 3 2 0 2 2 3 3 3 3 3 0 3 0 3 3 3 0 3 1 2 2 1 2 2 0 3 0 3 2 3 0 0\n 0 1 0 1 0 0 2 0 0 2 3]\n\n\nNot only are the observations no longer sorted by group, but the group sizes are also different.\n\nnp.unique(group_idx_values, return_counts=True)\n\n(array([0, 1, 2, 3]), array([16,  5, 12, 15]))\n\n\nAll the rest is exactly the same:\n\n# Fix model parameters and data\nmodel_fixed_parameters = pm.do(\n    model,\n    {\"mu\": mu_values, \"sigma\": sigma_value, \"group_idx\": group_idx_values}\n)\n\n# Display model graph\ndisplay(model_fixed_parameters.to_graphviz())\n\n# Simulate values from the outcome\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\n\n# Fix group indexes and observe the outcome values\nmodel_observed_data = pm.observe(\n    pm.do(model, {\"group_idx\": group_idx_values}),\n    {\"y\": y_values}\n)\n\ndisplay(model_observed_data.to_graphviz())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd so on‚Ä¶"
  },
  {
    "objectID": "posts/2024-11-01_pymc-data-simulation/index.html#good-practices",
    "href": "posts/2024-11-01_pymc-data-simulation/index.html#good-practices",
    "title": "Data simulation with PyMC",
    "section": "Good practices",
    "text": "Good practices\nHere, I want to mention a few practices that are recommended when you work with PyMC models. They do not change the previous workflow, but make everything we do with our model more robust and, once we are familiar with it, more intuitive.\n\nUse pm.Data containers to register data variables\nAfter using the do operator to fix parameters as constants, the corresponding nodes will display a Data label, indicating that they represent data rather than random variables\nTurns out it‚Äôs possible to register all fixed quantities in the model with pm.Data containers. For example, let‚Äôs have a look at our simple linear regression model.\n\nx_values = np.array(\n    [\n        0.786, -0.399,  1.018,  0.657, -0.195, -0.083,  0.651, -0.476, -0.584, -0.194,\n        0.282,  0.176, -0.309,  2.022,  0.341, -0.982, -0.904,  0.491, -2.07 , -0.568\n    ]\n)\n\n\nN = 20\n\nwith pm.Model() as model:\n1    x_data = pm.Data(\"x\", x_values)\n\n    alpha = pm.Normal(\"alpha\")\n    beta = pm.Normal(\"beta\")\n    mu = alpha + beta * x_data # Use 'x_data' instead of 'x'\n    sigma = pm.Gamma(\"sigma\", alpha=2, beta=2)\n\n    pm.Normal(\"y\", mu=mu, sigma=sigma, shape=N)\n\nmodel.to_graphviz()\n\n\n1\n\nRegisters \"x\" in the model.\n\n\n\n\n\n\n\n\n\n\n\nAnd when we modify the model, the Data for x is still there.\n\n# Simulate values for the parameters\nalpha_value, beta_value, sigma_value = pm.draw([alpha, beta, sigma], random_seed=rng)\n\n# Fix parameter values in the model\nmodel_fixed_parameters = pm.do(\n    model,\n    {\"alpha\": alpha_value, \"beta\": beta_value, \"sigma\": sigma_value}\n)\n\n# Simulate values for the outcome variable\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\n\n# Set the outcome values as observed in the original model\nmodel_observed_data = pm.observe(model, {\"y\": y_values})\n\n# Visualize graph\nmodel_observed_data.to_graphviz()\n\n\n\n\n\n\n\n\n\n\nUse coords and dims\nSo far, we have always used shape to indicate the dimensions of a variable. It works fine, but it‚Äôs not the recommended approach when using PyMC. A more robust practice is to use dims, which allows us to specify the real-world entities that each dimension of the variable corresponds to.\nFor example:\n\nwith pm.Model() as model:\n    pm.Normal(\"mu\", shape=4, dims=\"group\")\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\nNow it is clear that the length of mu is 4 because there are 4 groups.\nThis becomes truly powerful when used together with coordinates. While dims defines the dimensions of a variable, coords provides the length of the dimensions and the labels for the positions (or indices) within those dimensions:\n\ncoords = {\n    \"group\": [\"Big\", \"Medium\", \"Small\", \"Very small\"]\n}\nwith pm.Model(coords=coords) as model:\n    pm.Normal(\"mu\", dims=\"group\")\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\nNote how it was not necessary to specify the shape explicitly. It was inferred from the coordinates.\nIf two variables have the same dims, they will have the same shape, and it will be clear their dimensions are mapped to the same entities.\nThis is how we could have used coords and dims in our simple linear regression model:\n\nN = 20\n\ncoords = {\n    \"obs_idx\": np.arange(N)\n}\n\nwith pm.Model(coords=coords) as model:\n    x_data = pm.Data(\"x\", value=x_values, dims=\"obs_idx\")\n\n    alpha = pm.Normal(\"alpha\")\n    beta = pm.Normal(\"beta\")\n    mu = alpha + beta * x_data\n    sigma = pm.Gamma(\"sigma\", alpha=2, beta=2)\n\n    pm.Normal(\"y\", mu=mu, sigma=sigma, dims=\"obs_idx\")\n\nmodel.to_graphviz()\n\n\n\n\n\n\n\n\nAnd this is how we could have done it for the normal model for multiple groups:\n\nJ = 4    # Number of groups\nN = 48   # Total number of observations\n\ncoords = {\n    \"group\": [\"Group 1\", \"Group 2\", \"Group 3\", \"Group 4\"],\n    \"obs_idx\": np.arange(N)\n}\n\nwith pm.Model(coords=coords) as model:\n    group_idx = pm.DiscreteUniform(\"group_idx\", lower=0, upper=J-1, dims=\"obs_idx\")\n    mu = pm.Normal(\"mu\", mu=0, sigma=3, dims=\"group\")\n    sigma = pm.Gamma(\"sigma\", alpha=2, beta=2)\n    pm.Normal(\"y\", mu=mu[group_idx], sigma=sigma, dims=\"obs_idx\")\n\nmodel.to_graphviz()\n\n\n\n\n\n\n\n\n\nmu_values, sigma_value, group_idx_values = pm.draw([mu, sigma, group_idx], random_seed=rng)\n\n# Fix model parameters _and_ data\nmodel_fixed_parameters = pm.do(\n    model,\n    {\"mu\": mu_values, \"sigma\": sigma_value, \"group_idx\": group_idx_values}\n)\n\n# Simulate values from the outcome\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\n\n# Observe the outcome values\nmodel_observed_data = pm.observe(\n    pm.do(model, {\"group_idx\": group_idx_values}),\n    {\"y\": y_values}\n)\n\nmodel_observed_data.to_graphviz()\n\n\n\n\n\n\n\n\n\nwith model_observed_data:\n    idata = pm.sample(random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\nThe great thing about coords and dims is that everything is labeled, so we no longer have to mentally map the positions to the names we have in our heads\n\naz.summary(idata, kind=\"stats\")\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\n\n\n\n\nmu[Group 1]\n0.388\n0.189\n0.028\n0.736\n\n\nmu[Group 2]\n-3.817\n0.150\n-4.105\n-3.550\n\n\nmu[Group 3]\n-3.890\n0.157\n-4.173\n-3.580\n\n\nmu[Group 4]\n3.016\n0.172\n2.680\n3.333\n\n\nsigma\n0.568\n0.062\n0.453\n0.681\n\n\n\n\n\n\n\n\naz.plot_posterior(idata, var_names=\"mu\");\n\n\n\n\n\n\n\n\nEven slicing of the posterior draws becomes easier.\n\nidata.posterior[\"mu\"].sel(group=\"Group 3\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'mu' (chain: 4, draw: 1000)&gt; Size: 32kB\narray([[-4.03705149, -3.78900314, -3.76479051, ..., -4.04628137,\n        -3.88638045, -4.21007427],\n       [-3.74006981, -4.03929702, -4.01893038, ..., -3.92875527,\n        -3.76260604, -4.02621378],\n       [-3.77833314, -3.85270145, -3.86176492, ..., -3.76218849,\n        -3.93732774, -3.91962555],\n       [-4.25014046, -3.70552177, -3.99825767, ..., -3.76144302,\n        -3.61515607, -3.93889675]])\nCoordinates:\n  * chain    (chain) int64 32B 0 1 2 3\n  * draw     (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\n    group    &lt;U7 28B 'Group 3'xarray.DataArray'mu'chain: 4draw: 1000-4.037 -3.789 -3.765 -4.043 -3.829 ... -4.011 -3.761 -3.615 -3.939array([[-4.03705149, -3.78900314, -3.76479051, ..., -4.04628137,\n        -3.88638045, -4.21007427],\n       [-3.74006981, -4.03929702, -4.01893038, ..., -3.92875527,\n        -3.76260604, -4.02621378],\n       [-3.77833314, -3.85270145, -3.86176492, ..., -3.76218849,\n        -3.93732774, -3.91962555],\n       [-4.25014046, -3.70552177, -3.99825767, ..., -3.76144302,\n        -3.61515607, -3.93889675]])Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])group()&lt;U7'Group 3'array('Group 3', dtype='&lt;U7')Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (0)"
  },
  {
    "objectID": "posts/2024-11-01_pymc-data-simulation/index.html#a-not-so-trivial-logistic-regression-model",
    "href": "posts/2024-11-01_pymc-data-simulation/index.html#a-not-so-trivial-logistic-regression-model",
    "title": "Data simulation with PyMC",
    "section": "A not-so-trivial logistic regression model",
    "text": "A not-so-trivial logistic regression model\nLet‚Äôs conclude this blog post by applying the concepts discussed in the previous examples to a more advanced example.\nHere, we work on a logistic regression model with varying intercepts and slopes. The covariates are given by a grouping variable and a continuous variable representing age.\n\nInitial attempt\n\\[\n\\begin{aligned}\nY_i \\mid \\pi_i &\\sim \\text{Bernoulli}(\\pi_i) \\\\\n\\pi_i & = \\text{expit}(\\alpha_{j[i]} + \\beta_{j[i]} \\times \\text{age}_i) \\\\\n\\alpha_j &\\sim \\text{Normal}(0, 1^2) & \\text{for all } j \\\\\n\\beta_j &\\sim \\text{Normal}(0, 1^2) & \\text{for all } j \\\\\n\\end{aligned}\n\\]\nwe have \\(J=3\\) groups and \\(N=200\\) observations in total.\n\nJ = 3\np = [0.5, 0.3, 0.2]\nN = 200\n\ncoords = {\n    \"group\": [\"A\", \"B\", \"C\"],\n    \"obs_idx\": np.arange(N)\n}\n\nwith pm.Model(coords=coords) as model:\n1    age = pm.Uniform(\"age\", lower=16, upper=70, dims=\"obs_idx\")\n2    group_idx = pm.Categorical(\"group_idx\", p=p, dims=\"obs_idx\")\n\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=1, dims=\"group\")\n    beta = pm.Normal(\"beta\", mu=0, sigma=1, dims=\"group\")\n\n3    pi = pm.Deterministic(\n        \"pi\",\n        pm.math.sigmoid(alpha[group_idx] + beta[group_idx] * age),\n        dims=\"obs_idx\"\n    )\n\n    pm.Bernoulli(\"y\", p=pi, dims=\"obs_idx\")\n\nmodel.to_graphviz()\n\n\n1\n\nAge is simulated using a uniform distribution between 18 and 70.\n\n2\n\nFor group memberships, we move from a uniform to a categorical, which enables us to assign different probabilities to each group.\n\n3\n\nWe register the success probability pi in the model graph using pm.Deterministic. Although pi is a random variable, it is represented as deterministic because its values are derived from other random variables and constant values.\n\n\n\n\n\n\n\n\n\n\n\nAt this point, we are quite familiar with what comes next:\n\n# Simulate values for parameters and data\nalpha_values, beta_values, age_values, group_idx_values = pm.draw(\n    [alpha, beta, age, group_idx],\n    random_seed=rng\n)\n\nprint(\"alpha:\", alpha_values)\nprint(\"beta:\", beta_values)\n\nprint(\"Groups and their frequencies:\")\nprint(np.unique(group_idx_values, return_counts=True))\n\n# Fix model parameters and_data\nmodel_fixed_parameters = pm.do(\n    model,\n    {\n        \"alpha\": alpha_values,\n        \"beta\": beta_values,\n        \"age\": age_values,\n        \"group_idx\": group_idx_values\n    }\n)\n\n# Simulate values from the outcome\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\n\n# Fix covariate values and observe the outcome\nmodel_observed_data = pm.observe(\n    pm.do(model, {\"age\": age_values, \"group_idx\": group_idx_values}),\n    {\"y\": y_values}\n)\n\nmodel_observed_data.to_graphviz()\n\nalpha: [0.47438197 0.20107513 1.36161128]\nbeta: [ 0.3889577  -1.65269511  2.58244597]\nGroups and their frequencies:\n(array([0, 1, 2]), array([98, 58, 44]))\n\n\n\n\n\n\n\n\n\nAnd, finally, we can get our sampler rolling:\n\nwith model_observed_data:\n    idata = pm.sample(random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, beta]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\nThere were 3306 divergences after tuning. Increase `target_accept` or reparameterize.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n\n\nWell, that doesn‚Äôt seem to work.\n\n\nTaking the covariate scale into account\nThe main goal of the blog post is not to show you how to diagnose the model and/or sampler when things are not working, so I will jump straight to the solution.\nThe sampler above is failing because the scale of the age covariate, combined with the slope parameters, resulted in an extremely large contribution to the linear predictor, which somehow ruined our computation.\nOne solution is to use a standardized version of age:\n\\[\n\\begin{aligned}\nY_i \\mid \\pi_i &\\sim \\text{Bernoulli}(\\pi_i) \\\\\n\\pi_i & = \\text{expit}(\\alpha_{j[i]} + \\beta_{j[i]} \\times \\text{age}^*_i) \\\\\n\\alpha_j &\\sim \\text{Normal}(0, 1^2) & \\text{for all } j \\\\\n\\beta_j &\\sim \\text{Normal}(0, 1^2) & \\text{for all } j \\\\\n\\end{aligned}\n\\]\nwhere:\n\\[\n\\text{age}^*_i = \\frac{\\text{age}_i - \\text{mean}(\\text{age})}{\\text{std}(\\text{age})}\n\\]\nWe need to import pytensor.tensor to compute the standard deviation of PyMC random variables.\n\nimport pytensor.tensor as pt\n\n\nJ = 3\np = [0.5, 0.3, 0.2]\nN = 200\n\ncoords = {\n    \"group\": [\"A\", \"B\", \"C\"],\n    \"obs_idx\": np.arange(N)\n}\n\nwith pm.Model(coords=coords) as model:\n    age = pm.Uniform(\"age\", lower=16, upper=70, dims=\"obs_idx\")\n    group_idx = pm.Categorical(\"group_idx\", p=p, dims=\"obs_idx\")\n\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=1, dims=\"group\")\n    beta = pm.Normal(\"beta\", mu=0, sigma=1, dims=\"group\")\n\n    age_scaled = pm.Deterministic(\n        \"age_scaled\", (age - pt.mean(age)) / pt.std(age), dims=\"obs_idx\"\n    )\n\n    pi = pm.Deterministic(\n        \"pi\",\n        pm.math.sigmoid(alpha[group_idx] + beta[group_idx] * age_scaled),\n        dims=\"obs_idx\"\n    )\n\n    pm.Bernoulli(\"y\", p=pi, dims=\"obs_idx\")\n\nmodel.to_graphviz()\n\n\n\n\n\n\n\n\nIt‚Äôs lovely to see how things come together in the graph. Let‚Äôs simulate now.\n\n# Fix model parameters and data, use the ones we got before\nmodel_fixed_parameters = pm.do(\n    model,\n    {\n        \"alpha\": alpha_values,\n        \"beta\": beta_values,\n        \"age\": age_values,\n        \"group_idx\": group_idx_values\n    }\n)\n\n# Simulate values from the outcome\ny_values = pm.draw(model_fixed_parameters[\"y\"], random_seed=rng)\n\n# Fix covariate values and observe the outcome\nmodel_observed_data = pm.observe(\n    pm.do(model, {\"age\": age_values, \"group_idx\": group_idx_values}),\n    {\"y\": y_values}\n)\n\nmodel_observed_data.to_graphviz()\n\n\n\n\n\n\n\n\nFingers crossed!\n\nwith model_observed_data:\n    idata = pm.sample(random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, beta]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\naz.plot_posterior(idata, var_names=\"alpha\", ref_val=alpha_values.tolist());\n\n\n\n\n\n\n\n\n\naz.plot_posterior(idata, var_names=\"beta\", ref_val=beta_values.tolist());\n\n\n\n\n\n\n\n\nThe sampler worked perfectly this time and we can also see that the posterior successfully recovered the true parameter values.\n\n\nOut of model predictions\nTo conclude this blog post, I would like to quickly show one last feature of PyMC that I really like: out of model predictions.\nImagine we want to see how the probability of success changes with age for the different groups. To create such a plot, we need a grid of age values along with posterior draws of \\(\\pi\\) conditional on each age value and group. However, the age values we currently have are not organized in a grid, they are just random. What should we do?\nOne can always manually work with the posterior draws to perform the necessary computations. But I usually prefer to let PyMC do that for me.\nWe are going to create a new PyMC model, which is like the previous logistic regression model, but uses the grid of age values that we want. Then, we are going to ask PyMC to ‚Äúpredict‚Äù (compute) the values of \\(\\pi\\) in this model using the draws we obtained with the previous model. That‚Äôs the out of model prediction.\n\n1age_mean = np.mean(age_values)\nage_std = np.std(age_values)\n\nage_range = np.arange(18, 71)\nage_values = np.tile(age_range, J)\ngroup_idx = np.repeat(np.arange(J), len(age_range))\n\ncoords = {\n    \"group\": [\"A\", \"B\", \"C\"],\n    \"obs_idx\": np.arange(len(age_values))\n}\n\nwith pm.Model(coords=coords) as model:\n    age_scaled_data = pm.Data(\"age_scaled\", (age_values - age_mean) / age_std, dims=\"obs_idx\")\n    group_idx_data = pm.Data(\"group_idx\", group_idx, dims=\"obs_idx\")\n\n2    alpha = pm.Flat(\"alpha\", dims=\"group\")\n    beta = pm.Flat(\"beta\", dims=\"group\")\n\n    pi = pm.Deterministic(\n        \"pi\",\n        pm.math.sigmoid(alpha[group_idx_data] + beta[group_idx_data] * age_scaled_data),\n        dims=\"obs_idx\"\n    )\n3\n\nmodel.to_graphviz()\n\n\n1\n\nCovariate transformations have to be done with the original summaries.\n\n2\n\nThe pm.Flat objects represent distributions from which sampling is not possible. We use them to be extra sure the forward sampling below uses the draws available in idata.\n\n3\n\nSince our goal is not to predict values of y, it does not need to be defined in this new model.\n\n\n\n\n\n\n\n\n\n\n\nNotice we do forward sampling specifying we want to sample from pi and setting predictions=True, which gives us a new group.\n\nwith model:\n    predictions = pm.sample_posterior_predictive(\n        idata, var_names=[\"pi\"], predictions=True, random_seed=rng\n    )[\"predictions\"]\n\nSampling: []\n\n\n\n\n\n\n\n\n\n\n\n\n1pi_mean = predictions[\"pi\"].mean((\"chain\", \"draw\"))\n2pi_lower, pi_upper = predictions[\"pi\"].quantile((0.025, 0.975), (\"chain\", \"draw\"))\n\nfig, ax = plt.subplots(figsize=(9, 5))\n\nfor j, group in enumerate([\"A\", \"B\", \"C\"]):\n    ax.plot(age_range, pi_mean.sel(obs_idx=group_idx==j), label=group)\n    ax.fill_between(\n        age_range,\n        pi_lower.sel(obs_idx=group_idx==j),\n        pi_upper.sel(obs_idx=group_idx==j),\n        alpha=0.5\n    )\n\nax.legend(title=\"Group\")\nax.set(xlabel=\"Age\", ylabel=\"P(Y = 1)\");\n\n\n1\n\nCompute the mean of \\(\\pi\\) across all chains and draws. It gives the mean for every observation.\n\n2\n\nCompute quantiles of \\(\\pi\\) across all chains and draws. It gives the bounds for a credible interval of \\(\\pi\\) for every observation."
  },
  {
    "objectID": "posts/2024-11-01_pymc-data-simulation/index.html#where-to-go-next",
    "href": "posts/2024-11-01_pymc-data-simulation/index.html#where-to-go-next",
    "title": "Data simulation with PyMC",
    "section": "Where to go next",
    "text": "Where to go next\nIf you, like me, enjoyed exploring these less obvious uses of PyMC, I invite you to check out two other articles I co-authored with Ricardo Vieira. They dive deeper into advanced data simulation and out-of-model predictions with PyMC.\n\nSimulating data with PyMC\nOut of model predictions with PyMC"
  },
  {
    "objectID": "posts/2024-11-01_pymc-data-simulation/index.html#footnotes",
    "href": "posts/2024-11-01_pymc-data-simulation/index.html#footnotes",
    "title": "Data simulation with PyMC",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou could also say it generates values from a random variable‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/es/index.html",
    "href": "posts/2025-08-14_temperatura-mate/es/index.html",
    "title": "En busca del mejor termo con Thomas Bayes",
    "section": "",
    "text": "English version\nC√≥digo\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport preliz as pz\nimport pymc as pm\nimport xarray as xr\n\nrandom_seed = sum(map(ord, \"cooling\"))\n\n\ndef compute_temperature(time, r, T_0, T_env):\n    return T_env + (T_0 - T_env) * np.exp(-r * time)\n\n\ndef plot_estimated_curves(idata, x, y, T_env, color=\"C0\", kind=\"mean\", axes=None):\n    if axes is None:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    else:\n        fig = axes[0].figure\n\n    t_grid = np.linspace(0, 9, num=100)\n    t_grid_xr = xr.DataArray(t_grid, coords={\"__idx__\": list(range(100))}, dims=\"__idx__\")\n\n    mu_transformed = idata.posterior[\"alpha\"] - idata.posterior[\"beta\"] * t_grid_xr\n\n    if kind == \"outcome\":\n        coords = mu_transformed.coords\n        dims = mu_transformed.dims\n        draws = np.random.normal(\n            loc=mu_transformed, scale=idata.posterior[\"sigma\"].to_numpy()[..., np.newaxis]\n        )\n\n        mu_transformed = xr.DataArray(\n            draws,\n            coords=coords,\n            dims=dims\n        )\n\n    mu_original = np.exp(mu_transformed) + T_env\n    mu_transformed_mean = mu_transformed.mean((\"chain\", \"draw\")).to_numpy()\n    mu_transformed_ci50 = mu_transformed.quantile((0.25, 0.75), (\"chain\", \"draw\")).to_numpy()\n    mu_transformed_ci90 = mu_transformed.quantile((0.05, 0.95), (\"chain\", \"draw\")).to_numpy()\n\n    mu_original_mean = mu_original.mean((\"chain\", \"draw\")).to_numpy()\n    mu_original_ci50 = mu_original.quantile((0.25, 0.75), (\"chain\", \"draw\")).to_numpy()\n    mu_original_ci90 = mu_original.quantile((0.05, 0.95), (\"chain\", \"draw\")).to_numpy()\n\n\n    axes[0].plot(t_grid, mu_original_mean, color=color)\n    axes[0].fill_between(\n        x=t_grid, y1=mu_original_ci50[0], y2=mu_original_ci50[1], alpha=0.5, color=color\n    )\n    axes[0].fill_between(\n        x=t_grid, y1=mu_original_ci90[0], y2=mu_original_ci90[1], alpha=0.5, color=color\n    )\n\n    axes[1].plot(t_grid, mu_transformed_mean, color=color)\n    axes[1].fill_between(\n        x=t_grid, y1=mu_transformed_ci50[0], y2=mu_transformed_ci50[1], alpha=0.5, color=color\n    )\n    axes[1].fill_between(\n        x=t_grid, y1=mu_transformed_ci90[0], y2=mu_transformed_ci90[1], alpha=0.5, color=color\n    )\n\n    axes[0].scatter(x, y, color=\"0.33\");\n    axes[1].scatter(x, np.log(y - T_env), color=\"0.33\");\n\n    axes[0].set(xlabel=\"Tiempo (horas)\", ylabel=\"Temperatura (¬∞C)\");\n    axes[1].set(xlabel=\"Tiempo (horas)\", ylabel=\"$\\\\log(T - T_\\\\text{ambiente})$\");\n\n    axes[0].grid(ls=\"--\")\n    axes[1].grid(ls=\"--\")\n    axes[0].set_axisbelow(True)\n    axes[1].set_axisbelow(True)\n\n    return fig, axes"
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/es/index.html#introducci√≥n",
    "href": "posts/2025-08-14_temperatura-mate/es/index.html#introducci√≥n",
    "title": "En busca del mejor termo con Thomas Bayes",
    "section": "Introducci√≥n",
    "text": "Introducci√≥n\nHace algunas semanas, mientras navegaba por internet, me encontr√© con este posteo en la red social X. All√≠, un productor agropecuario comentaba un experimento que estaba a punto de realizar para medir la capacidad de retener calor que ofrec√≠an varios termos que ten√≠a a su disposici√≥n.\n\nEl experimento consisti√≥ en colocar agua a 80 ¬∞C en cada uno de los termos y registrar su temperatura varias veces durante el d√≠a. Con los datos obtenidos, ser√≠a posible concluir sobre cu√°l termo ofrec√≠a la mejor (y cu√°l la peor) capacidad de retener el calor.\nLuego de varias mediciones, el autor del experimento comparti√≥ los resultados obtenidos:\n\nDe ellos se desprende que el termo ‚Äúnn tapa rosa‚Äù fue el de peor desempe√±o, ya que el agua en su interior perdi√≥ calor considerablemente m√°s r√°pidamente que en cualquier otro.\nAhora bien, la pregunta inevitable es: ¬øcu√°l es el termo que ofrece una mejor retenci√≥n de calor?\nPara responder a esta inc√≥gnita usaremos un modelo Bayesiano basado en al Ley de enfriamiento de Newton."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/es/index.html#ley-de-enfriamiento-de-newton",
    "href": "posts/2025-08-14_temperatura-mate/es/index.html#ley-de-enfriamiento-de-newton",
    "title": "En busca del mejor termo con Thomas Bayes",
    "section": "Ley de enfriamiento de Newton",
    "text": "Ley de enfriamiento de Newton\nLa ley de enfriamiento de Newton dice que la temperatura de un objeto cambia a una velocidad proporcional a la diferencia entre su temperatura y la del ambiente:\n\\[\n\\frac{dT(t)}{dt} = r \\, (T_\\text{env} - T(t))\n\\tag{1}\\]\ndonde \\(r\\) es una tasa de enfriamiento.\nUna soluci√≥n a la Ecuaci√≥n¬†1 es:\n\\[\nT(t) = T_\\text{env} + (T(0)- T_\\text{env}) \\, e^{-rt}\n\\tag{2}\\]\nEsto significa que la temperatura presenta un decaimiento exponencial hacia la del ambiente conforme pasa el tiempo.\nTambi√©n se puede notar que el logaritmo de la diferencia entre la temperatura en un tiempo \\(t\\) y la temperatura ambiente es una funci√≥n lineal:\n\\[\n\\log\\left(T(t) - T_\\text{env}\\right) = \\log\\left(T(0) - T_\\text{env}\\right) - rt\n\\]\nque puede escribirse de manera m√°s compacta como:\n\\[\n\\log\\left(T(t) - T_\\text{env}\\right) = \\alpha - \\beta t\n\\]\ndonde \\(\\alpha = \\log\\left(T(0) - T_\\text{env}\\right)\\) y \\(\\beta = r\\).\nLa figura debajo muestra la forma que toma la Ecuaci√≥n¬†2 para diferentes tasas de enfriamiento \\(r\\) junto a su correspondiente versi√≥n transformada.\n\n\nC√≥digo\nT_0, T_env = 80, 24\ntime_grid = np.linspace(0, 24, num=200)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nfor r in (0.02, 0.05, 0.1, 0.2, 0.5, 1):\n    axes[0].plot(\n        time_grid,\n        compute_temperature(time=time_grid, r=r, T_0=T_0, T_env=T_env),\n        label=f\"$r={r}$\"\n    );\n    axes[1].plot(\n        time_grid,\n        np.log(compute_temperature(time=time_grid, r=r, T_0=T_0, T_env=T_env) - T_env),\n        label=f\"$r={r}$\"\n    );\n\naxes[0].axhline(y=T_env, color=\"0.2\", lw=1.5, ls=\"--\")\naxes[0].grid(zorder=-99, ls=\"--\")\naxes[1].grid(zorder=-99, ls=\"--\")\naxes[1].set(ylim=(-5, 4.4))\n\naxes[0].legend()\naxes[0].set(xlabel=\"Tiempo\", ylabel=\"Temperatura\");\naxes[1].set(xlabel=\"Tiempo\", ylabel=\"$\\\\log(T(t) - T_\\\\text{ambiente})$\");\n\n\n\n\n\n\n\n\nFigura¬†1\n\n\n\n\n\nCuanto mayor sea el valor de \\(r\\), peor ser√° la capacidad del termo para retener temperatura. En otras palabras, el mejor termo ser√° aquel con el valor de \\(r\\) m√°s bajo (asumiendo que \\(r &gt; 0\\))."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/es/index.html#elucidaci√≥n-de-priors",
    "href": "posts/2025-08-14_temperatura-mate/es/index.html#elucidaci√≥n-de-priors",
    "title": "En busca del mejor termo con Thomas Bayes",
    "section": "Elucidaci√≥n de priors",
    "text": "Elucidaci√≥n de priors\nEn este art√≠culo vamos a trabajar con modelos de la siguiente forma:\n\\[\n\\begin{aligned}\n\\log(T(t_i) - T_\\text{env}) \\mid t_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\alpha - \\beta t_i\n\\end{aligned}\n\\]\nEs decir, para un tiempo dado \\(t_i\\), suponemos que el logaritmo de la diferencia entre la temperatura del agua y la temperatura ambiente sigue una distribuci√≥n normal.\nEl par√°metro de mayor inter√©s es \\(\\beta\\), que representa la tasa de enfriamiento del agua en el termo. En primer lugar, sabemos que su valor debe ser positivo, ya que la temperatura del agua inicial desciende hasta la temperatura ambiente. Adem√°s, a partir de las curvas mostradas en la Figura¬†1, podemos suponer que un rango razonable para este par√°metro se encuentra en el intervalo \\((0, 1)\\). Este rango implica que el agua del termo alcanzar√≠a la temperatura ambiente, como r√°pido, unas 5 horas despu√©s de haberlo llenado.\nUsando PreliZ se puede obtener los par√°metros de una distribuci√≥n gamma que satisfagan nuestros requisitos.\n\npz.maxent(pz.Gamma(), lower=0.001, upper=1, mass=0.99);\n\n\n\n\n\n\n\n\nOtro par√°metro desconocido en nuestro modelo es \\(\\sigma\\), el desv√≠o est√°ndar condicional. Es importante destacar que este desv√≠o no est√° expresado en grados cent√≠grados, ya que describe la variabilidad de \\(\\log(T(t_i) - T_\\text{env})\\), y no a la variablidad de \\(T(t_i)\\).\nAl observar el panel derecho de la Figura¬†1, se puede notar que el rango de variaci√≥n de la respuesta cubre apenas unas pocas unidades. Por eso, en este caso vamos a optar por una distribuci√≥n gamma moderadamente informativa, que concentre una alta probabilidad en el intervalo \\((0.05, 0.3)\\).\n\npz.maxent(pz.Gamma(), lower=0.05, upper=0.3, mass=0.95);\n\n\n\n\n\n\n\n\nFinalmente, podr√≠amos elucidar una distribuci√≥n a priori para \\(\\alpha\\). Sin embargo, no es un par√°metro con una interpretaci√≥n intuitiva.\nLo que s√≠ podemos hacer es establecer un prior para la temperatura inicial, que de manera impl√≠cita determine un prior para \\(\\alpha\\).\nDado que en nuestro caso conocemos la temperatura del agua en \\(t = 0\\), consideraremos dos enfoques:\n\nValor fijo para \\(T(0)\\): en este caso, \\(\\alpha\\) queda fijado a \\(\\log(T(0) - T_\\text{env})\\).\nDistribuci√≥n informativa para \\(T(0)\\): centrada en el valor observado. Usamos una distribuci√≥n normal con media \\(T(0)\\) y desv√≠o est√°ndar de \\(0.3\\) ¬∞C, lo que equivale a expresar que la temperatura inicial difiere, como m√°ximo, en un grado de la temperatura medida."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/es/index.html#datos",
    "href": "posts/2025-08-14_temperatura-mate/es/index.html#datos",
    "title": "En busca del mejor termo con Thomas Bayes",
    "section": "Datos",
    "text": "Datos\nA partir de la foto compartida en el posteo de X, se pueden obtener los siguientes valores de tiempos y temperaturas:\n\nT_env = 24\ntime = np.array([0, 180, 320, 500]) / 60 # en horas\ndata = {\n    \"stanley\": np.array([80.0, 70.0, 65.0, 60.0]),\n    \"aluminio\": np.array([80.0, 62.5, 57.5, 50.0]),\n    \"lumilagro\": np.array([75.0, 65.0, 60.0, 55.0]),\n    \"nn-rosa\": np.array([80.0, 47.5, 37.5, 30.0])\n}\n\nCabe destacar que el valor de la temperatura ambiente (T_env) es un supuesto, ya que no aparece en ninguno de los posteos.\n\n\nC√≥digo\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nfig.subplots_adjust(bottom=0.2)\n\nfor i, (brand, temperatures) in enumerate(data.items()):\n    axes[0].plot(time, temperatures, color=f\"C{i}\", lw=1)\n    axes[1].plot(time, np.log(temperatures - T_env), color=f\"C{i}\", lw=1)\n    axes[0].scatter(time, temperatures, color=f\"C{i}\", label=brand)\n    axes[1].scatter(time, np.log(temperatures - T_env), color=f\"C{i}\", label=brand)\n\naxes[0].set(xlabel=\"Tiempo (horas)\", ylabel=\"Temperatura (¬∞C)\");\naxes[1].set(xlabel=\"Tiempo (horas)\", ylabel=\"$\\\\log(T(t) - T_\\\\text{ambiente})$\");\naxes[0].grid(ls=\"--\")\naxes[1].grid(ls=\"--\")\n\naxes[0].axhline(y=T_env, color=\"0.2\", lw=1.5, ls=\"--\")\n\naxes[1].legend(\n    loc=\"lower center\",\n    ncol=4,\n    bbox_to_anchor=(0.5, 0.025),\n    bbox_transform=fig.transFigure,\n    handletextpad=0.1,\n    columnspacing=1\n);\n\n\n\n\n\n\n\n\nFigura¬†2\n\n\n\n\n\nA simple vista, el termo Stanley mantiene las temperaturas m√°s altas en todo momento, mientras que el de tapa rosa destaca por su pobre desempe√±o. El Lumilagro, por su parte, muestra un rendimiento superior al de aluminio: aunque comenz√≥ con una temperatura inicial m√°s baja, su enfriamiento fue m√°s lento. Por √∫ltimo, no puede afirmarse con certeza si el Stanley supera realmente al Lumilagro, ya que, si bien sus mediciones fueron siempre m√°s altas, tambi√©n parti√≥ con una temperatura mayor.\nPor otra parte, el panel derecho de Figura¬†2 muestra una tendencia lineal para cada termo, lo que es consistente con el uso de un modelo lineal sobre \\(\\log(T(t) - T_\\text{env})\\)."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/es/index.html#modelos",
    "href": "posts/2025-08-14_temperatura-mate/es/index.html#modelos",
    "title": "En busca del mejor termo con Thomas Bayes",
    "section": "Modelos",
    "text": "Modelos\n\nModelo 1: Un termo + intercepto conocido\nAntes de comenzar a trabajar con un modelo que considere a todas las marcas de termos, trabajemos con un modelo solo para la marca \"stanley\".\n\nwith pm.Model() as model_1:\n1    T_0 = pm.Data(\"T_0\", data[\"stanley\"].item(0))\n2    alpha = pm.Deterministic(\"alpha\", np.log(T_0 - T_env))\n    beta = pm.Gamma(\"beta\", alpha=3.3, beta=9)\n    sigma = pm.Gamma(\"sigma\", alpha=6.2, beta=37)\n    mu = alpha - beta * time\n    pm.Normal(\"log(T - T_env)\", mu=mu, sigma=sigma, observed=np.log(data[\"stanley\"] - T_env))\n\ndisplay(model_1.to_graphviz())\n\nwith model_1:\n3    idata_1 = pm.sample(random_seed=random_seed, target_accept=0.95)\n\n\n1\n\nSe registra la temperatura inicial como dato dentro del modelo para que muestre el nodo correspondiente en el grafo.\n\n2\n\nSe registra alpha con Deterministic para que sus valores se almacenen en el grupo posterior de InferenceData.\n\n3\n\nSe utiliza siempre la misma semilla random_seed y un valor alto de target_accept para reducir la chance de divergencias.\n\n\n\n\n\n\n\n\n\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [beta, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\nA continuaci√≥n, usamos az.summary() para obtener un resumen de los posteriors marginales:\n\naz.summary(idata_1, var_names=[\"beta\", \"sigma\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nbeta\n0.059\n0.010\n0.041\n0.081\n0.000\n0.000\n1421.0\n1350.0\n1.0\n\n\nsigma\n0.095\n0.047\n0.024\n0.179\n0.001\n0.001\n921.0\n899.0\n1.0\n\n\n\n\n\n\n\nPara ambos par√°metros, el tama√±o efectivo de muestra es lo suficientemente grande, y las cadenas convergen y se mezclan correctamente. 1\n\nfig, axes = plot_estimated_curves(idata=idata_1, x=time, y=data[\"stanley\"], T_env=T_env);\n\n\n\n\n\n\n\nFigura¬†3\n\n\n\n\n\nEn el panel derecho de Figura¬†3 se muestra la recta de regresi√≥n ajustada, junto con los intervalos de credibilidad del 50% y 95%, en la escala de datos transformada, mientras que en el panel izquierdo se muestran los resultados en la escala original. La recta de regresi√≥n se ajusta adecuadamente a los puntos, aunque la incertidumbre asociada a su estimaci√≥n aumenta a medida que transcurre el tiempo. Por otro lado, la nula incertidumbre en \\(t=0\\) se explica porque el intercepto tiene un valor fijo.\n\n\nModelo 2: Un termo + intercepto desconocido\nEn este segundo modelo seguimos trabajando con un solo termo. La diferencia est√° en que, en vez de fijar la temperatura inicial al valor observado, le asignamos una distribuci√≥n a priori muy informativa. De esta forma, seguimos incorporando la informaci√≥n que ya conocemos, pero no forzamos a la recta de regresi√≥n a pasar por un punto fijo.\n\nwith pm.Model() as model_2:\n    T_0 = pm.Normal(\"T_0\", mu=80, sigma=0.5)\n    alpha = pm.Deterministic(\"alpha\", np.log(T_0 - T_env))\n    beta = pm.Gamma(\"beta\", alpha=3.34, beta=12.8)\n    sigma = pm.Gamma(\"sigma\", alpha=6.2, beta=37)\n    mu = pm.Deterministic(\"mu\", alpha - beta * time)\n    pm.Normal(\"log(T - T_env)\", mu=mu, sigma=sigma, observed=np.log(data[\"stanley\"] - T_env))\n\ndisplay(model_2.to_graphviz())\n\nwith model_2:\n    idata_2 = pm.sample(random_seed=random_seed, target_accept=0.95)\n\n\n\n\n\n\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [T_0, beta, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\n\naz.summary(idata_2, var_names=[\"T_0\", \"alpha\", \"beta\", \"sigma\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nT_0\n79.993\n0.508\n79.056\n80.967\n0.010\n0.009\n2720.0\n2448.0\n1.0\n\n\nalpha\n4.025\n0.009\n4.008\n4.042\n0.000\n0.000\n2720.0\n2448.0\n1.0\n\n\nbeta\n0.059\n0.010\n0.040\n0.079\n0.000\n0.000\n1891.0\n1432.0\n1.0\n\n\nsigma\n0.096\n0.049\n0.022\n0.187\n0.001\n0.001\n1399.0\n1500.0\n1.0\n\n\n\n\n\n\n\n\nfig, axes = plot_estimated_curves(idata=idata_2, x=time, y=data[\"stanley\"], T_env=T_env);\n\n\n\n\n\n\n\n\n\naz.plot_forest([idata_1, idata_2], model_names=[\"Modelo 1\", \"Modelo 2\"], var_names=[\"beta\", \"sigma\"], combined=True, figsize=(6, 4));\n\n\n\n\n\n\n\n\nLos posteriors marginales de beta y sigma son pr√°cticamente id√©nticos a los del primer modelo. En otras palabras, las conclusiones que se extraen sobre beta y sigma son similares a las que se obtienen cuando se usa una temperatura inicial fija. Adem√°s, los tama√±os efectivos de muestra resultan siempre superiores a los obtenidos previamente.\n\n\nModelo 3: Todas las marcas\nAhora que ya estamos familiarizados con el modelo para una sola marca, podemos extenderlo para trabajar con todas las marcas.\nEn lugar de tener un √∫nico T_0, alpha y beta, tendremos uno para cada marca. En PyMC, esto se logra usando dims, lo que nos permite trabajar con vectores de variables aleatorias en lugar de escalares.\n\n1y = np.log(np.concatenate([temps for temps in data.values()]) - T_env)\ntimes = np.tile(time, 4)\nbrand_idx = np.repeat(np.arange(4), 4)\ncoords = {\n    \"brand\": list(data)\n}\n\nwith pm.Model(coords=coords) as model_3:\n    T_0 = pm.Normal(\"T_0\", mu=[v.item(0) for v in data.values()], sigma=0.5, dims=\"brand\")\n    alpha = pm.Deterministic(\"alpha\", np.log(T_0 - T_env), dims=\"brand\")\n    beta = pm.Gamma(\"beta\", alpha=3.3, beta=12.8, dims=\"brand\")\n    sigma = pm.Gamma(\"sigma\", alpha=6.2, beta=37)\n    mu = pm.Deterministic(\"mu\", alpha[brand_idx] - beta[brand_idx] * times)\n    pm.Normal(\"log(T - T_env)\", mu=mu, sigma=sigma, observed=y)\n\ndisplay(model_3.to_graphviz())\n\nwith model_3:\n    idata_3 = pm.sample(random_seed=random_seed, target_accept=0.95)\n\n\n1\n\nSe crean arreglos unidimensionales con las temperaturas, los tiempos y el √≠ndice de marca para todas las marcas. Adem√°s, se prepara un diccionario de coordenadas para poder usar dims=\"brand\" en el modelo\n\n\n\n\n\n\n\n\n\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [T_0, beta, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\n\n\nVeamos el resumen a posteriori que nos devuelve az.summary().\n\naz.summary(idata_3, var_names=[\"T_0\", \"beta\", \"sigma\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nT_0[stanley]\n79.971\n0.487\n79.047\n80.883\n0.008\n0.008\n3452.0\n2791.0\n1.0\n\n\nT_0[aluminio]\n79.895\n0.493\n79.028\n80.855\n0.008\n0.008\n3578.0\n3149.0\n1.0\n\n\nT_0[lumilagro]\n74.971\n0.483\n74.097\n75.889\n0.008\n0.007\n4030.0\n3046.0\n1.0\n\n\nT_0[nn-rosa]\n79.933\n0.487\n79.015\n80.797\n0.008\n0.008\n3840.0\n3018.0\n1.0\n\n\nbeta[stanley]\n0.056\n0.005\n0.047\n0.067\n0.000\n0.000\n2920.0\n2203.0\n1.0\n\n\nbeta[aluminio]\n0.096\n0.005\n0.086\n0.105\n0.000\n0.000\n2860.0\n2534.0\n1.0\n\n\nbeta[lumilagro]\n0.063\n0.005\n0.053\n0.073\n0.000\n0.000\n3685.0\n2653.0\n1.0\n\n\nbeta[nn-rosa]\n0.269\n0.005\n0.259\n0.278\n0.000\n0.000\n3425.0\n2750.0\n1.0\n\n\nsigma\n0.050\n0.014\n0.028\n0.075\n0.000\n0.000\n2049.0\n2109.0\n1.0\n\n\n\n\n\n\n\nLo primero que llama la atenci√≥n es que las medias a posteriori de las temperaturas iniciales se dividen en dos grupos: uno con valores cercanos a 80 y otro en torno a 75. Este resultado tiene sentido, ya que la temperatura inicial fue de 80 ¬∞C para todos los termos, excepto para el Lumilagro, que fue de 75 ¬∞C.\nPor su parte, los beta para cada termo tambi√©n muestran cierta heterogeneidad. En este caso, es posible concluir que el termo ‚Äúnn-rosa‚Äù es el que presenta la mayor p√©rdida de calor (mayor valor de beta), aunque no es posible determinar con certeza cu√°l de ellos ofrece la mejor capacidad de retener de temperatura.\nFinalmente, observamos que los tama√±os efectivos de muestra oscilan entre 2000 y 4000, superando en todos los casos a los obtenidos en los modelos previos. Esto sugiere que el posterior presenta una geometr√≠a m√°s accesible para el sampler NUTS que utiliza PyMC.\nSi usamos la funci√≥n az.plot_forest() podemos obtener un resumen de la distribuci√≥n marginal de beta para cada marca:\n\naz.plot_forest(idata_3, var_names=\"beta\", combined=True, figsize=(6, 4));\n\n\n\n\n\n\n\n\nComo se mencion√≥ anteriormente, el ‚Äúnn-rosa‚Äù presenta la mayor p√©rdida de calor, seguido por el termo ‚Äúaluminio‚Äù, y finalmente los termos ‚Äúlumilagro‚Äù y ‚Äústanley‚Äù. A simple vista, podr√≠a parecer que el ‚Äústanley‚Äù tiene una mejor capacidad para conservar la temperatura, aunque este gr√°fico por s√≠ solo no permite sacar conclusiones certeras.\nSi bien el par√°metro sigma no es relevante en s√≠ mismo, resulta interesante explorar su distribuci√≥n a posteriori, ya que ofrece una medida de la incertidumbre aleatoria alrededor de nuestra recta de regresi√≥n.\n\naz.plot_forest(\n    [idata_2, idata_3],\n    model_names=[\"Modelo 2\", \"Modelo 3\"],\n    var_names=\"sigma\",\n    combined=True,\n    figsize=(6, 4)\n);\n\n\n\n\n\n\n\n\nAl utilizar los datos de todos los termos en este tercer modelo, se obtiene una estimaci√≥n de sigma con una incertidumbre considerablemente menor.\nFinalmente, podemos visualizar las curvas estimadas para cada termos, tanto en su escala original como en la transformada:\n\nfor i, brand in enumerate(data):\n    fig, axes = plot_estimated_curves(\n        idata=idata_3.sel(brand=brand),\n        x=time,\n        y=data[brand],\n        T_env=T_env,\n        color=f\"C{i}\",\n    )\n    fig.suptitle(f\"Marca: {brand}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEn primer lugar, podemos notar que la incertidumbre en la recta de regresi√≥n para la marca ‚Äústanley‚Äù se redujo. Adem√°s, parece que en aquellos termos cuya temperatura del agua se acerc√≥ m√°s r√°pidamente a la temperatura ambiente (es decir, los de menor capacidad para retener el calor) la incertidumbre es menor. El ejemplo m√°s claro de este comportamiento es el termo ‚Äúnn-rosa‚Äù, cuya temperatura pr√°cticamente igual√≥ a la del ambiente hacia el final del experimento.\n\n\nModelo 4: Todas las marcas + partial pooling para \\(\\beta\\)\nPara finalizar, vamos a crear un modelo jer√°rquico donde asumimos que las tasas de enfriamiento \\(\\beta\\) pertenecen a una poblaci√≥n com√∫n. De este modo, al compartir informaci√≥n entre los termos, esperamos obtener posteriors m√°s precisos y estables, reflejando tanto las caracter√≠sticas individuales de cada termo como la tendencia general de la poblaci√≥n.\n\nwith pm.Model(coords=coords) as model_4:\n    T_0 = pm.Normal(\"T_0\", mu=[v.item(0) for v in data.values()], sigma=0.5, dims=\"brand\")\n    alpha = pm.Deterministic(\"alpha\", np.log(T_0 - T_env), dims=\"brand\")\n\n1    beta_mu = pm.Normal(\"beta_mu\", mu=3.3 / 12.8, sigma=0.05)\n2    beta_sigma = pm.InverseGamma(\"beta_sigma\", alpha=4.2, beta=0.3)\n    beta = pm.Gamma(\n        \"beta\",\n        alpha=(beta_mu / beta_sigma)**2,\n        beta=beta_mu / (beta_sigma**2),\n        dims=\"brand\"\n    )\n\n    sigma = pm.Gamma(\"sigma\", alpha=6.2, beta=37)\n    mu = pm.Deterministic(\"mu\", alpha[brand_idx] - beta[brand_idx] * times)\n    pm.Normal(\"log(T - T_env)\", mu=mu, sigma=sigma, observed=y)\n\n\nwith model_4:\n3    idata_4 = pm.sample_prior_predictive(draws=1000, random_seed=random_seed)\n    idata_4.extend(pm.sample(random_seed=random_seed, target_accept=0.95))\n\n\n1\n\nLos beta siguen teniendo un prior gamma con media y desv√≠o com√∫n. Pero en este caso estos par√°metros son desconocidos y determinados por los datos.\n\n2\n\nSe utiliz√≥ pz.maxent(pz.InverseGamma(), lower=0.01, upper=0.2, mass=0.95).\n\n3\n\nSe obtienen muestras del prior para generar la Figura¬†4.\n\n\n\n\nSampling: [T_0, beta, beta_mu, beta_sigma, log(T - T_env), sigma]\nInitializing NUTS using jitter+adapt_diag...\n/home/tomas/miniconda3/envs/pymc-env/lib/python3.12/site-packages/pytensor/tensor/elemwise.py:710: RuntimeWarning: invalid value encountered in log\n  variables = ufunc(*ufunc_args, **ufunc_kwargs)\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [T_0, beta_mu, beta_sigma, beta, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n\n\nDebajo se presentan los posteriors marginales de \\(\\beta\\) y \\(\\sigma\\) para ambos modelos, el no jer√°rquico y el jer√°rquico:\n\naz.plot_forest(\n    [idata_3, idata_4],\n    model_names=[\"Modelo 3\", \"Modelo 4\"],\n    var_names=[\"beta\", \"sigma\"],\n    combined=True,\n    figsize=(6, 4)\n);\n\n\n\n\n\n\n\n\nVisualmente, se puede concluir que no hay diferencias entre los posteriors marginales de ambos modelos. En otras palabras, el partial pooling que ofrece el modelo jer√°rquico es pr√°cticamente inexistente.\nPara entender por qu√© no se observan diferencias entre los modelos, podemos examinar el prior y el posterior de la media y el desv√≠o poblacional de \\(\\beta\\) (beta_mu y beta_sigma).\n\naz.plot_dist_comparison(idata_4, var_names=[\"beta_mu\", \"beta_sigma\"], figsize=(12, 15));\n\n\n\n\n\n\n\nFigura¬†4\n\n\n\n\n\nEn ambos casos, el posterior es muy similar al prior. Esto se debe a que la informaci√≥n disponible para estimar beta_mu y beta_sigma es insuficiente para obtener posteriors con baja incertidumbre. Este resultado es esperable, ya que el n√∫mero de grupos es muy bajo (solo 4). En situaciones as√≠, salvo que se disponga de abundante informaci√≥n a priori, el enfoque jer√°rquico no ofrecer√° diferencias apreciables respecto a un modelo con par√°metros independientes."
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/es/index.html#conclusiones",
    "href": "posts/2025-08-14_temperatura-mate/es/index.html#conclusiones",
    "title": "En busca del mejor termo con Thomas Bayes",
    "section": "Conclusiones",
    "text": "Conclusiones\n\n¬øCu√°l es el mejor termo?\nEl mejor termo es aquel que presenta la menor tasa de enfriamiento \\(\\beta\\).\nEn base a nuestro modelo, podemos obtener un resultado probabil√≠stico de la siguiente manera:\n\\[\nP(\\beta_j = \\min\\left\\{\\beta_1, \\beta_2, \\beta_3, \\beta_4 \\right\\} ) \\qquad \\forall j \\in {1, 2, 3, 4}\n\\]\n\nbeta_argmin = idata_4.posterior[\"beta\"].argmin(axis=-1)\np_min = [(beta_argmin == j).mean().item() for j in range(4)]\ndict(zip(data, p_min))\n\n{'stanley': 0.826, 'aluminio': 0.0, 'lumilagro': 0.174, 'nn-rosa': 0.0}\n\n\nSeg√∫n nuestro modelo, hay un 82% de probabilidad de que el termo de la marca Stanley sea el que mejor conserva la temperatura.\nEn la pr√°ctica, nos corresponde a nosotros evaluar si esa probabilidad es suficiente para concluir que ‚Äústanley‚Äù es efectivamente superior a ‚Äúlumilagro‚Äù. Por ejemplo, podr√≠a considerarse tambi√©n la diferencia en grados que ‚Äústanley‚Äù logra mantener por encima de ‚Äúlumilagro‚Äù a medida que pasa el tiempo.\nAunque el resultado no sea tan sorprendente, tambi√©n es posible determinar de manera probabil√≠stica cu√°l es el peor de los termos:\n\\[\nP(\\beta_j = \\max\\left\\{\\beta_1, \\beta_2, \\beta_3, \\beta_4 \\right\\} ) \\qquad \\forall j \\in {1, 2, 3, 4}\n\\]\n\nbeta_argmax = idata_4.posterior[\"beta\"].argmax(axis=-1)\np_max = [(beta_argmax == j).mean().item() for j in range(4)]\ndict(zip(data, p_max))\n\n{'stanley': 0.0, 'aluminio': 0.0, 'lumilagro': 0.0, 'nn-rosa': 1.0}"
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/es/index.html#ap√©ndice",
    "href": "posts/2025-08-14_temperatura-mate/es/index.html#ap√©ndice",
    "title": "En busca del mejor termo con Thomas Bayes",
    "section": "Ap√©ndice",
    "text": "Ap√©ndice\n\nInferencia en base a priors uniformes\nTiene sentido preguntarse si el trabajo que conlleva la especificaci√≥n de los priors vale la pena. Debajo, ajustamos el modelo multimarca utilizando priors uniformes y comparamos los resultados con los obtenidos anteriormente.\n\nwith pm.Model(coords=coords) as model_5:\n    alpha = pm.Flat(\"alpha\", dims=\"brand\")\n    beta = pm.Flat(\"beta\", dims=\"brand\")\n    sigma = pm.Uniform(\"sigma\", lower=0, upper=100)\n    mu = pm.Deterministic(\"mu\", alpha[brand_idx] - beta[brand_idx] * times)\n    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=y)\n\n    idata_5 = pm.sample(random_seed=random_seed, target_accept=0.99, progressbar=False)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, beta, sigma]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\n\n\n\naz.plot_forest(\n    [idata_4, idata_5],\n    model_names=[\"Priors informativos\", \"Priors uniformes\"],\n    var_names=[\"alpha\"],\n    combined=True,\n    figsize=(6, 4)\n);\n\n\n\n\n\n\n\n\n\nax = az.plot_forest(\n    [idata_4, idata_5],\n    model_names=[\"Priors informativos\", \"Priors uniformes\"],\n    var_names=[\"beta\"],\n    combined=True,\n    figsize=(6, 4)\n)\nax[0].grid(axis=\"x\", ls=\"--\");\n\n\n\n\n\n\n\n\n\nax = az.plot_forest(\n    [idata_4, idata_5],\n    model_names=[\"Priors informativos\", \"Priors uniformes\"],\n    var_names=[\"sigma\"],\n    combined=True, figsize=(6, 4)\n)\nax[0].grid(axis=\"x\", ls=\"--\");\n\n\n\n\n\n\n\n\n\nbeta_argmin = idata_5.posterior[\"beta\"].argmin(axis=-1)\np_min = [(beta_argmin == j).mean().item() for j in range(4)]\ndict(zip(data, p_min))\n\n{'stanley': 0.74575, 'aluminio': 0.001, 'lumilagro': 0.25325, 'nn-rosa': 0.0}\n\n\n\nbeta_argmax = idata_5.posterior[\"beta\"].argmax(axis=-1)\np_max = [(beta_argmax == j).mean().item() for j in range(4)]\ndict(zip(data, p_max))\n\n{'stanley': 0.0, 'aluminio': 0.0, 'lumilagro': 0.0, 'nn-rosa': 1.0}\n\n\nEn base a un modelo con priors uniformes se extraen conclusiones en la misma direcci√≥n, pero con un nivel mayor de incertidumbre.\n\n\n\nUn agradable Thomas Bayes tomando mate"
  },
  {
    "objectID": "posts/2025-08-14_temperatura-mate/es/index.html#footnotes",
    "href": "posts/2025-08-14_temperatura-mate/es/index.html#footnotes",
    "title": "En busca del mejor termo con Thomas Bayes",
    "section": "Notas",
    "text": "Notas\n\n\nEn este blog no hacemos uso de los traceplots porque las cadenas siempre se mezclan bien y resulta suficiente usar el tama√±o efectivo de muestra y el \\(\\hat{R}\\).‚Ü©Ô∏é"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The R2-D2 prior for GLMMs in Python\n            \n            \n                    Dec 6, 2025\n            \n            \n                    If you had to bet against a free throw, would you bet against the first or the second attempt?\nWhat if the player were a rookie? And what if it were a specific rookie?\nLet's try to find out.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                Do rookies get better with each throw? Playing Bayes with NBA free-throw data\n            \n            \n                    Sep 26, 2025\n            \n            \n                    If you had to bet against a free throw, would you bet against the first or the second attempt?\nWhat if the player were a rookie? And what if it were a specific rookie?\nLet's try to find out.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                In search of the best thermos for mate with Thomas Bayes\n            \n            \n                    Aug 14, 2025\n            \n            \n                    A few weeks ago, I saw a post on X where a farmer described an experiment to measure the heat retention of several thermoses. Can Bayesian statistics reveal the best one?\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                When to fix the intercept\n            \n            \n                    Aug 12, 2025\n            \n            \n                    I was modeling a case where I knew Y at X = 0, forcing me to choose between giving the intercept a strong prior or fixing it. Too lazy to do the math by hand, I used a computational approach to check if that choice made sense.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                Data simulation with PyMC\n            \n            \n                    Nov 1, 2024\n            \n            \n                    Parameter recovery studies are crucial for assessing whether a model can accurately recover the true values of its parameters.\nIn this post, I'll walk through a series of increasingly complex examples showing how to simulate data with PyMC in the context of such a study.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                How to create a custom family in Bambi?\n            \n            \n                    Jan 14, 2023\n            \n            \n                    Bambi now allows you to define custom statistical families in addition to its built-in distributions.\nThis post illustrates the process with the Zero-Inflated Poisson (ZIP) distribution,\nshowing how to implement it as a new family in Bambi and use it to simulate data, fit models, and assess their performance.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                Let's use tidypolars more\n            \n            \n                    Jun 26, 2022\n            \n            \n                    In this blogpost I'm going to show how to perform the same task with pandas, the most popular library for data analysis in Python,\nand tidypolars, a new library to do data analysis with tabular data inspired on the tidyverse.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                Hierarchical modeling with the LKJ prior in PyMC\n            \n            \n                    Jun 12, 2022\n            \n            \n                    I describe how to use the `LKJCholeskyCov` and `LKJCorr` distributions\nto include correlated priors in Bayesian hierarchical modeling using PyMC.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                GSOC 2021: Final evaluation\n            \n            \n                    Aug 17, 2021\n            \n            \n                    Final post about Google Summer of Code 2021. \nThis post sums up my contributions to the Bambi library during the ten weeks of this program.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                Binomial family in Bambi\n            \n            \n                    Aug 3, 2021\n            \n            \n                    My fourth post describing work done during GSoC 2021. On this\noccasion, I'm introducing the Binomial family. This new family is very useful\nto build models for binary data when each row in the data set contains the\nnumber of successes and the number of trials instead of the results of\nBernoulli trials.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                New families in Bambi\n            \n            \n                    Jul 14, 2021\n            \n            \n                    In this third post about my work during this Google Summer of Code\nI describe two families of models recently added. The first one, is the\nStudent T family, used to make linear regressions more robust. The second,\nis the Beta family which can be used to model ratings and proportions.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                Robust linear regression in Bambi\n            \n            \n                    Jul 5, 2021\n            \n            \n                    Second post about this Google Summer of Code season. \nToday I show some of the problems associated with outliers in linear regression and demonstrate\nhow one can implement a robust linear regression in Bambi.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                First weeks of GSoC\n            \n            \n                    Jun 28, 2021\n            \n            \n                    First post of a series about my contributions to Bambi in this Google Summer of Code season.\nThis post highlights new features related to default priors and priors for group-specific effects.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                Design matrices for group-specific effects in formulae and lme4\n            \n            \n                    Jun 8, 2021\n            \n            \n                    Bambi uses the library formulae to automatically construct design\nmatrices for both common and group-specific effects. This post compares design\nmatrices for group-specific effects obtained with formulae for a variety of\nscenarios involving categorical variables with the ones obtained with the R\npackage lme4.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                Why Bambi?\n            \n            \n                    May 24, 2021\n            \n            \n                    An example comparing how to fit a GLM with Bambi and PyMC3. Here I\nattempt to highlight how Bambi can help us to write a Bayesian GLM in a concise\nmanner, saving us from having to realize error-prone tasks that are sometimes\nnecessary when directly working with PyMC3.\n\n            \n        \n        \n            \n        \n    \n    \n    \n        \n            \n                How to generate bingo cards in R\n            \n            \n                    Nov 3, 2020\n            \n            \n                    A walkthrough the process of understanding how bingo cards are composed\nand a set of R functions that let us generate random bingo cards and print them\nin a nice looking .pdf output.\n\n            \n        \n        \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "posts/2025-12-06_r2d2_glmm/index.html",
    "href": "posts/2025-12-06_r2d2_glmm/index.html",
    "title": "The R2-D2 prior for GLMMs in Python",
    "section": "",
    "text": "In R-squared for Bayesian Regression Models by Gelman et al.¬†(2018), the authors propose a generalization of the classical coefficient of determination \\(R^2\\) to Bayesian regression models.\nThe coefficient of determination they propose is defined as:\n\\[\n\\frac{\\text{Explained variance}}{\\text{Explained variance} + \\text{Residual variance}} = \\frac{\\text{var}_\\text{fit}}{\\text{var}_\\text{fit} + \\text{var}_\\text{res}}\n\\]\nwhere \\(\\text{var}_\\text{fit}\\) and \\(\\text{var}_\\text{res}\\) are computed as:\n\\[\n\\begin{aligned}\n\\text{var}_\\text{fit} &= \\text{V}\\left(\\mathbb{E}(\\tilde{y}_i \\mid \\boldsymbol{x}_i, \\boldsymbol{\\theta})\\right) =  \\text{V} \\left(y_i^\\text{pred}\\right) \\\\\n\\text{var}_\\text{res} &= \\text{M}\\left(\\mathbb{V}\\left(\\tilde{y}_i - y_i^\\text{pred} \\mid \\boldsymbol{x}_i, \\boldsymbol{\\theta}\\right) \\right)\n\\end{aligned}\n\\]\nwhere \\(M\\) and \\(V\\) denote the sample mean and variance operators, respectively.\nThe first term is the variance of the expectation of future data and the second one is the expected variance of future residuals.\nThis Bayesian \\(R^2\\) is conditional on the explanatory variables and the model parameters \\(\\boldsymbol{\\theta}\\). For this reason, this \\(R^2\\) is proposed as an a posteriori mesasure of model fit.\nIf we have draws from the posterior distribution, we can compute the Bayesian \\(R^2\\) for each draw.\n\n\nIn the case of normal regression models, the components of the Bayesian \\(R^2\\) simplify to:\n\\[\n\\begin{aligned}\n\\text{var}_\\text{fit} &= \\text{V}(\\mu_i) \\\\\n\\text{var}_\\text{res} &= \\sigma^2\n\\end{aligned}\n\\]\nwhere \\(\\mu_i = \\boldsymbol{x}_i^T \\boldsymbol{\\beta}\\).\nAnd for logistic regression models, they become:\n\\[\n\\begin{aligned}\n\\text{var}_\\text{fit} &= \\text{V}(\\pi_i) \\\\\n\\text{var}_\\text{res} &= \\text{M}(\\pi_i (1 - \\pi_i))\n\\end{aligned}\n\\]\nwhere \\(\\pi_i = \\text{expit}(\\boldsymbol{x}_i^T \\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "posts/2025-12-06_r2d2_glmm/index.html#r2-for-bayesian-regression-models",
    "href": "posts/2025-12-06_r2d2_glmm/index.html#r2-for-bayesian-regression-models",
    "title": "The R2-D2 prior for GLMMs in Python",
    "section": "",
    "text": "In R-squared for Bayesian Regression Models by Gelman et al.¬†(2018), the authors propose a generalization of the classical coefficient of determination \\(R^2\\) to Bayesian regression models.\nThe coefficient of determination they propose is defined as:\n\\[\n\\frac{\\text{Explained variance}}{\\text{Explained variance} + \\text{Residual variance}} = \\frac{\\text{var}_\\text{fit}}{\\text{var}_\\text{fit} + \\text{var}_\\text{res}}\n\\]\nwhere \\(\\text{var}_\\text{fit}\\) and \\(\\text{var}_\\text{res}\\) are computed as:\n\\[\n\\begin{aligned}\n\\text{var}_\\text{fit} &= \\text{V}\\left(\\mathbb{E}(\\tilde{y}_i \\mid \\boldsymbol{x}_i, \\boldsymbol{\\theta})\\right) =  \\text{V} \\left(y_i^\\text{pred}\\right) \\\\\n\\text{var}_\\text{res} &= \\text{M}\\left(\\mathbb{V}\\left(\\tilde{y}_i - y_i^\\text{pred} \\mid \\boldsymbol{x}_i, \\boldsymbol{\\theta}\\right) \\right)\n\\end{aligned}\n\\]\nwhere \\(M\\) and \\(V\\) denote the sample mean and variance operators, respectively.\nThe first term is the variance of the expectation of future data and the second one is the expected variance of future residuals.\nThis Bayesian \\(R^2\\) is conditional on the explanatory variables and the model parameters \\(\\boldsymbol{\\theta}\\). For this reason, this \\(R^2\\) is proposed as an a posteriori mesasure of model fit.\nIf we have draws from the posterior distribution, we can compute the Bayesian \\(R^2\\) for each draw.\n\n\nIn the case of normal regression models, the components of the Bayesian \\(R^2\\) simplify to:\n\\[\n\\begin{aligned}\n\\text{var}_\\text{fit} &= \\text{V}(\\mu_i) \\\\\n\\text{var}_\\text{res} &= \\sigma^2\n\\end{aligned}\n\\]\nwhere \\(\\mu_i = \\boldsymbol{x}_i^T \\boldsymbol{\\beta}\\).\nAnd for logistic regression models, they become:\n\\[\n\\begin{aligned}\n\\text{var}_\\text{fit} &= \\text{V}(\\pi_i) \\\\\n\\text{var}_\\text{res} &= \\text{M}(\\pi_i (1 - \\pi_i))\n\\end{aligned}\n\\]\nwhere \\(\\pi_i = \\text{expit}(\\boldsymbol{x}_i^T \\boldsymbol{\\beta})\\)."
  },
  {
    "objectID": "posts/2025-12-06_r2d2_glmm/index.html#the-r2-d2-prior",
    "href": "posts/2025-12-06_r2d2_glmm/index.html#the-r2-d2-prior",
    "title": "The R2-D2 prior for GLMMs in Python",
    "section": "The R2-D2 prior",
    "text": "The R2-D2 prior\nThe R2-D2 prior for normal regression models has been introduced in the paper Bayesian Regression Using a Prior on the Model Fit: The R2-D2 Shrinkage Prior by Zhang et al.¬†(2016).\nConsider the normal regression model:\n\\[\n\\begin{aligned}\nY_i \\mid \\mu_i, \\sigma^2 &\\underset{iid}{\\sim} \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{pi} \\\\\n&= \\alpha + \\boldsymbol{X}\\boldsymbol{\\beta}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{X}\\) is the \\(n \\times p\\) design matrix (without the intercept), \\(\\alpha\\) is the intercept term, and \\(\\boldsymbol{\\beta}\\), of length \\(p\\), is the vector of coefficients.\nThe central idea in the R2-D2 prior is to place a prior directly on the coefficient of determination \\(R^2\\). For the purpose of defining prior distributions, however, Zhang et al.¬†work with the marginal coefficient of determination: a version of \\(R^2\\) that averages over both the design matrix \\(\\boldsymbol{X}\\) and the regression coefficients \\(\\boldsymbol{\\beta}\\), rather than the conditional coefficient of determination proposed by Gelman et al, that we mentioned above.\nFor the linear regression model, the marginal \\(R^2\\) is defined as:\n\\[\nR^2 = \\frac{\\mathbb{V}(\\boldsymbol{x}^T \\boldsymbol{\\beta})}{\\mathbb{V}(\\boldsymbol{x}^T \\boldsymbol{\\beta}) + \\sigma^2}\n= \\frac{\\sigma^2 W}{\\sigma^2 W + \\sigma^2}\n= \\frac{W}{W + 1}\n\\]\nwhich is the ratio of the marginal variance of the linear predictor to the marginal variance of the outcome.\nThen, the R2-D2 prior is specified as:\n\\[\n\\begin{aligned}\n\\beta_j &\\sim \\text{Normal}(0, \\phi_j W \\sigma^2) \\\\\n\\boldsymbol{\\phi} &\\sim \\text{Dirichlet}(\\xi_1, \\dots, \\xi_p) \\\\\nW & = \\frac{R^2}{1 - R^2}\\\\\nR^2 &\\sim \\text{Beta}(a, b) \\\\\n\\end{aligned}\n\\]\nThrough the transformation \\(W = R^2/(1-R^2)\\), the prior on \\(R^2\\) induces a prior on \\(W\\), which governs the total prior variance of the linear predictor \\(\\boldsymbol{x}^T \\boldsymbol{\\beta}\\). Combined with the Dirichlet prior on the variance proportions \\(\\boldsymbol{\\phi}\\), this results in the \\(R^2\\)-induced Dirichlet Decomposition (R2-D2) prior.\nIt can be shown that the induced prior on \\(W\\) is a Beta Prime distribution with parameters \\(a\\) and \\(b\\)."
  },
  {
    "objectID": "posts/2025-12-06_r2d2_glmm/index.html#r2-d2-for-glmms",
    "href": "posts/2025-12-06_r2d2_glmm/index.html#r2-d2-for-glmms",
    "title": "The R2-D2 prior for GLMMs in Python",
    "section": "R2-D2 for GLMMs",
    "text": "R2-D2 for GLMMs\nThe R2-D2 prior for Generalized Linear Mixed Models (GLMMs) has been introduced in the paper The R2D2 Prior for Generalized Linear Mixed Models by Yanchenko et al.¬†(2021).\nSuppose we have a GLMM of the form:\n\\[\n\\begin{aligned}\nY_i \\mid \\mu_i, \\theta &\\underset{iid}{\\sim} \\mathcal{F}(\\mu_i, \\theta) \\\\\ng(\\mu_i) = \\eta_i &= \\alpha + \\boldsymbol{x}_i \\boldsymbol{\\beta} + \\sum_{k=1}^{q}u_{k g_k[i]} \\\\\n\\end{aligned}\n\\]\nLet‚Äôs be super clear about what we have in there:\nThere are \\(n\\) observations indexed by \\(i \\in \\{1,\\dots,n\\}\\). The response variable for observation \\(i\\) is denoted \\(Y_i\\), with mean \\(\\mu_i\\). Conditional on \\(\\mu_i\\), and possibly additional parameters \\(\\theta\\), the response variables \\(Y_i\\) are assumed to be independent and identically distributed (\\(\\mathcal{F}\\) represents an arbitrary family). A link function \\(g(\\cdot)\\) relates the mean \\(\\mu_i\\) to the linear predictor \\(\\eta_i\\).\nThe linear predictor consists of an intercept \\(\\alpha\\), \\(p\\) explanatory variables, and \\(q\\) types of random effects. For the \\(i\\)-th observation, the covariates are collected in the vector \\(\\boldsymbol{x}_i = (x_{i1},\\dots,x_{ip})\\), and the full set of covariates for all observations forms the design matrix \\(\\boldsymbol{X}\\), of dimension \\(n \\times p\\). It is assumed that \\(\\boldsymbol{X}\\) has been standardized so that each column has mean zero and variance one.\nThe fixed-effect coefficients form the vector \\(\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_p)^T\\). For the \\(k\\)-th random effect, there are \\(L_k\\) levels collected in \\(\\boldsymbol{u}_k = (u_{k1},\\dots,u_{kL_k})^T\\), and we use \\(g_k[i]\\) indicate the level of the random effect \\(k\\) for the observation \\(i\\).\nTo construct the R2D2 prior for GLMMs, Yanchenko et. al.¬†(2021) specify the following prior model:\n\\[\n\\begin{aligned}\n\\beta_j \\mid \\phi_j, W &\\underset{iid}{\\sim}\\text{Normal}(0, \\phi_j W) \\\\\n\\boldsymbol{u}_k \\mid \\phi_{p + k}, W &\\underset{iid}{\\sim}\\text{Normal}(0, \\phi_{p + k}W \\boldsymbol{I}_{L_k})\n\\end{aligned}\n\\]\nThe parameter \\(W &gt; 0\\) governs the overall variance of the linear predictor, controlling the total amount of variation in the fixed and random effects. Larger values of \\(W\\) correspond to more flexible models, while smaller values shrink the model toward an intercept-only model.\nThe parameters \\(\\phi_j \\ge 0\\), which satisfy \\(\\sum_{j=1}^{p+q} \\phi_j = 1\\), determine how this total variance \\(W\\) is distributed across the individual fixed and random effect components. The vector of variance proportions \\(\\boldsymbol{\\phi} = (\\phi_1,\\dots,\\phi_{p+q})\\) is typically given a Dirichlet prior,\n\\[\n\\boldsymbol{\\phi} \\sim \\text{Dirichlet}(\\xi_1, \\dots, \\xi_{p+q}),\n\\]\nand in many applications all concentration parameters are set to a common value \\(\\xi_0\\). Larger values of \\(\\xi_0\\) shrink the variance proportions toward the uniform allocation \\(1 / (p+q)\\), while smaller values allow for more dispersed and uncertain allocations across components.\nLike in the linear model case, the R2-D2 prior for GLMMs is specified by placing a prior on the marginal \\(R^2\\) that averages over explanatory variables and random effects (\\(\\boldsymbol{X}\\) and \\(\\boldsymbol{g}\\)) as well as parameters (\\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{u}_k\\)).\nThe marginal \\(R^2\\) is defined as:\n\\[\nR^2 = \\frac{\\mathbb{V}(\\mathbb{E}(Y \\mid \\eta))}{\\mathbb{V}(\\mathbb{E}(Y \\mid \\eta)) + \\mathbb{E}(\\mathbb{V}(Y \\mid \\eta))}\n\\]\nSince \\(R^2\\) is based on summaries of the distribution of the linear predictor \\(\\eta\\), and it is assumed that \\(\\eta_i \\mid \\alpha, W \\sim \\text{Normal}(\\alpha, W)\\), \\(R^2\\) also depends on the parameters \\(\\alpha\\) and \\(W\\).\nThe essence of the R2-D2 prior for GLMMs is to define a joint prior on \\((\\alpha, W)\\) such that the resulting induced prior on \\(R^2\\) is \\(\\text{Beta}(a, b)\\).\nYanchenko et al.¬†(2021) construct such a prior by decomposing the joint distribution into a marginal prior for \\(\\alpha\\) and a conditional prior for \\(W \\mid \\alpha\\). They then choose a prior for \\(W \\mid \\alpha\\) that ensures the induced distribution of \\(R^2\\) is \\(\\text{Beta}(a, b)\\) for any fixed value of \\(\\alpha\\). Because this holds conditionally for all \\(\\alpha\\), the marginal distribution of \\(R^2\\) under the joint prior on \\((\\alpha, W)\\) is also \\(\\text{Beta}(a, b)\\), regardless of the marginal prior placed on \\(\\alpha\\). Combined with a Dirichlet prior on the variance proportions, this construction yields the R2-D2 prior for GLMMs.\nFor some model families, the prior on \\(W\\) that induces \\(R^2 \\sim \\text{Beta}(a, b)\\) can be derived in closed form. However, since this is not generally possible for GLMMs, the authors propose a unified approximate approach that works across all model families.\nThe idea is to place a Generalized Beta Prime (GBP) prior on \\(W\\) and choose its parameters so that the resulting induced distribution of \\(R^2\\) closely matches the target \\(\\text{Beta}(a, b)\\) distribution.\nThe problem is then to find the values of the parameters of the GBP prior \\((a^*, b^*, c^*, d^*)\\) so that the prior \\(W \\sim \\text{GBP}(a^*, b^*, c^*, d^*)\\) induces a prior on \\(R^2\\) that is close to \\(\\text{Beta}(a, b)\\).\nLet \\(W \\sim p(w)\\) be the distribution that yields exactly \\(R^2 \\sim \\text{Beta}(a, b)\\).\nTo obtain the values of the parameters of the GBP distribution Yanchenko et. al.¬†(2021) minimize the Pearson \\(\\chi^2\\) divergence between the true density \\(p(w)\\) and its GBP approximation \\(f_{GBP}\\), with an added regularization term that shrinks the solution toward \\(\\text{GBP}(a^*, b^*, 1, 1)\\), which is the exact solution for certain model families and can be considered the baseline.\nThe optimization problem is formulated as: \\[\n\\begin{aligned}\n(a^*, b^*, c^*, d^*)\n& =\n    \\arg\\min_{\\alpha, \\beta, c, d} \\left[\n        \\int_0^{\\infty}\n        \\left\\{\n            \\frac{f_{GBP}(w, \\alpha, \\beta, c, d) - p(w)}{p(w)}\n        \\right\\}^2\n        p(w) dw + \\lambda\n    \\left[\n        (\\alpha - a)^2 + (\\beta - b)^2 + (c - 1)^2 + (d - 1)^2\n    \\right]\n\\right]\n\\end{aligned}\n\\]\nHere, \\(\\lambda \\ge 0\\) is a tuning parameter that controls the amount of the regularization, which the authors suggest setting to \\(\\lambda = 1 / 4\\).\nIn practice, specifying the R2-D2 prior for GLMMs requires the user to choose the Beta hyperparameters \\(a\\) and \\(b\\) for the desired prior on \\(R^2\\). On then solves the optimization problem above to find the parameters \\((a^*, b^*, c^*, d^*)\\) of the GBP prior on \\(W\\). The resulting prior can be implemented via the transformation:\n\\[\nV \\sim \\text{Beta}(a^*, b^*) \\implies W = d^*\\left[\\frac{V}{1 - V}\\right]^{1/c^*} \\sim \\text{GBP}(a^*, b^*, c^*, d^*)\n\\]\nWhen doing inference wiht MCMC, because the optimal values \\((a^*, b^*, c^*, d^*)\\) depend on \\(\\alpha\\) and possibly other response-distribution parameters \\(\\theta\\), the approximation should be updated with their values at every iteration.\nHowever, performing this optimization at every MCMC iteration would be computationally infeasible. To address this, the authors recommend computing the GBP approximation once at the start of the analysis using \\(\\hat{\\alpha} = g(\\bar{Y}_i)\\) and, if needed, the maximum likelihood estimate \\(\\hat{\\theta}\\). Once the values of \\((a^*, b^*, c^*, d^*)\\) are found, \\(\\alpha\\) and \\(\\theta\\) are treated as unknown again during the subsequent Bayesian inference.\n\nGeneralized Beta Prime Approximation in Python\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport polars as pl\n\nfrom scipy import stats\nfrom scipy.optimize import minimize, minimize_scalar\nfrom scipy import special\n\n\ndef gbp_pdf_plain(x, a, b, c, d):\n    # Probability Density Function (PDF) of the Generalized Beta Prime (GBP) distribution\n    # Equation in Yanchenko et al. (2021)\n    return c * (x / d) ** (a * c - 1) * (1 + (x / d) ** c) ** (-a - b) / (d * special.beta(a, b))\n\n\ndef gbp_logpdf(x, a, b, c, d):\n    log_num = (\n        np.log(c)\n        + (a * c - 1) * (np.log(x) - np.log(d))\n        - (a + b) * np.log1p((x / d) ** c)\n    )\n    log_den = np.log(d) + special.betaln(a, b)\n    return log_num - log_den\n\n\ndef gbp_pdf_stable(x, a, b, c, d):\n    return np.exp(gbp_logpdf(x, a, b, c, d))\n\nprint(gbp_pdf_plain(2, 2, 2, 4.5, 3.8))\nprint(gbp_pdf_stable(2, 2, 2, 4.5, 3.8))\n\n0.03368536527260107\n0.03368536527260113\n\n\n\nclass GenericFamily:\n    def cdf(self, w):\n        # Cumulative Distribution Function (CDF) of W\n        # Computes values of the CDF of W induced by a Beta(a, b) prior on R-squared.\n        # The strategy is to convert W to R-squared and find the CDF of the corresponding R-squared.\n        return stats.beta.cdf(self.W_to_R2(w), a=self.a, b=self.b)\n\n    def ppf(self, p, bounds=None):\n        # Quantile Function of W\n        # Computes the quantiles of W induced by a Beta(a, b) prior on R2.\n\n        if bounds is None:\n            bounds = self._find_bounds()\n\n        log_bounds = (np.log(bounds[0]), np.log(bounds[1]))\n\n        # Numerically inverts the CDF of W.\n        def inner(p):\n            def distance(logw):\n                return (self.cdf(w=np.exp(logw)) - p) ** 2\n\n            result = minimize_scalar(distance, bounds=log_bounds)\n            output = result.x\n\n            return np.exp(output)\n\n        return np.array([inner(p_i) for p_i in np.atleast_1d(p)])\n\n    def _find_bounds(self):\n        # NOTE: Larger upper bounds make it fail sometimes\n        ub_candidates = [1, 10, 100, 1_000, 10_000, 100_000, 500_000, 1_000_000]\n        for candidate in ub_candidates:\n            if self.cdf(candidate) &gt; 0.99:\n                break\n        bounds = (1 / 100_000, candidate)\n        return bounds\n\n\nclass PoissonFamily(GenericFamily):\n    def __init__(self, a, b, intercept):\n        self.a = a\n        self.b = b\n        self.intercept = intercept\n\n    def W_to_R2(self, w):\n        return np.expm1(w) / (np.expm1(w) + np.exp(-self.intercept -0.5 * w))\n\n    def pdf(self, w):\n        a, b, alpha = self.a, self.b, self.intercept\n        f1 = 1 / special.beta(a, b)\n        f2_num = np.expm1(w) ** (a - 1) * np.exp(-b * (alpha + w / 2)) * (3 * np.exp(w) - 1)\n        f2_den = 2 * (np.expm1(w) + np.exp(-alpha - w / 2)) ** (a + b)\n        return f1 * (f2_num / f2_den)\n\n\nclass NegativeBinomialFamily(GenericFamily):\n    def __init__(self, a, b, intercept, theta):\n        self.a = a\n        self.b = b\n        self.intercept = intercept\n        self.theta = theta\n\n    def W_to_R2(self, w):\n        return np.expm1(w) / (np.expm1(w) + self.theta * np.exp(-self.intercept -0.5 * w))\n\n    def pdf(self, w):\n        a, b, alpha, theta = self.a, self.b, self.intercept, self.theta\n        f1 = theta ** b / special.beta(a, b)\n        f2_num = np.expm1(w) ** (a - 1) * np.exp(-b * (alpha + w / 2)) * (3 * np.exp(w) - 1)\n        f2_den = 2 * (np.expm1(w) + theta * np.exp(-alpha - w / 2)) ** (a + b)\n        return f1 * (f2_num / f2_den)\n\n\nclass GaussianFamily(GenericFamily):\n    def __init__(self, a, b, intercept, sigma):\n        self.a = a\n        self.b = b\n        self.intercept = intercept\n        self.sigma = sigma\n\n    def W_to_R2(self, w):\n        return w / (w + self.sigma ** 2)\n\n    def pdf(self, w):\n        return gbp_pdf_stable(w, self.a, self.b, 1, self.sigma ** 2)\n\n\nclass LogisticFamily(GenericFamily):\n    def __init__(self, a, b, intercept):\n        self.a = a\n        self.b = b\n        self.intercept = intercept\n\n    def _W_to_R2_scalar(self, w, K=1000):\n        # Equation 13 in Yanchenko et al. (2021)\n        # We are estimating the integrals via Quasi-Monte Carlo (QMC) integration\n        p_grid = np.linspace(1, K - 1, num=K - 1) / K\n        eta = stats.norm(loc=self.intercept, scale=w ** 0.5).ppf(p_grid)\n        mu = self._mean(eta)\n        mu_1 = np.mean(mu).item()\n        mu_2 = np.mean(mu ** 2).item()\n        sigma_squared = np.mean(self._var(eta)).item()\n\n        # Usage of M and V comes from Equation 3\n        M = mu_2 - mu_1 ** 2 # E(mu^2) - E(mu)^2\n        V = sigma_squared\n\n        return M / (M + V)\n\n    def W_to_R2(self, w):\n        # Vectorized version of Equation 13 in Yanchenko et al. (2021)\n        return np.array([self._W_to_R2_scalar(w) for w in np.atleast_1d(w)])\n\n    def pdf(self, w):\n        # Density function of W\n        # Computes values of the density function of W induced by a Beta(a, b) prior on R2.\n        # The computation uses a numeric approxiation to the derivative of the CDF.\n\n        delta = 0.001\n        diff = self.cdf(w=w + delta) - self.cdf(w=w - delta)\n        return diff / (2 * delta)\n\n    def _mean(self, eta):\n        return special.expit(eta)\n\n    def _var(self, eta):\n        mean = self._mean(eta)\n        return mean * (1 - mean)\n\n\ndef WGBP(family, lam=0.25, x0=np.ones(4), method=\"Powell\"):\n    \"\"\"Compute parameters for the GBP Approximation\n\n    This function finds the closest generalized beta prime (GBP) distribution to the true\n    pdf of W as measured by the Pearson chi-squared divergence.\n    \"\"\"\n    a, b = family.a, family.b\n\n    tau = np.linspace(0.01, 0.99, num=500)\n    w = family.ppf(p=tau).flatten()\n    p_W = family.pdf(w=w).flatten()\n\n    # Copied from the R implementation because sometimes p_W has NaNs\n    w = w[~np.isnan(p_W)]\n    f_true = p_W[~np.isnan(p_W)]\n\n    def distance_fun(log_params):\n        params = np.exp(log_params)\n        f_GBP = gbp_pdf_stable(w, *params)\n        log_target = np.log(np.array([a, b, 1, 1]))\n\n        integral = np.sum((1 - f_GBP / f_true) ** 2)\n        penalty = lam * np.sum((log_params - log_target) ** 2)\n\n        return integral + penalty\n\n    result = minimize(distance_fun, x0=np.log(x0), method=method)\n\n    if result.success:\n        return np.exp(result.x)\n\n    raise Exception(\"Minimization didn't converge\")\n\n\ndef penalized_divergence(f_true, f_approx, penalty, lam=0.25):\n    # Penalized Pearson chi-squared divergence\n    integral = np.sum((1 - f_approx / f_true) ** 2)\n    penalty = lam * penalty\n    return integral + penalty"
  },
  {
    "objectID": "posts/2025-12-06_r2d2_glmm/index.html#examples",
    "href": "posts/2025-12-06_r2d2_glmm/index.html#examples",
    "title": "The R2-D2 prior for GLMMs in Python",
    "section": "Examples",
    "text": "Examples\n\ndf = pl.read_csv(\"data/paper_approximations.csv\")\ndf\n\n\nshape: (45, 8)\n\n\n\nbeta_0\na\nb\na_star\nb_star\nc_star\nd_star\nfamily\n\n\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n-2\n0.5\n0.5\n0.19\n0.77\n4.22\n3.17\n\"poisson\"\n\n\n-2\n0.5\n0.5\n0.48\n0.22\n1.23\n1.78\n\"logistic\"\n\n\n-2\n0.5\n0.5\n0.21\n0.74\n4.78\n3.49\n\"negative binomial\"\n\n\n-2\n1.0\n1.0\n0.42\n1.5\n3.75\n2.56\n\"poisson\"\n\n\n-2\n1.0\n1.0\n1.45\n0.51\n0.99\n1.74\n\"logistic\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2\n4.0\n1.0\n8.21\n0.65\n0.74\n1.49\n\"logistic\"\n\n\n2\n4.0\n1.0\n2.16\n2.78\n1.35\n0.87\n\"negative binomial\"\n\n\n2\n4.0\n4.0\n3.66\n9.86\n0.97\n0.36\n\"poisson\"\n\n\n2\n4.0\n4.0\n8.15\n2.18\n0.88\n1.57\n\"logistic\"\n\n\n2\n4.0\n4.0\n2.92\n6.44\n1.24\n0.43\n\"negative binomial\"\n\n\n\n\n\n\n\ndef plot_w_approximations(rows_dict, family, **family_kwargs):\n\n    def format_params(params):\n        names = [\"a\", \"b\", \"c\", \"d\"]\n        return \"$\" + \", \".join(f\"{n}^*:{p:.2f}\" for n, p in zip(names, params)) + \"$\"\n\n    fig, axes = plt.subplots(5, 1, figsize=(4, 15), layout=\"constrained\")\n    for ax, row in zip(axes.ravel(), rows_dict):\n        a = row[\"a\"]\n        b = row[\"b\"]\n        intercept = row[\"beta_0\"]\n        a_star = row[\"a_star\"]\n        b_star = row[\"b_star\"]\n        c_star = row[\"c_star\"]\n        d_star = row[\"d_star\"]\n\n        family_obj = family(a=a, b=b, intercept=intercept, **family_kwargs)\n\n        # NOTE: This is a patch...|\n        w_lower = family_obj.ppf(0.005).item()\n        w_upper = family_obj.ppf(0.995).item()\n        w_upper = min(w_upper, 250)\n\n        w = np.linspace(w_lower, w_upper, num=500)\n        params = WGBP(family_obj, lam=0.25)\n        w_gbp_pdf = gbp_pdf_stable(w, *params)\n        w_gbp_paper_pdf = gbp_pdf_stable(w, a_star, b_star, c_star, d_star)\n        w_pdf = family_obj.pdf(w=w)\n\n        divergence_own = penalized_divergence(\n            w_pdf,\n            w_gbp_pdf,\n            penalty=np.sum((np.log(params) - np.log([a, b, 1, 1])) ** 2)\n        )\n        divergence_paper = penalized_divergence(\n            w_pdf,\n            w_gbp_paper_pdf,\n            penalty=np.sum((np.log([a_star, b_star, c_star, d_star]) - np.log([a, b, 1, 1])) ** 2)\n        )\n\n        ax.plot(w, w_pdf, label=\"True\")\n        ax.plot(w, w_gbp_pdf, label=f\"Approx [own] (d={divergence_own:.2f})\")\n        ax.plot(w, w_gbp_paper_pdf, color=\"0.3\", ls=\"--\", label=f\"Approx [paper] (d={divergence_paper:.2f})\")\n\n\n        ax.set_title(\n            format_params(params) + \" [own]\\n\"\n            + format_params((a_star, b_star, c_star, d_star)) + \" [paper]\",\n            loc=\"left\"\n        )\n        ax.legend(title=f\"a={a}, b={b}\")\n\n    return fig, axes\n\n\nPoisson\n\ndef format_params(params):\n    names = [\"a\", \"b\", \"c\", \"d\"]\n    return \"$\" + \", \".join(f\"{n}^*={p:.2f}\" for n, p in zip(names, params)) + \"$\"\n\n\nfamily_obj = PoissonFamily(4, 4, -2)\na, b = 4, 4\nw_lower = family_obj.ppf(0.005).item()\nw_upper = family_obj.ppf(0.995).item()\nw_upper = min(w_upper, 250)\nw = np.linspace(w_lower, w_upper, num=500)\n\nparams_own = WGBP(family_obj)\nparams_paper = [2.00, 6.38 ,3.14, 2.25]\n\npdf_own = gbp_pdf_stable(w, *params_own)\npdf_paper = gbp_pdf_stable(w, *params_paper)\nw_pdf = family_obj.pdf(w=w)\n\ndivergence_own = penalized_divergence(\n    w_pdf,\n    pdf_own,\n    penalty=np.sum((np.log(params_own) - np.log([a, b, 1, 1])) ** 2)\n)\ndivergence_paper = penalized_divergence(\n    w_pdf,\n    pdf_paper,\n    penalty=np.sum((np.log(params_paper) - np.log([a, b, 1, 1])) ** 2)\n)\n\n\nfig, ax = plt.subplots(figsize=(6, 5), tight_layout=True)\n\nax.plot(w, pdf_paper, label=\"Paper\")\nax.plot(w, pdf_own, label=\"Own\")\nax.plot(w, w_pdf, color=\"0.3\", ls=\"--\")\n\ns_list = [\n    \"$\\\\bf{Approximations}$\",\n    format_params(params_paper) + \" [paper]\",\n    format_params(params_own) + \" [own]\",\n    \"$\\\\bf{Divergences}$\",\n    f\"{divergence_paper:.2f} [paper] vs \" + f\"{divergence_own:.2f} [own]\",\n]\n\nax.text(\n    x=0,\n    y=1.025,\n    s=\"\\n\".join(s_list),\n    ha=\"left\",\n    size=11,\n    transform=ax.transAxes\n)\n\nax.text(\n    x=1,\n    y=1.025,\n    s=\"$R^2 \\\\sim \\\\text{Beta}(4, 4)$\",\n    ha=\"right\",\n    size=13,\n    transform=ax.transAxes\n)\n\nax.set(xlabel=\"W\", yticks=[])\nax.legend();\nfig.savefig(\"asd.png\")\n\n\n\n\n\n\n\n\n\ndef plot_w_apprximations(w, pdfs, title=\"\", figsize=(4, 3)):\n    fig, ax = plt.subplots(figsize=figsize)\n\n    for label, pdf in pdfs.items():\n        ax.plot(w, pdf, label=label)\n\n    ax.set_title(title)\n    ax.legend()\n    return fig, ax\n\n\nrows_dict = df.filter(\n    pl.col(\"family\") == \"poisson\",\n    pl.col(\"beta_0\") == -2\n).to_dicts()\n\nfig, axes = plot_w_approximations(rows_dict, PoissonFamily);\nfig.suptitle(\"Family: Poisson, Intercept: -2\", y=1.05, fontsize=16);\n\n\n\n\n\n\n\n\n\nrows_dict = df.filter(\n    pl.col(\"family\") == \"poisson\",\n    pl.col(\"beta_0\") == 0\n).to_dicts()\n\nfig, axes = plot_w_approximations(rows_dict, PoissonFamily)\nfig.suptitle(\"Family: Poisson, Intercept: 0\", y=1.05, fontsize=16);\n\n\n\n\n\n\n\n\n\nrows_dict = df.filter(\n    pl.col(\"family\") == \"poisson\",\n    pl.col(\"beta_0\") == 2\n).to_dicts()\n\nfig, axes = plot_w_approximations(rows_dict, PoissonFamily)\nfig.suptitle(\"Family: Poisson, Intercept: 0\", y=1.05, fontsize=16);\n\n\n\n\n\n\n\n\n\n\nLogistic\n\nrows_dict = df.filter(\n    pl.col(\"family\") == \"logistic\",\n    pl.col(\"beta_0\") == -2\n).to_dicts()\n\nfig, axes = plot_w_approximations(rows_dict, LogisticFamily)\nfig.suptitle(\"Family: Logistic, Intercept: -2\", y=1.05, fontsize=16);\n\n/tmp/ipykernel_96055/4149930673.py:98: RuntimeWarning: invalid value encountered in scalar power\n  eta = stats.norm(loc=self.intercept, scale=w ** 0.5).ppf(p_grid)\n\n\n\nrows_dict = df.filter(\n    pl.col(\"family\") == \"logistic\",\n    pl.col(\"beta_0\") == 0\n).to_dicts()\n\nfig, axes = plot_w_approximations(rows_dict, LogisticFamily)\nfig.suptitle(\"Family: Logistic, Intercept: 0\", y=1.05, fontsize=16);\n\n/tmp/ipykernel_71151/4149930673.py:98: RuntimeWarning: invalid value encountered in scalar power\n  eta = stats.norm(loc=self.intercept, scale=w ** 0.5).ppf(p_grid)\n\n\n\n\n\n\n\n\n\n\nrows_dict = df.filter(\n    pl.col(\"family\") == \"logistic\",\n    pl.col(\"beta_0\") == 2\n).to_dicts()\n\nfig, axes = plot_w_approximations(rows_dict, LogisticFamily)\nfig.suptitle(\"Family: Logistic, Intercept: 2\", y=1.05, fontsize=16);\n\n/tmp/ipykernel_71151/4149930673.py:98: RuntimeWarning: invalid value encountered in scalar power\n  eta = stats.norm(loc=self.intercept, scale=w ** 0.5).ppf(p_grid)\n\n\n\n\n\n\n\n\n\nIsn‚Äôt it better to use values of ‚Äòa‚Äô and ‚Äòb‚Äô that are above 1 in both cases? See for example\n\n# fig, axes = plt.subplots(2, 3, figsize=(15, 8), layout=\"constrained\")\n\n# ab_pairs = ((1.5, 1.5), (10, 2), (2, 10), (6, 6), (20, 20), (20, 2))\n# intercept = 2\n\n# for ax, (a, b) in zip(axes.ravel(), ab_pairs):\n#     family = LogisticFamily(a=a, b=b, intercept=intercept)\n#     params = WGBP(family, lam=0.25)\n\n#     w = np.linspace(0.01, family.ppf(0.9).item(), num=500)\n#     w_gbp_pdf = gbp_pdf_stable(w, *params)\n#     ax.plot(w, w_gbp_pdf, label=\"GBP Approx\");\n#     ax.set(title=f\"a={a}, b={b}\\n\" + format_params(params))\n#     ax.legend()\n\n\n\nNegative Binomial family\n\ndf\n\n\nshape: (45, 8)\n\n\n\nbeta_0\na\nb\na_star\nb_star\nc_star\nd_star\nfamily\n\n\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n-2\n0.5\n0.5\n0.19\n0.77\n4.22\n3.17\n\"poisson\"\n\n\n-2\n0.5\n0.5\n0.48\n0.22\n1.23\n1.78\n\"logistic\"\n\n\n-2\n0.5\n0.5\n0.21\n0.74\n4.78\n3.49\n\"negative binomial\"\n\n\n-2\n1.0\n1.0\n0.42\n1.5\n3.75\n2.56\n\"poisson\"\n\n\n-2\n1.0\n1.0\n1.45\n0.51\n0.99\n1.74\n\"logistic\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2\n4.0\n1.0\n8.21\n0.65\n0.74\n1.49\n\"logistic\"\n\n\n2\n4.0\n1.0\n2.16\n2.78\n1.35\n0.87\n\"negative binomial\"\n\n\n2\n4.0\n4.0\n3.66\n9.86\n0.97\n0.36\n\"poisson\"\n\n\n2\n4.0\n4.0\n8.15\n2.18\n0.88\n1.57\n\"logistic\"\n\n\n2\n4.0\n4.0\n2.92\n6.44\n1.24\n0.43\n\"negative binomial\"\n\n\n\n\n\n\n\nrows_dict = df.filter(\n    pl.col(\"family\") == \"negative binomial\",\n    pl.col(\"beta_0\") == -2\n).to_dicts()\n\nfig, axes = plot_w_approximations(rows_dict, NegativeBinomialFamily, theta=2)\nfig.suptitle(\"Family: Negative Binomial, Intercept: -2\", y=1.05, fontsize=16);\n\n/tmp/ipykernel_71151/1913648031.py:11: RuntimeWarning: overflow encountered in power\n  - (a + b) * np.log1p((x / d) ** c)\n\n\n\n\n\n\n\n\n\n\nrows_dict = df.filter(\n    pl.col(\"family\") == \"negative binomial\",\n    pl.col(\"beta_0\") == 0\n).to_dicts()\n\nfig, axes = plot_w_approximations(rows_dict, NegativeBinomialFamily, theta=2)\nfig.suptitle(\"Family: Negative Binomial, Intercept: 0\", y=1.05, fontsize=16);\n\n\n\n\n\n\n\n\n\nrows_dict = df.filter(\n    pl.col(\"family\") == \"negative binomial\",\n    pl.col(\"beta_0\") == 2\n).to_dicts()\n\nfig, axes = plot_w_approximations(rows_dict, NegativeBinomialFamily, theta=2)\nfig.suptitle(\"Family: Negative Binomial, Intercept: 2\", y=1.05, fontsize=16);"
  }
]