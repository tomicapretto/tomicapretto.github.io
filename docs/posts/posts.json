[
  {
    "path": "posts/2021-08-17-gsoc-2021-final-evaluation/",
    "title": "GSOC 2021: Final evaluation",
    "description": "Final post about Google Summer of Code 2021. This post sums up my\ncontributions to the Bambi library during the ten weeks of this program.",
    "author": [],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nIn this short blogpost, I’m going to summarize my contributions to the Bambi library during this Google Summer of Code.\nGSoC has been great. I’ve learnt so much during the past weeks. And I’m obviously eager to keep learning and doing stuff with Bambi in the future. The following is a summary of what we were able to achieve during these time of code:\nImplemented new default priors #360, #385.\nAdded new Student-T family #367.\nAdded new Beta family #368.\nImplemented predictions #372.\nImproved internal model specification by splitting it into smaller and composable classes #366.\nAdded the new Binomial family #386. This also implied some changes in its sibling project, formulae.\nThis, with many other smaller changes or improvements that you can find here were included in Bambi 0.6.0.\nOn the other hand, the items on my original proposal that left to do are multinomial regression and ordered categorical terms. I’ve started to do some work on the formulae side, but these features require a more involved work in Bambi, and thus it is left for future contributions.\nTo conclude, I want to thank Google for having such an amazing program and everyone who contributed or helped me to contribute to Bambi. Specially, I want to recognize the the work of my mentors Ravin Kumar and Thomas Wiecki, and my director Osvaldo Martin for all the support, feedback, and work during this program.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-16T11:34:08-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-binomial-family-in-Bambi/",
    "title": "Binomial family in Bambi",
    "description": "My fourth post describing work done during GSoC 2021. On this \noccasion, I'm introducing the Binomial family. This new family is very useful \nto build models for binary data when each row in the data set contains the \nnumber of successes and the number of trials instead of the results of \nBernoulli trials.",
    "author": [],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\n\n\n\nIntroduction\nAlthough GSoC 2021 is close to come to an end, there’s still a lot of exciting things going on around Bambi. Today I’m going to talk about another new family that’s about to be merged into the main branch, the Binomial family.\nLet’s get started by trying to see why we need to have another new family for modeling binary data in Bambi.\nAggregated vs disaggregated data\nBambi already has the Bernoulli family to model binary data. This family fits very well when you have a data set where each row represents a single observation and there’s a column that represents the binary outcome ( i.e the result of the Bernoulli trial) as well as other columns with the predictor variables.\nLet’s say we want to study the lethality of a certain drug and we have a group of mice to experiment with. An approach could be to divide the mice into smaller groups, assign a certain dose to all the mice in each group, and then finally count the number of units that died after a fixed amount of time. Under the Bernoulli family paradigm, each row has to represent a single observation, looking like this:\nObs\nDose\nDied\n1\n1.3\n0\n2\n1.8\n1\n3\n2.2\n1\n\nwhere each row represents a single mouse (i.e. a single Bernoulli trial). The 0 is used to represent a failure/survival, and 1 is used to represent a successes/death.\nWhat if our data is aggregated? The nature of the experiment makes it natural to have rows representing groups, a column representing the number of deaths, and another column representing the number of mice in the group.\nGroup\nDose\nDead\nTotal\n1\n1.3\n12\n20\n2\n1.8\n18\n25\n3\n2.2\n24\n34\n\nwhere each row represents a group of mice. Dose is the dose applied to all the units in the group, Dead is the number of mice that died, and Total is the number of mice in the group. If we focus on the Dead and Total columns we can easily see they resemble data coming from a Binomial distribution (i.e. number of successes out of a series of \\(n\\) independent Bernoulli trials). In other words, for a given row, we can think there’s a Binomial distribution where Dead represents the number of successes out of Total number of trials (each mouse is a trial).\nBefore continuing, it’s important to note that if the data is originally aggregated as in the lower table, it can always be disaggregated to resemble the one in the upper table. So what’s the problem?\nThe answer is that there’s actually nothing wrong with having the data in such a granular form! But, if the data already comes aggregated, why doing extra work when we now have the Binomial family? Let’s have a look at the examples below!\nSetup\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom bambi import Model, Prior\n\naz.style.use(\"arviz-darkgrid\")\n\nBLUE = \"#003f5c\"\nPURPLE = \"#7a5195\"\nPINK = \"#ef5675\"\n\nWe’re going to use real data in this example1. This data consists of the numbers of beetles dead after five hours of exposure to gaseous carbon disulphide at various concentrations:\nDose, \\(x_i\\) (\\(\\log_{10}\\text{CS}_2\\text{mgl}^{-1}\\))\nNumber of beetles, \\(n_i\\)\nNumber killed, \\(y_i\\)\n1.6907\n59\n6\n1.7242\n60\n13\n1.7552\n62\n18\n1.7842\n56\n28\n1.8113\n63\n52\n1.8369\n59\n53\n1.8610\n62\n61\n1.8839\n60\n60\n And now let’s write it down into a data frame:\n\nx = np.array([1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839])\nn = np.array([59, 60, 62, 56, 63, 59, 62, 60])\ny = np.array([6, 13, 18, 28, 52, 53, 61, 60])\n\ndata = pd.DataFrame({\n    \"x\": x,\n    \"y\": y,\n    \"n\": n\n})\n\nQuite simple, right? Can we use it as it is with the Bernoulli family? Let’s have a look below.\nBernoulli family\nNope, no surprises today. To use the Bernoulli family, we first need to transform the data into the dissagregated or long format. One approach is the following\n\ndata_bernoulli = pd.DataFrame({\n    \"x\": np.concatenate([np.repeat(x, n) for x, n in zip(x, n)]),\n    \"killed\": np.concatenate([np.repeat([1, 0], [y, n - y]) for y, n in zip(y, n)])\n})\n\nDo you realize how bothering it can be to do that if we have many more variables? Nevermind, let’s keep going.\nNow let’s initialize a Bambi model and sample from the posterior:\n\nmodel_brn = Model(\"killed ~ x\", data_bernoulli, family=\"bernoulli\")\nidata_brn = model_brn.fit()\n█\nModeling the probability that killed==1\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [x, Intercept]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\n\nand explore the marginal posteriors\n\naz.summary(idata_brn, kind=\"stats\")\n             mean     sd  hdi_3%  hdi_97%\nIntercept -60.871  5.518 -71.780  -51.223\nx          34.357  3.101  28.888   40.512\n\nWe can predict the the probability of dying for out-of-sample data to see how it evolves with the different concentration levels.\n\nnew_data = pd.DataFrame({\"x\": np.linspace(1.6, 2, num=200)})\nmodel_brn.predict(idata_brn, data=new_data)\n\nLet’s visualize the mean of the probability of dying as well as the HDI for the posterior of this probability:\n\nfig, ax = plt.subplots()\n\n# Plot HDI for the mean of the probability of dying\naz.plot_hdi(\n  new_data[\"x\"], \n  idata_brn.posterior[\"killed_mean\"].values, \n  color=BLUE,\n  ax=ax\n);\n\nax.plot(\n  new_data[\"x\"], \n  idata_brn.posterior[\"killed_mean\"].values.mean((0, 1)), \n  color=BLUE\n);\n\nax.scatter(x, y / n, s=50, color=PURPLE, edgecolors=\"black\", zorder=10);\nax.set_ylabel(\"Probability of death\");\nax.set_xlabel(r\"Dose $\\log_{10}CS_2mgl^{-1}$\");\nax.set_title(\"family='bernoulli'\");\nplt.show()\n\n\nBinomial family\nBefore writing down the model with the Binomial family, let’s take a moment to review new notation that was added specifically for this purpose.\nThe model formula syntax only allows us to pass one variable on its LHS. Then, how do we tell Bambi that what we want to model is the proportion that results from dividing y over n?\nThanks to recent developments, it’s as easy as writing proportion(y, n), or any of its aliases prop(y, n) and p(y, n). To keep it shorter, let’s use the last one.\n\nmodel_bnml = Model(\"p(y, n) ~ x\", data, family=\"binomial\")\nidata_bnml = model_bnml.fit()\n█\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [x, Intercept]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 2 seconds.\n\nQuite simple, right? The code here is very similar to the one for the model with the Bernoulli family. However, the new Binomial family allows us to use the data in its original form.\nLet’s finish this section by getting the marginal posteriors as well as a figure as the one displayed above.\n\naz.summary(idata_bnml, kind=\"stats\")\n             mean    sd  hdi_3%  hdi_97%\nIntercept -61.204  5.38 -71.738  -52.100\nx          34.545  3.03  29.461   40.487\n\n\nmodel_bnml.predict(idata_bnml, data=new_data)\n\n\nfig, ax = plt.subplots()\n\naz.plot_hdi(\n  new_data[\"x\"],\n  idata_bnml.posterior[\"p(y, n)_mean\"].values,\n  color=BLUE,\n  ax=ax\n);\n\nax.plot(\n  new_data[\"x\"], \n  idata_bnml.posterior[\"p(y, n)_mean\"].values.mean((0, 1)), \n  color=BLUE\n);\n\nax.scatter(x, y / n, s=50, color=PURPLE, edgecolors=\"black\", zorder=10);\nax.set_ylabel(\"Probability of death\");\nax.set_xlabel(r\"Dose $\\log_{10}CS_2mgl^{-1}$\");\nax.set_title(\"family='binomial'\");\nplt.show()\n\n\nConclusions\nThis blog post introduced the new Binomial family. This new family saves us from having to manipulate aggregated data prior to modeling, making it more pleasant and simpler to specify and fit models for binary data in Bambi.\n\nThis data can be found in An Introduction to Generalized Linear Models by A. J. Dobson and A. G. Barnett, but the original source is (Bliss, 1935).↩︎\n",
    "preview": "posts/2021-08-03-binomial-family-in-Bambi/binomial-family-in-Bambi_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-01-16T11:30:24-03:00",
    "input_file": {},
    "preview_width": 1382,
    "preview_height": 921
  },
  {
    "path": "posts/2021-07-14-new-families-in-Bambi/",
    "title": "New families in Bambi",
    "description": "In this third post about my work during this Google Summer of Code\nI describe two families of models recently added. The first one, is the \nStudent T family, used to make linear regressions more robust. The second, \nis the Beta family which can be used to model ratings and proportions.",
    "author": [],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\n\n\n\nIntroduction\nI’m very happy I could contribute with many exciting changes to Bambi. Some changes, such as the reorganization of the default priors and built-in families, are not visible to the user but make the codebase more modular and easier to read. Other changes, such as the ones I’m going to describe here, have a direct impact on what you can do with Bambi.\nToday I’ll describe two new built-in families that have been added to Bambi. The first one, already described in my previous post, is the \"t\" family. This can be used to make linear regressions more robust to outliers. The second one the \"beta\" family which can be used to model ratings and proportions.\nSetup\n\nimport arviz as az\nimport numpy as np\nimport pandas as pd\n\nfrom bambi import Model, Prior\n\nRobust linear regression with the t family.\nA Bayesian robust linear regression looks as follows\n\\[\ny_i \\sim \\text{StudentT}(\\mu_i, \\lambda, \\nu)\n\\]\nwhere \\(\\mu_i = \\beta_0 + \\beta_1 x_{1, i} + \\cdots + \\beta_p x_{p, i}\\), \\(\\lambda\\) is the precision parameter and \\(\\nu\\) is the degrees of freedom.\nThis wouldn’t be a Bayesian model without priors. Bambi uses the following priors by default:\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal}(\\mu_{\\beta_0}, \\sigma_{\\beta_0}) \\\\\n\\beta_j \\sim \\text{Normal}(\\mu_{\\beta_j}, \\sigma_{\\beta_j})  \\\\\n\\lambda \\sim \\text{HalfCauchy(1)}\n\\end{array}\n\\]\nwhere the \\(\\mu_{\\beta_j}\\) and \\(\\sigma_{\\beta_j}\\) are estimated from the data. By default, \\(\\nu=2\\), but it is also possible to assign it a probability distribution (as we’re going to see below).\nBefore seeing how this new family works, let’s simulate some data. On this opportunity, we’re using the same dataset than in the previous post. This is a toy dataset with one predictor x, one response y, and some outliers contaminating the beautiful linear relationship between the variables.\n\nsize = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ny = true_intercept + true_slope * x + np.random.normal(scale=0.5, size=size)\n\nx_out = np.append(x, [0.1, 0.15, 0.2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = pd.DataFrame(dict(x = x_out, y = y_out))\n\n\n\n\nModel specification and fit\nUsing this new family is extremely easy. It is almost as simple as running a default normal linear regression. The only difference is that we need to add the family=\"t\" argument to the Model() instantiation.\n\nmodel = Model(\"y ~ x\", data, family=\"t\")\nmodel\nFormula: y ~ x\nFamily name: T\nLink: identity\nObservations: 103\nPriors:\n  Common-level effects\n    Intercept ~ Normal(mu: 2.1636, sigma: 5.9343)\n    x ~ Normal(mu: 0.0, sigma: 10.3941)\n\n  Auxiliary parameters\n    sigma ~ HalfStudentT(nu: 4, sigma: 1.2196)\n    nu ~ Gamma(alpha: 2, beta: 0.1)\n\nThe output above shows information about the family being used and the parameters for the default priors. Next, we just do model.fit() to run the sampler.\n\nidata = model.fit()\n\nUse custom priors\nLet’s say we are not happy with having a fixed value for the degrees of freedom and we want to assign it a prior distribution. Is that a problem? Of course not!\n\n# Use a Gamma prior for the degrees of freedom\nmodel = Model(\"y ~ x\", data, family=\"t\")\nmodel.set_priors({\"nu\": Prior(\"Gamma\", alpha=3, beta=1)})\nmodel\nFormula: y ~ x\nFamily name: T\nLink: identity\nObservations: 103\nPriors:\n  Common-level effects\n    Intercept ~ Normal(mu: 2.1636, sigma: 5.9343)\n    x ~ Normal(mu: 0.0, sigma: 10.3941)\n\n  Auxiliary parameters\n    sigma ~ HalfStudentT(nu: 4, sigma: 1.2196)\n    nu ~ Gamma(alpha: 3, beta: 1)\n\nAnd hit the inference button\n\nidata = model.fit()\n█\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_nu, y_sigma, x, Intercept]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds.\n\nExplore results\nFirst of all we can see the marginal posteriors for the parameters in the model and their respective traces\n\n\n\n\n\n\nAnd it is also good to explore the posterior distribution of regression lines\n\n\n\nwhere the line in black is the true regression line.\nBeta regression with the beta family.\nBeta regression is useful to model response variables that have values within the \\((0, 1)\\) interval. This type of regression is based on the assumption that the conditional distribution of the response variable follows a Beta distribution with its mean related to a set of regressors through a linear predictor with unknown coefficients and a link function.\nThe beta regression model is based on an alternative parameterization of the beta density in terms of the mean \\(\\mu\\) and a precision parameter \\(\\kappa\\).\n\\[\n\\begin{array}{lr}\n\\displaystyle f(y | \\mu, \\kappa) = \n  \\frac{\\Gamma(\\kappa)}{\\Gamma(\\mu\\kappa)\\Gamma((1-\\mu)\\kappa)} \n  y^{\\mu\\kappa -1}\n  y^{(1 - \\mu)\\kappa -1}, & 0 < y < 1\n\\end{array}\n\\]\nwith \\(0 < \\mu < 1\\) and \\(\\kappa > 0\\).\nIf we use the same notation than for the robust linear regression, the beta regression model is defined as\n\\[\ny_i \\sim \\text{Beta}(g^{-1}(\\mu_i), \\kappa)\n\\]\nwhere \\(\\mu_i = \\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_p x_{p,i}\\), \\(\\kappa\\) is the precision parameter and \\(g\\) is a twice differentiable, strictly increasing, link function.\nBambi uses again the following priors by default:\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal}(\\mu_{\\beta_0}, \\sigma_{\\beta_0}) \\\\\n\\beta_j \\sim \\text{Normal}(\\mu_{\\beta_j}, \\sigma_{\\beta_j})  \\\\\n\\kappa \\sim \\text{HalfCauchy(1)}\n\\end{array}\n\\]\nwhere the \\(\\mu_{\\beta_j}\\) and \\(\\sigma_{\\beta_j}\\) are estimated from the data. By default, \\(g\\) is the logit function. Other options available are the identity, the probit, and the cloglog link functions.\nIt’s possible to resume all of this in a very simplistic way by seeing that the beta regression as a very close relative of the GLM family. This model presents all the characteristics of GLMs, with the exception that the beta distribution doesn’t belong to the exponential family.\nModel specification and fit\nHere we are going to use the GasolineYield dataset from the betareg R package. This dataset is about the proportion of crude oil converted to gasoline. The response variable is the proportion of crude oil after distillation and fractionation. In this example, we use the temperature at which gasoline has vaporized in Fahrenheit degrees (\"temp\") and a factor that indicates ten unique combinations of gravity, pressure and temperature (\"batch\").\nThe following is just a re-ordering of the categories in the \"batch\" variable so it matches the original contrasts used in the betareg package.\n\ndata = r.GasolineYield\ndata[\"batch\"] = pd.Categorical(\n  data[\"batch\"], \n  [\"10\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"], \n  ordered=True\n)\n\nNext, we define the model. The only difference is that we indicate family=\"beta\". Bambi handles all the rest for us.\n\n# Note this model does not include an intercept\nmodel = Model(\"yield ~ 0 + temp + batch\", data, family=\"beta\")\nmodel\nFormula: yield ~ 0 + temp + batch\nFamily name: Beta\nLink: logit\nObservations: 32\nPriors:\n  Common-level effects\n    temp ~ Normal(mu: 0.0, sigma: 0.0364)\n    batch ~ Normal(mu: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], sigma: [ 8.5769  7.5593  8.5769  8.5769  7.5593  8.5769  8.5769  7.5593  8.5769\n 10.328 ])\n\n  Auxiliary parameters\n    kappa ~ HalfCauchy(beta: 1)\n\nAnd model.fit() is all we need to ask the sampler to start running.\n\nidata = model.fit(target_accept=0.9)\n█\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [yield_kappa, batch, temp]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 12 seconds.\nThe number of effective samples is smaller than 25% for some parameters.\n\nExplore results\nOnce we got the posterior, we explore it. This time we’re going to plot highest density intervals for the marginal posteriors corresponding to the parameters in the model.\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-07-14-new-families-in-Bambi/new-families-in-Bambi_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-01-16T11:24:32-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-07-05-robust-linear-regression-with-Bambi/",
    "title": "Robust linear regression in Bambi",
    "description": "Second post about this Google Summer of Code season. Today I show\nsome of the problems associated with outliers in linear regression and demonstrate\nhow one can implement a robust linear regression in Bambi.",
    "author": [],
    "date": "2021-07-05",
    "categories": [],
    "contents": "\n\n\n\nThe next thing in my TODO list for this Google Summer of Code season with NumFOCUS is to add new families of models to Bambi. This is still a WIP but I wanted to show you how to build a robust linear regression model using the Family class in Bambi.\nSetup\n\n\nlibrary(ggplot2)\nlibrary(reticulate)\nuse_condaenv(\"bmb\", required = TRUE)\n\nBLUE = \"#003f5c\"\nPURPLE = \"#7a5195\"\nPINK = \"#ef5675\"\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\nfrom bambi import Model, Family, Likelihood, Prior\nfrom scipy import stats\n\nWhat do we mean with robust?\nBefore showing how to build a robust regression with Bambi we need to be clear about what we mean when we say that a model is robust. Robust to what? How is linear regression non-robust?\nIn this post, we say a method is robust if its inferences aren’t (seriously) affected by the presence of outliers.\nHow do outliers affect linear regression?\nI think it will be easier to understand how outliers affect linear regressions via an example based on the least squares method. This is not exactly how linear regression works in our Bayesian world, but outlier’s bad consequences are similar.\nIn classic statistics, linear regression models are usually fitted by ordinary least-squares method. This is equivalent to assuming the conditional distribution of the response given the predictors is normal (i.e. \\(y_i|\\boldsymbol{X}_i \\sim N(\\mu_i, \\sigma)\\)) and using the maximum likelihood estimator.\nLet’s get started by simulating some toy data.\n\nnp.random.seed(1234)\n\nx = np.array([1., 2., 4., 5.])\ny = x + np.random.normal(scale=0.5, size=4)\n\nThen, fit a linear regression between and visualize the result.\nThe next plot shows the data, the fitted line, and the contribution of each data point to the total (squared) error as a blue square (one way to see the least squares method is as the method that minimizes the sum of the areas of the squares associated to all the points).\n\n\n\nSo far so good! It looks like the fitted line is a good representation of the relationship between the variables.\nWhat happens if we introduce an outlier? In other words, what happens if there’s a new point that deviates too much from the pattern we’ve just seen above? Let’s see it!\n\nx = np.insert(x, 2, 2.25)\ny = np.insert(y, 2, 5.8)\n\n\n\n\nWhat a bummer! Why do we have such a huge error? It’s almost 10 times the previous error with only one extra data point! Why?!\nIt happens that each point’s contribution to the error grows quadratically as it moves away from the rest. Outliers not only contribute a lot to the total error, they also bias the estimation towards themselves, increasing the error associated with other points too. The final result? the fitted line is not a faithful representation of the relationship between the variables.\nLinear regression in a Bayesian way\nNow that we’ve seen how bad outliers can be above, let’s see how one can robust a Bayesian linear regression. This part of the post is based on the Robust Linear Regression in PyMC3 docs.\nHere, we simulate data suitable for a normal linear regression and contaminate it with a few outliers.\n\nsize = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\ntrue_regression_line = true_intercept + true_slope * x\ny = true_regression_line + np.random.normal(scale=0.5, size=size)\n\nx_out = np.append(x, [0.1, 0.15, 0.2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = pd.DataFrame(dict(x = x_out, y = y_out))\n\nNormal linear regression\nThe normal linear regression is as follows\n\\[\ny_i \\sim \\text{Normal}(\\mu_i, \\sigma)\n\\]\nwhere \\(\\mu_i = \\beta_0 + \\beta_1 x_i\\), and the priors are of the form\n\\[\n\\begin{array}{c}\n\\beta_0 \\sim \\text{Normal} \\\\\n\\beta_1 \\sim \\text{Normal}  \\\\\n\\sigma \\sim \\text{HalfStudentT}\n\\end{array}\n\\]\nwith their parameters automatically set by Bambi.\n\nmodel = Model(\"y ~ x\", data=data)\nidata = model.fit()\n█\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_sigma, x, Intercept]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 2 seconds.\n\n\n\n\nTo evaluate the fit, we use the posterior predictive regression lines. The line in black is the true regression line.\n\n\n\nAs you can see, the posterior distribution fo the regression lines is not centered around the true regression line, which means the estimations are highly biased. This is the same phenomena we saw above with the least-squares toy example.\nWhy does it happen here? The reason is that the normal distribution does not have a lot of mass in the tails and consequently, an outlier will affect the fit strongly.\nSince the problem is the light tails of the Normal distribution we can instead assume that our data is not normally distributed but instead distributed according to the Student T distribution which has heavier tails as shown next.\nNormal and Student-T distributions\nHere we plot the pdf of a standard normal distribution and the pdf of a student-t distribution with 3 degrees of freedom.\n\nx = np.linspace(-8, 8, num=200)\ny_normal = stats.norm.pdf(x)\ny_t = stats.t.pdf(x, df = 3)\n\n\n\n\nAs you can see, the probability of values far away from the mean are much more likely under the Student-T distribution than under the Normal distribution.\nRobust linear regression\nThe difference with the model above is that this one uses a StudentT likelihood instead of a Normal one.\nBambi does not support yet to use the student-t distribution as the likelihood function for linear regression. However, we can construct our own custom family and Bambi will understand how to work with it.\nCustom families are represented by the Family class in Bambi. Let’s see what we need to create a custom family.\nFirst of all, we need a name. In this case the name is going to be just \"t\". Second, there is the likelihood function. This is represented by an object of class Likelihood in Bambi. To define a likelihood function we need the following:\nThe name of the distribution in PyMC3. In this case, it is \"StudentT\".\nThe name of the parent parameter (the mean). It is \"mu\".\nThe prior distributions for the auxiliary parameters in the distribution. These are nu and sigma in the StudentT distribution.\nFinally, we pass the link function. This can be a string or an object of class Link. In this case it’s simply the identity function, which can be passed as a string.\n\n# Construct likelihood\nnu = Prior(\"Gamma\", alpha=2, beta=0.1)\nsigma = Prior(\"HalfNormal\", sigma=1)\nlikelihood = Likelihood(name=\"StudentT\", parent=\"mu\", sigma=sigma, nu=nu)\n\n# Construct family\nt_family = Family(name = \"t\", likelihood = likelihood, link = \"identity\")\n\n# In addition, we pass our custom priors for the terms in the model.\npriors = {\n  \"Intercept\": Prior(\"Normal\", mu=0, sigma=10),\n  \"x\": Prior(\"Normal\", mu=0, sigma=10)\n}\n\n# Just add the `prior` and `family` arguments\nmodel = Model(\"y ~ x\", data, priors=priors, family=t_family)\nidata = model.fit()\n█\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [y_nu, y_sigma, x, Intercept]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\n\n\n\n\n\n\n\nMuch better now! The posterior distribution of the regression lines is almost centered around the true regression line, and uncertainty has decreased, that’s great! The outliers are barely influencing our estimation because our likelihood function assumes that outliers are much more probable than under the Normal distribution.\n\n\n\n",
    "preview": "posts/2021-07-05-robust-linear-regression-with-Bambi/2021-07-05-robust-linear-regression-with-Bambi_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-01-16T11:16:58-03:00",
    "input_file": {},
    "preview_width": 1299,
    "preview_height": 1299
  },
  {
    "path": "posts/2021-06-28-first-weeks-of-gsoc/",
    "title": "First weeks of GSoC",
    "description": "First post of a series about my contributions to Bambi\nin this Google Summer of Code season. This post highlights new features related\nto default priors and priors for group-specific effects.",
    "author": [],
    "date": "2021-06-28",
    "categories": [],
    "contents": "\n\n\n\nI am really happy to participate in this Google Summer of Code season with NumFOCUS to contribute to the Bambi library. The coding period ranges from June 7 to August 16, with an intermediate evaluation taking place between July 12 and July 16.\nOverview\nMy project is called Extend available models and default priors in Bambi. The main goal of this project is to add new families of generalized linear models, such as beta regression, robust linear regression (i.e. linear model with error following a T-Student distribution)1 as well as multinomial regression. However, this raises a second problem, which is about default priors distributions.\nDefault priors in Bambi are limited to the families implemented in the GLM module in statsmodels, which does not include the families mentioned above. For this reason, it is first necessary to incorporate alternative automatic priors so new families work without requiring the user to manually specify priors.\nTherefore, these first weeks of the coding period were centered around understanding how default priors work on other high-level modeling packages such as brms and rstanarm, how to translate their ideas into PyMC3 code, and finally how to implement everything within Bambi.\nAlternative default priors\nCurrently, Bambi uses maximum likelihood estimates in the construction of its default priors. There are two limitations associated with this approach. First, current default priors don’t exist whenever uniquely identifiable maximum likelihood estimates don’t exist (e.g. \\(p > n\\) or complete separation scenarios). Secondly, these estimates are obtained via the GLM module in statsmodels, which means default priors can only be obtained for families made available in statsmodels.\nBased on the available documentation and simulations I’ve done, I decided to implement alternative default priors that are much like the default priors in rstanarm. These priors aim to be weakly-informative in most scenarios and do not depend on maximum likelihood estimates. Their documentation is excellent and it was a great guide for my implementation.\nThis is the PR where I implement alternative default priors inspired on rstanarm default priors. In addition, I also implement LKJ prior for the correlation matrices of group-specific effects.\nHow to invoke alternative default priors\nThe Model() class has gained one new argument, automatic_priors, that can be equal to \"default\" to use Bambi’s default method, or \"rstanarm\" to use the alternative implementation2.\nmodel = Model(\"y ~ x + z\", data, automatic_priors=\"rstanarm\")\nHow to use LKJ priors for correlation matrices of group-specific effects\nGroup-specific effects can now have non-independent priors. Instead of using independent normal distributions, we can use a multivariate normal distribution whose correlation matrix has an LKJ prior distribution. This distribution depends on a parameter \\(\\eta > 0\\). If \\(\\eta=1\\), the LJK prior is jointly uniform over all correlation matrices of the same dimension. If \\(\\eta >1\\) increases, the mode of the distribution is the identity matrix. The larger the value of \\(\\eta\\) the more sharply peaked the density is at the identity matrix.\nModel has an argument priors_cor where we can pass a dictionary to indicate which groups are going to have a LKJ prior. The keys of the dictionary are the names of the groups, and the values are the values for \\(\\eta\\).\nIn the following model, we have a varying intercept and varying slope for the groups given by group. These varying effects have a multivariate normal prior whose covariance matrix depends on a correlation matrix that has a LKJ hyperprior with \\(\\eta=1\\).\nmodel = Model(\"y ~ x + (x|group)\", data, priors_cor={\"group\": 1})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n These two distributions are not members of the exponential family so using them as the distribution of the random component does not result in a generalized linear model in a strict sense. But I would usually refer to them as GLMs since the linear predictor, link function, and random component properties are still present.↩︎\nBoth the argument name and the options may change↩︎\n",
    "preview": {},
    "last_modified": "2022-01-15T20:31:46-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-08-group-specific-effects-matrix/",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "description": "Bambi uses the library formulae to automatically construct design \nmatrices for both common and group-specific effects. This post compares design \nmatrices for group-specific effects obtained with formulae for a variety of \nscenarios involving categorical variables with the ones obtained with the R \npackage lme4.",
    "author": [],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\nIntroduction\nA linear mixed model can be written as\n\\[\n\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \n                 \\boldsymbol{Z}\\boldsymbol{u} + \\boldsymbol{\\epsilon}\n\\]\nwhere \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) are the two design matrices we need to somehow construct when dealing with this type of model. \\(\\boldsymbol{X}\\) is the design matrix for the common (a.k.a. fixed) effects, and \\(\\boldsymbol{Z}\\) is the design matrix for the group-specific (a.k.a. random or varying) effects.\nIt is quite easy to obtain the design matrix \\(\\boldsymbol{X}\\) in R using its popular formula interface. In Python, patsy provides equivalent functionality. Unfortunately, there aren’t as many alternatives to compute the matrix \\(\\boldsymbol{Z}\\).\nIn R, there’s lme4, the statistical package par excellence for mixed models. It extends the base formula interface to include group-specific effects via the pipe operator (|) and internally computes both \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) without the user noticing. That’s great!\nIn Python, we are working on formulae, a library we use to handle mixed model formulas in Bambi. In this process, I’ve found Fitting Linear Mixed-Effects Models Using lme4 vignette extremely useful when figuring out how to compute the design matrix for the group-specific effects.\nToday, I was adding tests to make sure we are constructing \\(\\boldsymbol{Z}\\) appropriately and found myself comparing the matrices obtained with formulae with matrices obtained with lme4. Then I was like … why not making this a blog post? 🤔\n… and so here we are! But before we get started, just note this post mixes both R and Python code. I will try to be explicit when I’m using one language or the other. But if you’re reading a chunk and it looks like Python, it’s Python. And if it looks like R… you guessed! It’s R.\nSetup\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lme4)\nlibrary(patchwork)\n\n\n\n\n\n\n\nfrom formulae import design_matrices\n\n\n\n\nProblem\nHere we will be comparing design matrices for the group-specific terms in a mixed-effects model obtained with both lme4 and formulae. We’re using the dataset Pixel that comes with the R package nlme.\n\n\ndata(\"Pixel\", package = \"nlme\")\nhead(Pixel)\n\n\nGrouped Data: pixel ~ day | Dog/Side\n  Dog Side day  pixel\n1   1    R   0 1045.8\n2   1    R   1 1044.5\n3   1    R   2 1042.9\n4   1    R   4 1050.4\n5   1    R   6 1045.2\n6   1    R  10 1038.9\n\nWe’re not interested in how to fit a certain model here. We’re interested in constructing the design matrix for group-specific effects with different characteristics. We use the following formula\n\n\nf1 = ~ (0 + day | Dog) + (1 | Side / Dog)\n\n\n\nwhere each part can be interpreted as follows\n(0 + day | Dog) means that day has a group-specific slope for each Dog. This is usually known as a random slope. The 0 indicates not to add the default group-specific intercept (because it’s added next).\n(1 | Side / Dog) is equivalent to (1 | Side) + (1 | Dog:Side). This means there’s a varying intercept for each Side and a varying intercept for each combination of Dog and Side. In other words, we have a nested group-specific intercept, where Dog is nested within Side.\n\n\nlme4_terms = mkReTrms(findbars(f1), model.frame(subbars(f1), data = Pixel))\n\n\n\nlme4_terms contains much more information than what we need for this post. We mostly use lme4_terms$Ztlist, which is a list that contains the transpose of the group-specific effects model matrix, separated by term. These matrices are stored as sparse matrices of dgCMatrix class. If we want to have the sub-matrix for a given group-specific term as a base R matrix, we have to do as.matrix(t(lme4_terms$Ztlist$[[\"term\"]])).\n\n\nnames(lme4_terms$Ztlist)\n\n\n[1] \"1 | Dog:Side\"  \"0 + day | Dog\" \"1 | Side\"     \n\nWe have three group-specific terms. The first and the last ones are the group-specific intercepts we mentioned. These are the result of the nested group-specific intercept (1 | Side / Dog). Dog is nested within Side and consequently there’s an intercept varying among Side and another varying among Dog within Side. The second term, 0 + day | Dog, represents varying slope of day for each level of Dog.\nWe finally store the sub-matrix for each term in different objects that we’ll later use when comparing results with those obtained with formulae.\n\n\nday_by_dog = as.matrix(t(lme4_terms$Ztlist$`0 + day | Dog`))\nintercept_by_side = as.matrix(t(lme4_terms$Ztlist$`1 | Side`))\nintercept_by_side_dog = as.matrix(t(lme4_terms$Ztlist$`1 | Dog:Side`))\n\n\n\nOn the other hand, in Python, we use design_matrices() from the formulae library to obtain a DesignMatrices object. All the information associated with the group-specific terms is contained in the .group attribute and the sub-matrix corresponding to a particular term is accessed with .group[term_name].\n\ndm = design_matrices(\"(0 + day | Dog) + (1 | Side / Dog)\", r.Pixel)\n\nThere’s a dictionary called terms_info within dm.group. To see the names of the group-specific effects we just retrieve the keys.\n\ndm.group.terms_info.keys()\ndict_keys(['day|Dog', '1|Side', '1|Side:Dog'])\n\nNames differ a little with the ones from lme4, but they represent the same thing.\n\nday_by_dog = dm.group['day|Dog']\nintercept_by_side = dm.group['1|Side']\nintercept_by_side_dog = dm.group['1|Side:Dog']\n\nNow let’s compare those matrices!\nDesign matrices for (day|Dog)\nRectangles in the following plot correspond to the cells in the matrix. The lowest value for day is 0, represented by violet, and the highest value is 21, represented by yellow. The 10 columns represent the 10 groups in Dog, and the rows represent the observations in Pixel. Here, and also in the other cases, the left panel contains the matrix obtained with lme4 and the right panel the one produced with formulae.\n\n\n\nIn this first case, both panels are representing the same data so we can happily conclude the result obtained with formulae matches the one from lme4. Yay!!\nBut we’re humans and our eyes can fail so it’s better to always check appropiately with\n\n\nall(py$day_by_dog == day_by_dog)\n\n\n[1] TRUE\n\nDesign matrices for (1|Side)\nHere the first column represents Side == \"L\" and the second column represents Side == \"R\". Since we’re dealing with an intercept, violet means 0 and yellow means 1. In this case it is much easier to see both results match.\n\n\n\n\n\nall(py$intercept_by_side == intercept_by_side)\n\n\n[1] TRUE\n\nDesign matrices for (1|Side:Dog)\nBut things are not always as one wishes. It’s clear from the following plot that both matrices aren’t equal here.\n\n\n\nBut don’t worry. We’re not giving up. We still have things to do1. We can check what are the groups being represented in the columns of the matrices we’re plotting.\n\n\ncolnames(intercept_by_side_dog)\n\n\n [1] \"1:L\"  \"1:R\"  \"10:L\" \"10:R\" \"2:L\"  \"2:R\"  \"3:L\"  \"3:R\"  \"4:L\" \n[10] \"4:R\"  \"5:L\"  \"5:R\"  \"6:L\"  \"6:R\"  \"7:L\"  \"7:R\"  \"8:L\"  \"8:R\" \n[19] \"9:L\"  \"9:R\" \n\n\ndm.group.terms_info[\"1|Side:Dog\"][\"full_names\"]\n['1|Side:Dog[L:1]', '1|Side:Dog[L:10]', '1|Side:Dog[L:2]', '1|Side:Dog[L:3]', '1|Side:Dog[L:4]', '1|Side:Dog[L:5]', '1|Side:Dog[L:6]', '1|Side:Dog[L:7]', '1|Side:Dog[L:8]', '1|Side:Dog[L:9]', '1|Side:Dog[R:1]', '1|Side:Dog[R:10]', '1|Side:Dog[R:2]', '1|Side:Dog[R:3]', '1|Side:Dog[R:4]', '1|Side:Dog[R:5]', '1|Side:Dog[R:6]', '1|Side:Dog[R:7]', '1|Side:Dog[R:8]', '1|Side:Dog[R:9]']\n\nAnd there it is! Matrices differ because columns are representing different groups. In lme4, groups are looping first along Dog and then along Side, while in formulae it is the other way around.\nWe can simply re-order the columns of one of the matrices and generate and check whether they match or not.\n\n\nintercept_by_side_dog_f = as.data.frame(py$intercept_by_side_dog)\ncolnames(intercept_by_side_dog_f) = py$dm$group$terms_info[[\"1|Side:Dog\"]]$groups\nnames_lme4_order = paste(\n  rep(c(\"L\", \"R\"), 10), \n  rep(c(1, 10, 2, 3, 4, 5, 6, 7, 8, 9), each = 2), \n  sep = \":\"\n)\n\nintercept_by_side_dog_f = intercept_by_side_dog_f[names_lme4_order] %>%\n  as.matrix() %>%\n  unname()\n\n\n\n\n\n\n\n\nall(intercept_by_side_dog_f == intercept_by_side_dog)\n\n\n[1] TRUE\n\nAnd there it is! Results match 🤩\nAnother formula\nThis other formula contains an interaction between categorical variables as the expression of the group-specific term, which is something we’re not covering above. In this case, we are going to subset the data so the design matrices are smaller and we can understand what’s going on with more ease.\n\n\n# Subset data\nPixel2 = Pixel %>%\n  filter(Dog %in% c(1, 2, 3), day %in% c(2, 4, 6)) %>%\n  mutate(Dog = forcats::fct_drop(Dog))\n\n# Create terms with lme4\nf2 = ~ day +  (0 + Dog:Side | day)\nlme4_terms = mkReTrms(findbars(f2), model.frame(subbars(f2), data = Pixel2))\ndog_and_side_by_day = as.matrix(t(lme4_terms$Ztlist$`0 + Dog:Side | day`))\n\n\n\nAnd now with design_matrices() in Python.\n\n# Create terms with \ndm = design_matrices(\"(0 + Dog:Side|day)\", r.Pixel2)\ndog_and_side_by_day = dm.group[\"Dog:Side|day\"]\n\nDesign matrix for (Dog:Side|day)\nAlthough this term is called slope, it is not actually a slope like the one for (day|Dog). Since both Dog and Side are categorical, the entries of this matrix consist of zeros and ones.\n\n\n\nWe have the same problem than above, matrices don’t match. So we know what to do: look at the groups represented in the columns.\n\n\ncolnames(dog_and_side_by_day)\n\n\n [1] \"2\" \"2\" \"2\" \"2\" \"2\" \"2\" \"4\" \"4\" \"4\" \"4\" \"4\" \"4\" \"6\" \"6\" \"6\" \"6\"\n[17] \"6\" \"6\"\n\n\ndm.group.terms_info[\"Dog:Side|day\"][\"full_names\"]\n['Dog[1]:Side[L]|2.0', 'Dog[1]:Side[R]|2.0', 'Dog[2]:Side[L]|2.0', 'Dog[2]:Side[R]|2.0', 'Dog[3]:Side[L]|2.0', 'Dog[3]:Side[R]|2.0', 'Dog[1]:Side[L]|4.0', 'Dog[1]:Side[R]|4.0', 'Dog[2]:Side[L]|4.0', 'Dog[2]:Side[R]|4.0', 'Dog[3]:Side[L]|4.0', 'Dog[3]:Side[R]|4.0', 'Dog[1]:Side[L]|6.0', 'Dog[1]:Side[R]|6.0', 'Dog[2]:Side[L]|6.0', 'Dog[2]:Side[R]|6.0', 'Dog[3]:Side[L]|6.0', 'Dog[3]:Side[R]|6.0']\n\nBut this they represent the same groups2. We can look if there’s a difference in how the interactions are ordered within each group.\n\n\nlme4_terms$cnms\n\n\n$day\n[1] \"Dog1:SideL\" \"Dog2:SideL\" \"Dog3:SideL\" \"Dog1:SideR\" \"Dog2:SideR\"\n[6] \"Dog3:SideR\"\n\nAnd again, thankfully, we see there’s a difference in how columns are being ordered. Let’s see if matrices match after we reorder the one obtained with formulae.\n\n\ndog_and_side_by_day_f = as.data.frame(py$dog_and_side_by_day)\ncolnames(dog_and_side_by_day_f) = py$dm$group$terms_info[[\"Dog:Side|day\"]]$full_names\nside = rep(rep(c(\"L\", \"R\"), each = 3), 3)\ndog = rep(1:3, 6)\nday = rep(c(\"2.0\", \"4.0\", \"6.0\"), each = 6)\n\nnames_lme4_order = glue::glue(\"Dog[{dog}]:Side[{side}]|{day}\")\ndog_and_side_by_day_f = dog_and_side_by_day_f[names_lme4_order] %>%\n  as.matrix() %>%\n  unname()\n\n\n\n\n\n\n\n\nall(dog_and_side_by_day_f == dog_and_side_by_day)\n\n\n[1] TRUE\n\nConclusion\nAlthough formulae works differently than lme4, and has different goals, we showed that formulae produces the same design matrices as lme4 for the variety of examples we covered. While case-based comparisons like these are not what one should rely on when writing software, the examples here were really helpful when working on the implementation in formulae and writing the corresponding tests. And if this post helps someone to better understand what’s going on when working with design matrices associated with group-specific effects, it will have been even more worth it!\n\nI was undoubtedly talking to myself was quite disappointed at this time, wondering what I did wrong. Suffering the consequences of mistakes I wasn’t even aware I made. Well, not that dramatic. But now I’m happy the problem wasn’t real 😅↩︎\nWe have six 2s, six 4s and six 6s in both cases↩︎\n",
    "preview": "posts/2021-06-08-group-specific-effects-matrix/group-specific-effects-matrix_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2022-01-15T20:28:41-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-24-bambi-wald/",
    "title": "Why Bambi?",
    "description": "An example comparing how to fit a GLM with Bambi and PyMC3. Here I \nattempt to highlight how Bambi can help us to write a Bayesian GLM in a concise \nmanner, saving us from having to realize error-prone tasks that are sometimes \nnecessary when directly working with PyMC3.",
    "author": [],
    "date": "2021-05-24",
    "categories": [],
    "contents": "\nIntroduction\nI’ve been thinking about writing a new blog post for a while now but honestly, there was nothing coming to my mind that made me think “Oh, yeah, this is interesting, it can be useful for someone else”. And it was just a few hours ago that I realized I could write about something quite curious that happened to me while trying to replicate a Bambi model with PyMC3.\nPyMC3 is a Python package for Bayesian statistical modeling that implements advanced Markov chain Monte Carlo algorithms, such as the No-U-Turn sampler (NUTS). Bambi is a high-level Bayesian model-building interface in Python. It is built on top of PyMC3 and allows users to specify and fit Generalized Linear Models (GLMs) and Generalized Linear Mixed Models (GLMMs) very easily using a model formula much similar to the popular model formulas in R.\nA couple of weeks ago Agustina Arroyuelo told me she was trying to replicate a model in one of the example notebooks we have in Bambi and wanted my opinion on what she was doing. After many attempts, neither of us could replicate the model successfully. It turned out to be we were messing up with the shapes of the priors and also had some troubles with the design matrix.\nThe point of this post is not about good practices when doing Bayesian modeling neither about modeling techniques. This post aims to show how Bambi can save you effort, code, and prevent us from making some mistakes when fitting not-so-trivial GLMs in Python.\nWell, I think this is quite enough for an introduction. Let’s better have a look at the problem at hand.\nSetup\n\n\n\n\nimport arviz as az\nimport bambi as bmb\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport theano.tensor as tt\n\nThe problem\nIn this problem we use a data set consisting of 67856 insurance policies and 4624 (6.8%) claims in Australia between 2004 and 2005. The original source of this dataset is the book Generalized Linear Models for Insurance Data by Piet de Jong and Gillian Z. Heller.\n\nurl = \"https://courses.ms.ut.ee/2020/glm/spring/uploads/Main/carclaims.csv\"\ndata = pd.read_csv(url)\ndata = data[data[\"claimcst0\"] > 0]\n\nThe age (binned), the gender, and the area of residence are used to predict the amount of the claim, conditional on the existence of the claim because we are only working with observations where there is a claim.\nWe use a Wald regression model. This is a GLM where the random component follows a Wald distribution. The link function we choose is the natural logarithm.\nPyMC3 model\nData preparation\nTo fit the model with PyMC3 we first need to create the model matrix. We need to represent age, area, and gender with dummy variables because they are categorical. We can think of the following objects as sub-matrices of the design matrix in the model.\n\nintercept = np.ones((len(data), 1))\nage = pd.get_dummies(data[\"agecat\"], drop_first=True).to_numpy()\narea = pd.get_dummies(data[\"area\"], drop_first=True).to_numpy()\ngender = pd.get_dummies(data[\"gender\"], drop_first=True).to_numpy()\n\nNote we have used drop_first=True. This means that we use n_levels - 1 dummies to represent each categorical variable, and the first level is taken as reference. This ensures the resulting design matrix is of full rank.\nNext, we stack these sub-matrices horizontally and convert the result to a Theano tensor variable so we can compute the dot product between this matrix and the vector of coefficients when writing our model in PyMC3.\n\nX = np.hstack([intercept, age, gender, area])\nX = tt.as_tensor_variable(X)\n\nFit\nWe start declaring the priors for each of the predictors in the model. They are all independent Gaussian distributions. You may wonder where I took the values for the parameters of these distributions. I’ve just copied Bambi’s default values for this particular problem.\nAt this stage, it is very important to give appropriate shapes to all the objects we create in the model. For example, β_age is a random variable that represents the coefficients for the age variable. Since 5 dummy variables are used to represent the age, both β_age and the values passed to mu and sigma must have shape=(5, 1). I’ve failed here many times when trying to replicate the model, so, unfortunately, I know what I’m talking about 😅\n\n# Create model and sample posterior\nwith pm.Model() as model_pymc3:  \n    # Build predictors\n    β_0 = pm.Normal(\n        \"β_0\",\n        mu=np.array([[7.61]]),\n        sigma=np.array([[2.73]]),\n        shape=(1, 1)\n    )\n    β_age = pm.Normal(\n        \"β_age\",\n        mu=np.array([[0] * 5]).T,\n        sigma=np.array([[0.32, 6.94, 1.13, 5.44, 9.01]]).T,\n        shape=(5, 1)\n    )\n    β_gender = pm.Normal(\n        \"β_gender\",\n        mu=np.array([[0]]),\n        sigma=np.array([[1.304491]]),\n        shape=(1, 1)\n    )\n    β_area = pm.Normal(\n      \"β_area\",\n      mu=np.array([[0] * 5]).T,\n      sigma=np.array([[0.86, 0.25, 1.3, 0.76, 5.33]]),\n      shape=(5, 1)\n    )\n    \n    # Concatenate the vectors for the coefficients into a single vector\n    β = tt.concatenate([β_0, β_age, β_gender, β_area], axis=0)\n    \n    # Compute and transform linear predictor\n    mu = tt.exp(X.dot(β))\n      \n    response = np.array([data[\"claimcst0\"]]).T\n    pm.Wald(\n      \"claim\", \n      mu=mu, \n      lam=pm.HalfCauchy(\"claim_lam\", beta=1), \n      observed=response\n    )\n    idata_pymc = pm.sample(\n      tune=2000, draws=4000, target_accept=0.9, random_seed=1234,\n      return_inferencedata=True\n    )\nclaim ~ Wald\n█\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [claim_lam, β_area, β_gender, β_age, β_0]\nSampling 2 chains for 2_000 tune and 4_000 draw iterations (4_000 + 8_000 draws total) took 30 seconds.\n\nBambi model\nAs you can see below, we don’t need to do any data preparation, or even specify priors by hand. Bambi automatically obtains sensible default priors when they are not specified, and also knows how to handle each variable type very well.\nThe model is specified using a model formula, quite similar to model formulas in R. The left-hand side of ~ is the response variable, and the rest are the predictors. Here C(agecat) tells Bambi that agecat should be interpreted as categorical. The family argument indicates the conditional distribution for the response, and the link tells Bambi which function of the mean is being modeled by the linear predictor. More information about how they work can be found here.\nThen we have the .fit() method, where you can pass arguments to the pm.sample() function that’s running in the background.\n\nmodel_bambi= bmb.Model(\n  \"claimcst0 ~ C(agecat) + gender + area\", \n  data, \n  family = \"wald\", \n  link = \"log\"\n)\nidata_bambi = model_bambi.fit(tune=2000, draws=4000, target_accept=0.9, random_seed=1234)\n█\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [claimcst0_lam, area, gender, C(agecat), Intercept]\nSampling 2 chains for 2_000 tune and 4_000 draw iterations (4_000 + 8_000 draws total) took 26 seconds.\n\nAnd that’s it! A model that took several lines of codes to specify in PyMC3 only took a few lines of code in Bambi. Quite an advantage, right?\nCheck results\nThe simplicity we gain with Bambi would be worthless if the results turned out to be different. We want an interface that makes our job easier, without affecting the quality of the inference. The following is a forest plot where the point gives the posterior mean and the bars indicate a 94% HDI.\n\n\n\n\n\n\n\n\n\nWhile most of the marginal posteriors match very well, we can clearly see the ones for β_area[3] and β_area[4] don’t overlap as much as the others. One of the possible explanations for this difference is related to the MCMC algorithm. While we know both models are indeed the same model, their internal representation is not exactly the same. For example, the model we wrote in pure PyMC3 computes a unique dot product between a matrix of shape (n, p) a vector of shape (p, 1), while the model in Bambi is computing the sum of many smaller dot products. As the internal representations are not exactly the same, the sampling spaces differ and the sampling algorithm obtained slightly different results.\nConclusion\nIn this post, we saw how the same GLM can be expressed in both PyMC3 and Bambi. PyMC3 allowed us to control every fine-grained detail of the model specification, while Bambi allowed us to express the same model in a much more concise manner.\nBambi’s advantages in these types of scenarios aren’t only related to the amount of code one has to write. Bambi also prevents us from making mistakes when writing the PyMC3 model, such as the mistakes I was making when specifying the shapes for the distributions. Or one could just simply don’t know how correctly prepare the data that should go in the design matrix, such as the conversion of the categorical data to numeric matrices in such a way that the information is retained without introducing structural redundancies.\nNevertheless, this doesn’t mean we should always favor Bambi over PyMC3. Whether Bambi or PyMC3 is appropriate for you actually depends on your use case. If you’re someone who mainly needs to fit GLMs or GLMMs, Bambi is the way to go and it would be nice you give it a chance. There are a bunch of examples showing how to specify and fit different GLMs with Bambi. On the other hand, if you’re someone who writes a lot of custom models, PyMC3 will be your best friend when it comes to working with Bayesian models in Python.\nBambi is a community project and welcomes contributions such as bug fixes, examples, issues related to bugs or desired enhancements, etc. Want to know more? Visit the official docs or explore the Github repo. Also, if you have any doubts about whether the feature you want is available or going to be developed, feel free to reach out to us! You can always open a new issue to request a feature or leave feedback about the library, and we welcome them a lot 😁.\nAcknowledgments\nI want to thank Agustina, Ravin, and Osvaldo for very useful comments and feedback on an earlier version of this post. They helped me to make this post much nicer than what it was originally.\n\n\n\n",
    "preview": "posts/2021-05-24-bambi-wald/bambi-wald_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-01-15T20:25:47-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-11-03-bingo-cards-in-r/",
    "title": "How to generate bingo cards in R",
    "description": "A walkthrough the process of understanding how bingo cards are composed\nand a set of R functions that let us generate random bingo cards and print them\nin a nice looking .pdf output.",
    "author": [],
    "date": "2020-11-03",
    "categories": [],
    "contents": "\nHello wor… Well, my first hello world post appeared about a year ago, but this site had the same fate as many of my othe side-projects… abandonment.\nUntil now.\nIntroduction\nToday I’m going to show you how I came up with “an algorithm” to generate random bingo cards and some utility functions to print them on a nice looking (?) .pdf file.\nFirst of all, what type of bingo card I’m referring to? As an Argentine, the only bingo cards I’ve ever heard of are bingo cards like this one\n\nExample bingo card from bingo.es\nIt contains fifteen numbers from 1 to 90 that are divided in three rows and nine columns. The first column contains numbers between 1 and 9, the second column numbers between 10 and 20, and so on until the last column that contains numbers between 80 and 90. The type of bingo that you play with this bingo card is known as the 90-ball bingo game or British bingo. As I said, this is the only version I knew before this project 1 and I think it is the only bingo version you’ll find here in Argentina (I also bet you’ll find some fellow Argentine confirming this a national invention).\nSo, if you entered this post thinking you’ll find how to print those bingo cards that are popular in places like United States, I’m sorry, this is not for you 2. Fortunately, other people have invented a tool for you even before I wondered how to generate bingo cards. If you are interested, have a look at this package and the Shiny app introduced there.\nNow, let’s go back to our business.\nAnyone who has gone to one of those events where people gather to play bingo 3 knows that bingo cards don’t usually come separated in individual pieces of paper. Sellers usually have strips of six bingo cards in their hands. In some events, you can buy bingo cards directly. In others, you have to buy the entire strip.\nSince this is a 90-ball bingo game and each card contains fifteen numbers, six bingo cards with no repeated numbers is all we need to have all the numbers of the game in a single strip. You see where it is going?. Yes, we won’t generate isolated cards, we’ll generate entire strips. This is how a bingo strip looks like (just imagine them vertically stacked on a single strip)\n\nExample bingo strip from bingo.es\nValid cards and valid strips\nBingo cards are not just a bunch of numbers thrown at a piece of paper. All valid strips are composed of six valid cards each made of three valid rows. But not any combinations of three valid rows make up a valid card nor any combinations of six valid cards make up a valid strip. What a shame!\nBut what is a valid row, a valid card, a va… whatever. Let’s just get to the point and list the rules that will govern how we generate bingo cards.\nValid row\nWe’re going to think that a row is a numeric vector of length nine where some elements are empty and some are filled with numbers.\nExactly five elements are numbers, and four are empty.\nThere can’t be more than two consecutive empty elements, which is equivalent to having at most three consecutive numbers.\nExample valid rows\n\n\n\nExample invalid rows\n\n\n\nValid card\nWe can think that a bingo card is a matrix of three rows and nine columns. Each row must be a valid row as specified in the previous point, plus\nNo column can be completely empty.\nNo column can be completely filled with numbers.\nNumbers are sorted in ascending order within columns.\nExample valid card\n\n\n\nValid strip\nA valid strip contains six valid cards that satisfy the following conditions\nThe first column must have nine numbers and nine empty slots.\nColumns 2 to 8 must have ten numbers and eight empty slots.\nColumn 9 must have eleven numbers and seven empty slots.\nIn total, we have \\(6\\times3\\times9 = 162\\) slots in a strip.\n90 of them are filled with numbers, 72 are not.\nSample this, sample that, I’ve got no need to compute them all4\nOne approach to generate bingo cards would be to get all possible combinations of row layouts, bingo layouts, number arrangements, etc. But the number of cards you could generate is huge and the task wouldn’t be easy at all.\nThe approach used here is one that mixes some simple combinatorics and random sampling. We use permutations to compute all the possible row layouts. Then, we sample rows to create cards and sample cards to create strips5.\nFirst of all, we are going to find valid layouts (i.e. the skeleton of our bingo strips). Once we have them, we are going to fill them with numbers.\nFinding valid rows\nIf we represent empty slots with a 0 and filled slots with a 1, getting all permutations between four 0s and five 1s is as simple as calling combinat::permn(c(rep(0, 4), rep(1, 5))). However, this is not what we want because not all the returned layouts are valid rows. We need to select only those row layouts that are valid in a bingo card.\nThe following function, find_window(), receives a numeric vector x and looks for find windows of length width where all the elements are equal to what. If such a window is found, the function returns TRUE, otherwise it returns FALSE.\n\n\nfind_window = function(x, width, what) {\n  for (i in 1:(length(x) - width)) {\n    if (all(x[i:(i + width)] == what)) return(TRUE)\n  }\n  return(FALSE)\n}\n\n\n\nThen we write a function called get_rows() that generates all the possible row layouts and uses find_window() to select the layouts that satisfy our conditions.\n\n\nget_rows = function() {\n  # Get all row layouts\n  rows = combinat::permn(c(rep(0, 4), rep(1, 5)))\n  # Keep rows with at most two consecutive empty slots\n  rows = rows[!vapply(rows, find_window, logical(1), 2, 0)]\n  # Keep rows with at most three consecutive filled slots\n  rows = rows[!vapply(rows, find_window, logical(1), 3, 1)]\n  return(rows)\n}\n\n\n\nSampling valid cards\nWe noted that a valid card is made of three valid rows, but not all combinations of three valid rows make up a valid card. What if we sample three row layouts and keep/discard the combination based on whether they make up a valid card or not? We can repeat this until we have some desired number of card layours. The process is as follows\nLet \\(N\\) be the number of cards we want to generate.\nWhile the number of cards generated is smaller than \\(N\\), do:\nSample three rows and make up the card.\nCount the number of filled slots per column.\nIf all the counts are between 1 and 3, keep the card, else discard it.\n\nOnce we’re done, we end up with \\(N\\) bingo card layouts that are valid in terms of our requirements above.\nThis idea is implemented in a function called get_cards(). It receives the rows we generate with get_rows() and the number of card layouts we want to generate. Finally it returns a list whose elements are vectors of length 3 with the row indexes6.\n\n\nget_cards = function(rows, cards_n = 2000) {\n  rows_n = length(rows)\n  cards = vector(\"list\", cards_n)\n  \n  attempts = 0\n  card_idx = 0\n  \n  while (card_idx < cards_n) {\n    attempts = attempts + 1\n    # Sample three rows\n    row_idxs = sample(rows_n, 3)\n    mm = matrix(unlist(rows[row_idxs]), ncol = 9, byrow = TRUE)\n    col_sums = colSums(mm)\n    \n    # Select valid cards.\n    # These have between 1 and 3 numbers per column.\n    if (all(col_sums != 0) && all(col_sums != 3)) {\n      card_idx = card_idx + 1\n      cards[[card_idx]] = list(row_idxs, col_sums)\n    }\n\n    # Print message every 1000 attempts\n    if (attempts %% 1000 == 0) {\n      message(\"Attempt \", attempts, \" | Cards built:\", card_idx, \"\\n\")\n    }\n  }\n  # Check duplicates\n  dups = duplicated(lapply(cards, `[[`, 1))\n  message(\"There are \", sum(dups), \" duplicated cards.\")\n  return(cards)\n}\n\n\n\nSampling valid strips\nThis is the much like what we did above, with two differences.\nInstead of sampling three row layouts, we sample six card layouts. Instead of checking if the number of filled slots per column are between 1 and 3, we check if they match a number between 9 and 11 specific to each of them.\nThen, we have get_strips(). It receives a list called cards where each element contains the three row indexes corresponding to each card layout. rows is a list of row layouts and strips_n controls how many strip layouts we want to generate.\n\n\nget_strips = function(cards, rows, strips_n = 100) {\n  valid_counts = c(9, rep(10, 7), 11)\n\n  cards_n = length(cards)\n  strips = vector(\"list\", strips_n)\n  \n  attempts = 0\n  strip_idx = 0\n  \n  while (strip_idx < strips_n) {\n    attempts = attempts + 1\n    \n    # Sample 6 cards\n    cards_idxs = sample(cards_n, 6)\n    strip = cards[cards_idxs]\n    \n    # Contains column counts by card\n    card_counts = matrix(\n      unlist(lapply(strip, `[[`, 2)), \n      ncol = 9, byrow = TRUE\n    )\n    \n    # Check if strip column counts are valid\n    if (all(colSums(card_counts) == valid_counts)) {\n      strip_idx = strip_idx + 1\n      # Get row indexes contained in the selected card indexes\n      rows_idxs = unlist(lapply(cards[cards_idxs], `[[`, 1))\n      strips[[strip_idx]] = matrix(\n        unlist(rows[rows_idxs]), \n        ncol = 9, byrow = TRUE\n      )\n    }\n    # Print message every 1000 attempts\n    if (attempts %% 1000 == 0) {\n      message(\"Attempt \", attempts, \" | Strips built:\", strip_idx, \"\\n\")\n    }\n  }\n  dups = duplicated(strips) \n  message(\"There are \", sum(dups), \" duplicatd layouts.\\n\")\n  return(strips)\n}\n\n\n\nA last but not least step\nI’ve never seen a bingo game where you are given empty layouts and are asked to put numbers yourself. So let’s wrap this up and fill our empty cards!\nfill_strips() receives the strip layouts we generated, randomly selects n of them, and, also randomly, fills the slots the cards with numbers. Of course, the first column contains numbers from 1 to 9, the second column contains numbers from 10 to 19… and so on until the last column, that has numbers from 80 to 90.\n\n\nfill_strips = function(strips, n = 100) {\n  # Numbers that go in each column\n  numbers = list(1:9, 10:19, 20:29, 30:39, 40:49, 50:59, 60:69, 70:79, 80:90)\n  # Row indexes corresponding to each card in the strip\n  card_rows = list(1:3, 4:6, 7:9, 10:12, 13:15, 16:18)\n  \n  fill_strip = function(strip) {\n    # Put numbers in the slots with a 1 (meaning they must contain a number)\n    strip[strip == 1] = unlist(\n      # This `sample()` reorders the numbers in each column randomly\n      mapply(sample, numbers, sapply(numbers, length))\n    )\n    \n    for (i in seq_along(card_rows)) {\n      strip_ = strip[card_rows[[i]], ]\n      # Numbers in a given column are sorted in ascending order within cards\n      x = sort(strip_)\n      strip_[strip_ != 0] = x[x != 0]\n      strip[card_rows[[i]], ] = strip_\n    }\n    return(strip)\n  }\n  # Strip layouts can be repeated\n  strips = lapply(sample(strips, n, replace = TRUE), fill_strip)\n  message(\"There are \", sum(duplicated(strips)), \" duplicated strips.\\n\")\n  return(strips)\n}\n\n\n\nAnd we finally get our bingo strips :)\n\n\nset.seed(0303456)\nrows = get_rows()\ncards = get_cards(rows, 1000)\nstrips = get_strips(cards, rows, 20)\nstrips = fill_strips(strips, 50)\n# Output messages have been suppressed\n\n\n\nLet’s check some of them\n\n\nstrips[[1]]\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n [1,]    0   11   20    0   48    0    0   74   80\n [2,]    8    0    0   31    0   51   60   78    0\n [3,]    0   19   27   39    0   54   62    0    0\n [4,]    1    0   26    0   42   55    0    0   84\n [5,]    2   14    0   34    0    0   65   77    0\n [6,]    0   17   29    0   43   59    0    0   89\n [7,]    0    0   22   33    0    0   64   75   88\n [8,]    0   15    0   35   45    0    0   79   90\n [9,]    9    0   25    0   49   50   66    0    0\n[10,]    3    0   28   30    0    0   61   71    0\n[11,]    7    0    0   36   40   58    0    0   81\n[12,]    0   10    0    0   44    0   63   76   87\n[13,]    0    0   21   37    0   52   68   70    0\n[14,]    5   16    0    0   41    0    0   72   82\n[15,]    0   18    0   38   47   57    0    0   86\n[16,]    0    0   23    0   46   53    0   73   83\n[17,]    4   12    0   32    0    0   67    0   85\n[18,]    6   13   24    0    0   56   69    0    0\n\n\n\nstrips[[30]]\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n [1,]    0    0   25    0   43   50    0   74   80\n [2,]    0   16   26   34    0    0   65   79    0\n [3,]    6   17    0   38    0   58    0    0   86\n [4,]    3    0   27    0   40   51   61    0    0\n [5,]    4    0    0   32   49   59    0    0   81\n [6,]    0   19   29   35    0    0   68   71    0\n [7,]    1   14    0    0   47    0   60   75    0\n [8,]    2    0   20   31    0    0   66    0   83\n [9,]    0    0   24    0   48   55    0   77   89\n[10,]    0    0   28   33   42    0   64   76    0\n[11,]    5   12    0   39    0    0   67    0   84\n[12,]    9   15    0    0   45   54    0    0   87\n[13,]    0   13   21    0    0   52    0   73   85\n[14,]    0   18   22    0   44    0   63   78    0\n[15,]    8    0    0   37   46   56    0    0   90\n[16,]    0    0   23   30    0   53   62    0   82\n[17,]    7   10    0   36    0    0   69   70    0\n[18,]    0   11    0    0   41   57    0   72   88\n\nAre we going to play on R consoles?\nAll we got so far are matrices that look like a bingo strip. But honestly, without any given context, they just look like a bunch of matrices of the same dimension filled with 0s and other integer numbers. Our last task is to generate a .pdf output where these matrices really look like bingo cards.\nIn this last part of the post we make use of the grid package. For those who haven’t heard of it, it is the low level plotting library behind ggplot2, for example.\nHere we have a little function, make_grid(), that given a number of rows and columns returns the natural parent coordinates of the borders the grid that defines the rectangles within each card.\n\n\nmake_grid = function(rows, cols) {\n  lines_rows = grid::unit((0:rows) / rows, \"npc\")\n  lines_cols = grid::unit((0:cols) / cols, \"npc\")\n  return(list(\"row\" = lines_rows, \"col\" = lines_cols))\n}\n\n\n\nAnd now we have the main function used to plot the bingo strips. Since the function is quite large, I prefer to explain how it works with comments in the body.\n\n\nplot_strips = function(strips, col = \"#8e44ad\", width_row = 0.925, \n                       width_col = 0.975) {\n  \n  # `rows` and `cols` are the dimensions of each card\n  rows = 3\n  cols = 9\n  g = make_grid(rows, cols)\n  # Compute the center of each square in the card grid\n  centers_rows = g$row[-1] - grid::unit(1 / (rows * 2), \"npc\")\n  centers_cols = g$col[-1] - grid::unit(1 / (cols * 2), \"npc\")\n  # Sort the centers appropiately\n  # This is required because of how we loop over the values in each card\n  x_coords = rep(centers_cols, each = rows)\n  y_coords = rep(rev(centers_rows), cols)\n  \n  # Create unique identifiers for the cards\n  cards_n = paste(paste0(\"CARD N\", intToUtf8(176)), \n                  seq_len(length(strips) * 6))\n  # Compute the number of sheets we're going to need. \n  # Each sheet contains two strips\n  sheets_n = ceiling(length(strips) / 2)\n  \n  # Initial numbers\n  card_idx = 0\n  strip_idx = 0\n  \n  # Loop over sheets\n  for (sheet_idx in seq_len(sheets_n)) {\n    # Each sheet is a grid of 6 rows and 3 columns. \n    # Columns 1 and 3 are where we place the strips. \n    # Column 2 just gives vertical separation.\n    l = grid::grid.layout(nrow = 6, ncol = 3, \n                          widths = c(48.75, 2.5 + 3.75, 48.75))\n    # Start a new page filled with white\n    grid::grid.newpage()\n    grid::grid.rect(gp = grid::gpar(col = NULL, fill = \"white\"))\n    \n    vp_mid = grid::viewport(0.5, 0.5, width_row, width_col, layout = l)\n    grid::pushViewport(vp_mid)\n    \n    # Loop over columns 1 and 3\n    for (j in c(1, 3)) {\n      # Select strip\n      strip_idx = strip_idx + 1\n      if (strip_idx > length(strips)) break\n      strip = strips[[strip_idx]]\n      \n      # Loop over rows (these rows represent the 6 rows assigned to cards)\n      for (i in 1L:l$nrow) {\n        card_idx = card_idx + 1\n        vp_inner = grid::viewport(layout.pos.row = i, layout.pos.col = j)\n        grid::pushViewport(vp_inner)\n        \n        # Add card identification number on top-left\n        grid::grid.text(\n          label = cards_n[card_idx],\n          x = 0,\n          y = 0.96,\n          just = \"left\",\n          gp = grid::gpar(fontsize = 9)\n        )\n        \n        # Draw a grill that separates the slots in the card\n        vp_mid_inner = grid::viewport(0.5, 0.5, 1, 0.80)\n        grid::pushViewport(vp_mid_inner)\n        grid::grid.grill(h = g$row,  v = g$col, gp = grid::gpar(col = col))\n        \n        # Select the numbers that correspond to this card\n        numbers = as.vector(strip[(3 * i - 2):(3 * i), ])\n        # Logical vector that indicates which rectangles are filled\n        # with nunumbers and which rectangles are empty\n        lgl = ifelse(numbers == 0, FALSE, TRUE)\n        \n        # Draw the numbers in positions given by the rectangle centers\n        grid::grid.text(\n          label = numbers[lgl],\n          x = x_coords[lgl],\n          y = y_coords[lgl],\n          gp = grid::gpar(fontsize = 18)\n        )\n        \n        # Fill empty slots with color\n        grid::grid.rect(\n          x = x_coords[!lgl],\n          y = y_coords[!lgl],\n          height = grid::unit(1 / rows, \"npc\"),\n          width = grid::unit(1 / cols, \"npc\"),\n          gp = grid::gpar(\n            col = NA,\n            fill = farver::encode_colour(farver::decode_colour(col), 0.7)\n          )\n        )\n        # End\n        grid::popViewport()\n        grid::popViewport()\n      }\n    }\n    grid::popViewport()\n  }\n}\n\n\n\nNow, all we need is to pass the strips generated above to plot_strips() and wrap that call within grDevices::pdf() and grDevices::dev.off().\n\n\n# Height and width are in inches and here they correspond to legal paper size\ngrDevices::pdf(\"strips.pdf\", height = 14, width = 8.5)\nplot_strips(strips)\ngrDevices::dev.off()\n\n\n\nIf it works, you’ll have a 25 pages pdf with bingo cards that look like this one\n\nFirst card in the output\nIf you can’t (or just don’t want to) run the code, here you have the generated pdf.\n\nHow I dare to call this a project?↩︎\nBut you should try this bingo, you gonna like it!↩︎\nSome are also known as sobremesa↩︎\nHaven’t you heard Estallando desde el océano by Sumo?↩︎\nIf you’ve heard of Sampford’s pps sampling, this is going to be familiar↩︎\nI know that returning row indexes is less intuitive than returning card layouts, but this approach requires less memory because it only stores 3 values per card, instead of 18.↩︎\n",
    "preview": {},
    "last_modified": "2022-01-15T20:16:36-03:00",
    "input_file": {}
  }
]
