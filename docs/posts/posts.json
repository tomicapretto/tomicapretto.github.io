[
  {
    "path": "posts/2021-06-28-first-weeks-of-gsoc/",
    "title": "First weeks of GSoC",
    "description": "First post of a series about my contributions to Bambi\nin this Google Summer of Code season. This post highlights new features related\nto default priors and priors for group-specific effects.",
    "author": [],
    "date": "2021-06-28",
    "categories": [],
    "contents": "\n\n\n\nI am really happy to participate in this Google Summer of Code season with NumFOCUS to contribute to the Bambi library. The coding period ranges from June 7 to August 16, with an intermediate evaluation taking place between July 12 and July 16.\nOverview\nMy project is called Extend available models and default priors in Bambi. The main goal of this project is to add new families of generalized linear models, such as beta regression, robust linear regression (i.e. linear model with error following a T-Student distribution)1 as well as multinomial regression. However, this raises a second problem, which is about default priors distributions.\nDefault priors in Bambi are limited to the families implemented in the GLM module in statsmodels, which does not include the families mentioned above. For this reason, it is first necessary to incorporate alternative automatic priors so new families work without requiring the user to manually specify priors.\nTherefore, these first weeks of the coding period were centered around understanding how default priors work on other high-level modeling packages such as brms and rstanarm, how to translate their ideas into PyMC3 code, and finally how to implement everything within Bambi.\nAlternative default priors\nCurrently, Bambi uses maximum likelihood estimates in the construction of its default priors. There are two limitations associated with this approach. First, current default priors don’t exist whenever uniquely identifiable maximum likelihood estimates don’t exist (e.g. \\(p > n\\) or complete separation scenarios). Secondly, these estimates are obtained via the GLM module in statsmodels, which means default priors can only be obtained for families made available in statsmodels.\nBased on the available documentation and simulations I’ve done, I decided to implement alternative default priors that are much like the default priors in rstanarm. These priors aim to be weakly-informative in most scenarios and do not depend on maximum likelihood estimates. Their documentation is excellent and it was a great guide for my implementation.\nThis is the PR where I implement alternative default priors inspired on rstanarm default priors. In addition, I also implement LKJ prior for the correlation matrices of group-specific effects.\nHow to invoke alternative default priors\nThe Model() class has gained one new argument, automatic_priors, that can be equal to \"default\" to use Bambi’s default method, or \"rstanarm\" to use the alternative implementation2.\nmodel = Model(\"y ~ x + z\", data, automatic_priors=\"rstanarm\")\nHow to use LKJ priors for correlation matrices of group-specific effects\nGroup-specific effects can now have non-independent priors. Instead of using independent normal distributions, we can use a multivariate normal distribution whose correlation matrix has an LKJ prior distribution. This distribution depends on a parameter \\(\\eta > 0\\). If \\(\\eta=1\\), the LJK prior is jointly uniform over all correlation matrices of the same dimension. If \\(\\eta >1\\) increases, the mode of the distribution is the identity matrix. The larger the value of \\(\\eta\\) the more sharply peaked the density is at the identity matrix.\nModel has an argument priors_cor where we can pass a dictionary to indicate which groups are going to have a LKJ prior. The keys of the dictionary are the names of the groups, and the values are the values for \\(\\eta\\).\nIn the following model, we have a varying intercept and varying slope for the groups given by group. These varying effects have a multivariate normal prior whose covariance matrix depends on a correlation matrix that has a LKJ hyperprior with \\(\\eta=1\\).\nmodel = Model(\"y ~ x + (x|group)\", data, priors_cor={\"group\": 1})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n These two distributions are not members of the exponential family so using them as the distribution of the random component does not result in a generalized linear model in a strict sense. But I would usually refer to them as GLMs since the linear predictor, link function, and random component properties are still present.↩︎\nBoth the argument name and the options may change↩︎\n",
    "preview": {},
    "last_modified": "2022-01-15T20:31:46-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-08-group-specific-effects-matrix/",
    "title": "Design matrices for group-specific effects in formulae and lme4",
    "description": "Bambi uses the library formulae to automatically construct design \nmatrices for both common and group-specific effects. This post compares design \nmatrices for group-specific effects obtained with formulae for a variety of \nscenarios involving categorical variables with the ones obtained with the R \npackage lme4.",
    "author": [],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\nIntroduction\nA linear mixed model can be written as\n\\[\n\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \n                 \\boldsymbol{Z}\\boldsymbol{u} + \\boldsymbol{\\epsilon}\n\\]\nwhere \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) are the two design matrices we need to somehow construct when dealing with this type of model. \\(\\boldsymbol{X}\\) is the design matrix for the common (a.k.a. fixed) effects, and \\(\\boldsymbol{Z}\\) is the design matrix for the group-specific (a.k.a. random or varying) effects.\nIt is quite easy to obtain the design matrix \\(\\boldsymbol{X}\\) in R using its popular formula interface. In Python, patsy provides equivalent functionality. Unfortunately, there aren’t as many alternatives to compute the matrix \\(\\boldsymbol{Z}\\).\nIn R, there’s lme4, the statistical package par excellence for mixed models. It extends the base formula interface to include group-specific effects via the pipe operator (|) and internally computes both \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) without the user noticing. That’s great!\nIn Python, we are working on formulae, a library we use to handle mixed model formulas in Bambi. In this process, I’ve found Fitting Linear Mixed-Effects Models Using lme4 vignette extremely useful when figuring out how to compute the design matrix for the group-specific effects.\nToday, I was adding tests to make sure we are constructing \\(\\boldsymbol{Z}\\) appropriately and found myself comparing the matrices obtained with formulae with matrices obtained with lme4. Then I was like … why not making this a blog post? 🤔\n… and so here we are! But before we get started, just note this post mixes both R and Python code. I will try to be explicit when I’m using one language or the other. But if you’re reading a chunk and it looks like Python, it’s Python. And if it looks like R… you guessed! It’s R.\nSetup\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lme4)\nlibrary(patchwork)\n\n\n\n\n\n\n\nfrom formulae import design_matrices\n\n\n\n\nProblem\nHere we will be comparing design matrices for the group-specific terms in a mixed-effects model obtained with both lme4 and formulae. We’re using the dataset Pixel that comes with the R package nlme.\n\n\ndata(\"Pixel\", package = \"nlme\")\nhead(Pixel)\n\n\nGrouped Data: pixel ~ day | Dog/Side\n  Dog Side day  pixel\n1   1    R   0 1045.8\n2   1    R   1 1044.5\n3   1    R   2 1042.9\n4   1    R   4 1050.4\n5   1    R   6 1045.2\n6   1    R  10 1038.9\n\nWe’re not interested in how to fit a certain model here. We’re interested in constructing the design matrix for group-specific effects with different characteristics. We use the following formula\n\n\nf1 = ~ (0 + day | Dog) + (1 | Side / Dog)\n\n\n\nwhere each part can be interpreted as follows\n(0 + day | Dog) means that day has a group-specific slope for each Dog. This is usually known as a random slope. The 0 indicates not to add the default group-specific intercept (because it’s added next).\n(1 | Side / Dog) is equivalent to (1 | Side) + (1 | Dog:Side). This means there’s a varying intercept for each Side and a varying intercept for each combination of Dog and Side. In other words, we have a nested group-specific intercept, where Dog is nested within Side.\n\n\nlme4_terms = mkReTrms(findbars(f1), model.frame(subbars(f1), data = Pixel))\n\n\n\nlme4_terms contains much more information than what we need for this post. We mostly use lme4_terms$Ztlist, which is a list that contains the transpose of the group-specific effects model matrix, separated by term. These matrices are stored as sparse matrices of dgCMatrix class. If we want to have the sub-matrix for a given group-specific term as a base R matrix, we have to do as.matrix(t(lme4_terms$Ztlist$[[\"term\"]])).\n\n\nnames(lme4_terms$Ztlist)\n\n\n[1] \"1 | Dog:Side\"  \"0 + day | Dog\" \"1 | Side\"     \n\nWe have three group-specific terms. The first and the last ones are the group-specific intercepts we mentioned. These are the result of the nested group-specific intercept (1 | Side / Dog). Dog is nested within Side and consequently there’s an intercept varying among Side and another varying among Dog within Side. The second term, 0 + day | Dog, represents varying slope of day for each level of Dog.\nWe finally store the sub-matrix for each term in different objects that we’ll later use when comparing results with those obtained with formulae.\n\n\nday_by_dog = as.matrix(t(lme4_terms$Ztlist$`0 + day | Dog`))\nintercept_by_side = as.matrix(t(lme4_terms$Ztlist$`1 | Side`))\nintercept_by_side_dog = as.matrix(t(lme4_terms$Ztlist$`1 | Dog:Side`))\n\n\n\nOn the other hand, in Python, we use design_matrices() from the formulae library to obtain a DesignMatrices object. All the information associated with the group-specific terms is contained in the .group attribute and the sub-matrix corresponding to a particular term is accessed with .group[term_name].\n\ndm = design_matrices(\"(0 + day | Dog) + (1 | Side / Dog)\", r.Pixel)\n\nThere’s a dictionary called terms_info within dm.group. To see the names of the group-specific effects we just retrieve the keys.\n\ndm.group.terms_info.keys()\ndict_keys(['day|Dog', '1|Side', '1|Side:Dog'])\n\nNames differ a little with the ones from lme4, but they represent the same thing.\n\nday_by_dog = dm.group['day|Dog']\nintercept_by_side = dm.group['1|Side']\nintercept_by_side_dog = dm.group['1|Side:Dog']\n\nNow let’s compare those matrices!\nDesign matrices for (day|Dog)\nRectangles in the following plot correspond to the cells in the matrix. The lowest value for day is 0, represented by violet, and the highest value is 21, represented by yellow. The 10 columns represent the 10 groups in Dog, and the rows represent the observations in Pixel. Here, and also in the other cases, the left panel contains the matrix obtained with lme4 and the right panel the one produced with formulae.\n\n\n\nIn this first case, both panels are representing the same data so we can happily conclude the result obtained with formulae matches the one from lme4. Yay!!\nBut we’re humans and our eyes can fail so it’s better to always check appropiately with\n\n\nall(py$day_by_dog == day_by_dog)\n\n\n[1] TRUE\n\nDesign matrices for (1|Side)\nHere the first column represents Side == \"L\" and the second column represents Side == \"R\". Since we’re dealing with an intercept, violet means 0 and yellow means 1. In this case it is much easier to see both results match.\n\n\n\n\n\nall(py$intercept_by_side == intercept_by_side)\n\n\n[1] TRUE\n\nDesign matrices for (1|Side:Dog)\nBut things are not always as one wishes. It’s clear from the following plot that both matrices aren’t equal here.\n\n\n\nBut don’t worry. We’re not giving up. We still have things to do1. We can check what are the groups being represented in the columns of the matrices we’re plotting.\n\n\ncolnames(intercept_by_side_dog)\n\n\n [1] \"1:L\"  \"1:R\"  \"10:L\" \"10:R\" \"2:L\"  \"2:R\"  \"3:L\"  \"3:R\"  \"4:L\" \n[10] \"4:R\"  \"5:L\"  \"5:R\"  \"6:L\"  \"6:R\"  \"7:L\"  \"7:R\"  \"8:L\"  \"8:R\" \n[19] \"9:L\"  \"9:R\" \n\n\ndm.group.terms_info[\"1|Side:Dog\"][\"full_names\"]\n['1|Side:Dog[L:1]', '1|Side:Dog[L:10]', '1|Side:Dog[L:2]', '1|Side:Dog[L:3]', '1|Side:Dog[L:4]', '1|Side:Dog[L:5]', '1|Side:Dog[L:6]', '1|Side:Dog[L:7]', '1|Side:Dog[L:8]', '1|Side:Dog[L:9]', '1|Side:Dog[R:1]', '1|Side:Dog[R:10]', '1|Side:Dog[R:2]', '1|Side:Dog[R:3]', '1|Side:Dog[R:4]', '1|Side:Dog[R:5]', '1|Side:Dog[R:6]', '1|Side:Dog[R:7]', '1|Side:Dog[R:8]', '1|Side:Dog[R:9]']\n\nAnd there it is! Matrices differ because columns are representing different groups. In lme4, groups are looping first along Dog and then along Side, while in formulae it is the other way around.\nWe can simply re-order the columns of one of the matrices and generate and check whether they match or not.\n\n\nintercept_by_side_dog_f = as.data.frame(py$intercept_by_side_dog)\ncolnames(intercept_by_side_dog_f) = py$dm$group$terms_info[[\"1|Side:Dog\"]]$groups\nnames_lme4_order = paste(\n  rep(c(\"L\", \"R\"), 10), \n  rep(c(1, 10, 2, 3, 4, 5, 6, 7, 8, 9), each = 2), \n  sep = \":\"\n)\n\nintercept_by_side_dog_f = intercept_by_side_dog_f[names_lme4_order] %>%\n  as.matrix() %>%\n  unname()\n\n\n\n\n\n\n\n\nall(intercept_by_side_dog_f == intercept_by_side_dog)\n\n\n[1] TRUE\n\nAnd there it is! Results match 🤩\nAnother formula\nThis other formula contains an interaction between categorical variables as the expression of the group-specific term, which is something we’re not covering above. In this case, we are going to subset the data so the design matrices are smaller and we can understand what’s going on with more ease.\n\n\n# Subset data\nPixel2 = Pixel %>%\n  filter(Dog %in% c(1, 2, 3), day %in% c(2, 4, 6)) %>%\n  mutate(Dog = forcats::fct_drop(Dog))\n\n# Create terms with lme4\nf2 = ~ day +  (0 + Dog:Side | day)\nlme4_terms = mkReTrms(findbars(f2), model.frame(subbars(f2), data = Pixel2))\ndog_and_side_by_day = as.matrix(t(lme4_terms$Ztlist$`0 + Dog:Side | day`))\n\n\n\nAnd now with design_matrices() in Python.\n\n# Create terms with \ndm = design_matrices(\"(0 + Dog:Side|day)\", r.Pixel2)\ndog_and_side_by_day = dm.group[\"Dog:Side|day\"]\n\nDesign matrix for (Dog:Side|day)\nAlthough this term is called slope, it is not actually a slope like the one for (day|Dog). Since both Dog and Side are categorical, the entries of this matrix consist of zeros and ones.\n\n\n\nWe have the same problem than above, matrices don’t match. So we know what to do: look at the groups represented in the columns.\n\n\ncolnames(dog_and_side_by_day)\n\n\n [1] \"2\" \"2\" \"2\" \"2\" \"2\" \"2\" \"4\" \"4\" \"4\" \"4\" \"4\" \"4\" \"6\" \"6\" \"6\" \"6\"\n[17] \"6\" \"6\"\n\n\ndm.group.terms_info[\"Dog:Side|day\"][\"full_names\"]\n['Dog[1]:Side[L]|2.0', 'Dog[1]:Side[R]|2.0', 'Dog[2]:Side[L]|2.0', 'Dog[2]:Side[R]|2.0', 'Dog[3]:Side[L]|2.0', 'Dog[3]:Side[R]|2.0', 'Dog[1]:Side[L]|4.0', 'Dog[1]:Side[R]|4.0', 'Dog[2]:Side[L]|4.0', 'Dog[2]:Side[R]|4.0', 'Dog[3]:Side[L]|4.0', 'Dog[3]:Side[R]|4.0', 'Dog[1]:Side[L]|6.0', 'Dog[1]:Side[R]|6.0', 'Dog[2]:Side[L]|6.0', 'Dog[2]:Side[R]|6.0', 'Dog[3]:Side[L]|6.0', 'Dog[3]:Side[R]|6.0']\n\nBut this they represent the same groups2. We can look if there’s a difference in how the interactions are ordered within each group.\n\n\nlme4_terms$cnms\n\n\n$day\n[1] \"Dog1:SideL\" \"Dog2:SideL\" \"Dog3:SideL\" \"Dog1:SideR\" \"Dog2:SideR\"\n[6] \"Dog3:SideR\"\n\nAnd again, thankfully, we see there’s a difference in how columns are being ordered. Let’s see if matrices match after we reorder the one obtained with formulae.\n\n\ndog_and_side_by_day_f = as.data.frame(py$dog_and_side_by_day)\ncolnames(dog_and_side_by_day_f) = py$dm$group$terms_info[[\"Dog:Side|day\"]]$full_names\nside = rep(rep(c(\"L\", \"R\"), each = 3), 3)\ndog = rep(1:3, 6)\nday = rep(c(\"2.0\", \"4.0\", \"6.0\"), each = 6)\n\nnames_lme4_order = glue::glue(\"Dog[{dog}]:Side[{side}]|{day}\")\ndog_and_side_by_day_f = dog_and_side_by_day_f[names_lme4_order] %>%\n  as.matrix() %>%\n  unname()\n\n\n\n\n\n\n\n\nall(dog_and_side_by_day_f == dog_and_side_by_day)\n\n\n[1] TRUE\n\nConclusion\nAlthough formulae works differently than lme4, and has different goals, we showed that formulae produces the same design matrices as lme4 for the variety of examples we covered. While case-based comparisons like these are not what one should rely on when writing software, the examples here were really helpful when working on the implementation in formulae and writing the corresponding tests. And if this post helps someone to better understand what’s going on when working with design matrices associated with group-specific effects, it will have been even more worth it!\n\nI was undoubtedly talking to myself was quite disappointed at this time, wondering what I did wrong. Suffering the consequences of mistakes I wasn’t even aware I made. Well, not that dramatic. But now I’m happy the problem wasn’t real 😅↩︎\nWe have six 2s, six 4s and six 6s in both cases↩︎\n",
    "preview": "posts/2021-06-08-group-specific-effects-matrix/group-specific-effects-matrix_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2022-01-15T20:28:41-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-24-bambi-wald/",
    "title": "Why Bambi?",
    "description": "An example comparing how to fit a GLM with Bambi and PyMC3. Here I \nattempt to highlight how Bambi can help us to write a Bayesian GLM in a concise \nmanner, saving us from having to realize error-prone tasks that are sometimes \nnecessary when directly working with PyMC3.",
    "author": [],
    "date": "2021-05-24",
    "categories": [],
    "contents": "\nIntroduction\nI’ve been thinking about writing a new blog post for a while now but honestly, there was nothing coming to my mind that made me think “Oh, yeah, this is interesting, it can be useful for someone else”. And it was just a few hours ago that I realized I could write about something quite curious that happened to me while trying to replicate a Bambi model with PyMC3.\nPyMC3 is a Python package for Bayesian statistical modeling that implements advanced Markov chain Monte Carlo algorithms, such as the No-U-Turn sampler (NUTS). Bambi is a high-level Bayesian model-building interface in Python. It is built on top of PyMC3 and allows users to specify and fit Generalized Linear Models (GLMs) and Generalized Linear Mixed Models (GLMMs) very easily using a model formula much similar to the popular model formulas in R.\nA couple of weeks ago Agustina Arroyuelo told me she was trying to replicate a model in one of the example notebooks we have in Bambi and wanted my opinion on what she was doing. After many attempts, neither of us could replicate the model successfully. It turned out to be we were messing up with the shapes of the priors and also had some troubles with the design matrix.\nThe point of this post is not about good practices when doing Bayesian modeling neither about modeling techniques. This post aims to show how Bambi can save you effort, code, and prevent us from making some mistakes when fitting not-so-trivial GLMs in Python.\nWell, I think this is quite enough for an introduction. Let’s better have a look at the problem at hand.\nSetup\n\n\n\n\nimport arviz as az\nimport bambi as bmb\nimport numpy as np\nimport pandas as pd\nimport pymc3 as pm\nimport theano.tensor as tt\n\nThe problem\nIn this problem we use a data set consisting of 67856 insurance policies and 4624 (6.8%) claims in Australia between 2004 and 2005. The original source of this dataset is the book Generalized Linear Models for Insurance Data by Piet de Jong and Gillian Z. Heller.\n\nurl = \"https://courses.ms.ut.ee/2020/glm/spring/uploads/Main/carclaims.csv\"\ndata = pd.read_csv(url)\ndata = data[data[\"claimcst0\"] > 0]\n\nThe age (binned), the gender, and the area of residence are used to predict the amount of the claim, conditional on the existence of the claim because we are only working with observations where there is a claim.\nWe use a Wald regression model. This is a GLM where the random component follows a Wald distribution. The link function we choose is the natural logarithm.\nPyMC3 model\nData preparation\nTo fit the model with PyMC3 we first need to create the model matrix. We need to represent age, area, and gender with dummy variables because they are categorical. We can think of the following objects as sub-matrices of the design matrix in the model.\n\nintercept = np.ones((len(data), 1))\nage = pd.get_dummies(data[\"agecat\"], drop_first=True).to_numpy()\narea = pd.get_dummies(data[\"area\"], drop_first=True).to_numpy()\ngender = pd.get_dummies(data[\"gender\"], drop_first=True).to_numpy()\n\nNote we have used drop_first=True. This means that we use n_levels - 1 dummies to represent each categorical variable, and the first level is taken as reference. This ensures the resulting design matrix is of full rank.\nNext, we stack these sub-matrices horizontally and convert the result to a Theano tensor variable so we can compute the dot product between this matrix and the vector of coefficients when writing our model in PyMC3.\n\nX = np.hstack([intercept, age, gender, area])\nX = tt.as_tensor_variable(X)\n\nFit\nWe start declaring the priors for each of the predictors in the model. They are all independent Gaussian distributions. You may wonder where I took the values for the parameters of these distributions. I’ve just copied Bambi’s default values for this particular problem.\nAt this stage, it is very important to give appropriate shapes to all the objects we create in the model. For example, β_age is a random variable that represents the coefficients for the age variable. Since 5 dummy variables are used to represent the age, both β_age and the values passed to mu and sigma must have shape=(5, 1). I’ve failed here many times when trying to replicate the model, so, unfortunately, I know what I’m talking about 😅\n\n# Create model and sample posterior\nwith pm.Model() as model_pymc3:  \n    # Build predictors\n    β_0 = pm.Normal(\n        \"β_0\",\n        mu=np.array([[7.61]]),\n        sigma=np.array([[2.73]]),\n        shape=(1, 1)\n    )\n    β_age = pm.Normal(\n        \"β_age\",\n        mu=np.array([[0] * 5]).T,\n        sigma=np.array([[0.32, 6.94, 1.13, 5.44, 9.01]]).T,\n        shape=(5, 1)\n    )\n    β_gender = pm.Normal(\n        \"β_gender\",\n        mu=np.array([[0]]),\n        sigma=np.array([[1.304491]]),\n        shape=(1, 1)\n    )\n    β_area = pm.Normal(\n      \"β_area\",\n      mu=np.array([[0] * 5]).T,\n      sigma=np.array([[0.86, 0.25, 1.3, 0.76, 5.33]]),\n      shape=(5, 1)\n    )\n    \n    # Concatenate the vectors for the coefficients into a single vector\n    β = tt.concatenate([β_0, β_age, β_gender, β_area], axis=0)\n    \n    # Compute and transform linear predictor\n    mu = tt.exp(X.dot(β))\n      \n    response = np.array([data[\"claimcst0\"]]).T\n    pm.Wald(\n      \"claim\", \n      mu=mu, \n      lam=pm.HalfCauchy(\"claim_lam\", beta=1), \n      observed=response\n    )\n    idata_pymc = pm.sample(\n      tune=2000, draws=4000, target_accept=0.9, random_seed=1234,\n      return_inferencedata=True\n    )\nclaim ~ Wald\n█\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [claim_lam, β_area, β_gender, β_age, β_0]\nSampling 2 chains for 2_000 tune and 4_000 draw iterations (4_000 + 8_000 draws total) took 30 seconds.\n\nBambi model\nAs you can see below, we don’t need to do any data preparation, or even specify priors by hand. Bambi automatically obtains sensible default priors when they are not specified, and also knows how to handle each variable type very well.\nThe model is specified using a model formula, quite similar to model formulas in R. The left-hand side of ~ is the response variable, and the rest are the predictors. Here C(agecat) tells Bambi that agecat should be interpreted as categorical. The family argument indicates the conditional distribution for the response, and the link tells Bambi which function of the mean is being modeled by the linear predictor. More information about how they work can be found here.\nThen we have the .fit() method, where you can pass arguments to the pm.sample() function that’s running in the background.\n\nmodel_bambi= bmb.Model(\n  \"claimcst0 ~ C(agecat) + gender + area\", \n  data, \n  family = \"wald\", \n  link = \"log\"\n)\nidata_bambi = model_bambi.fit(tune=2000, draws=4000, target_accept=0.9, random_seed=1234)\n█\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [claimcst0_lam, area, gender, C(agecat), Intercept]\nSampling 2 chains for 2_000 tune and 4_000 draw iterations (4_000 + 8_000 draws total) took 26 seconds.\n\nAnd that’s it! A model that took several lines of codes to specify in PyMC3 only took a few lines of code in Bambi. Quite an advantage, right?\nCheck results\nThe simplicity we gain with Bambi would be worthless if the results turned out to be different. We want an interface that makes our job easier, without affecting the quality of the inference. The following is a forest plot where the point gives the posterior mean and the bars indicate a 94% HDI.\n\n\n\n\n\n\n\n\n\nWhile most of the marginal posteriors match very well, we can clearly see the ones for β_area[3] and β_area[4] don’t overlap as much as the others. One of the possible explanations for this difference is related to the MCMC algorithm. While we know both models are indeed the same model, their internal representation is not exactly the same. For example, the model we wrote in pure PyMC3 computes a unique dot product between a matrix of shape (n, p) a vector of shape (p, 1), while the model in Bambi is computing the sum of many smaller dot products. As the internal representations are not exactly the same, the sampling spaces differ and the sampling algorithm obtained slightly different results.\nConclusion\nIn this post, we saw how the same GLM can be expressed in both PyMC3 and Bambi. PyMC3 allowed us to control every fine-grained detail of the model specification, while Bambi allowed us to express the same model in a much more concise manner.\nBambi’s advantages in these types of scenarios aren’t only related to the amount of code one has to write. Bambi also prevents us from making mistakes when writing the PyMC3 model, such as the mistakes I was making when specifying the shapes for the distributions. Or one could just simply don’t know how correctly prepare the data that should go in the design matrix, such as the conversion of the categorical data to numeric matrices in such a way that the information is retained without introducing structural redundancies.\nNevertheless, this doesn’t mean we should always favor Bambi over PyMC3. Whether Bambi or PyMC3 is appropriate for you actually depends on your use case. If you’re someone who mainly needs to fit GLMs or GLMMs, Bambi is the way to go and it would be nice you give it a chance. There are a bunch of examples showing how to specify and fit different GLMs with Bambi. On the other hand, if you’re someone who writes a lot of custom models, PyMC3 will be your best friend when it comes to working with Bayesian models in Python.\nBambi is a community project and welcomes contributions such as bug fixes, examples, issues related to bugs or desired enhancements, etc. Want to know more? Visit the official docs or explore the Github repo. Also, if you have any doubts about whether the feature you want is available or going to be developed, feel free to reach out to us! You can always open a new issue to request a feature or leave feedback about the library, and we welcome them a lot 😁.\nAcknowledgments\nI want to thank Agustina, Ravin, and Osvaldo for very useful comments and feedback on an earlier version of this post. They helped me to make this post much nicer than what it was originally.\n\n\n\n",
    "preview": "posts/2021-05-24-bambi-wald/bambi-wald_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-01-15T20:25:47-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-11-03-bingo-cards-in-r/",
    "title": "How to generate bingo cards in R",
    "description": "A walkthrough the process of understanding how bingo cards are composed\nand a set of R functions that let us generate random bingo cards and print them\nin a nice looking .pdf output.",
    "author": [],
    "date": "2020-11-03",
    "categories": [],
    "contents": "\nHello wor… Well, my first hello world post appeared about a year ago, but this site had the same fate as many of my othe side-projects… abandonment.\nUntil now.\nIntroduction\nToday I’m going to show you how I came up with “an algorithm” to generate random bingo cards and some utility functions to print them on a nice looking (?) .pdf file.\nFirst of all, what type of bingo card I’m referring to? As an Argentine, the only bingo cards I’ve ever heard of are bingo cards like this one\n\nExample bingo card from bingo.es\nIt contains fifteen numbers from 1 to 90 that are divided in three rows and nine columns. The first column contains numbers between 1 and 9, the second column numbers between 10 and 20, and so on until the last column that contains numbers between 80 and 90. The type of bingo that you play with this bingo card is known as the 90-ball bingo game or British bingo. As I said, this is the only version I knew before this project 1 and I think it is the only bingo version you’ll find here in Argentina (I also bet you’ll find some fellow Argentine confirming this a national invention).\nSo, if you entered this post thinking you’ll find how to print those bingo cards that are popular in places like United States, I’m sorry, this is not for you 2. Fortunately, other people have invented a tool for you even before I wondered how to generate bingo cards. If you are interested, have a look at this package and the Shiny app introduced there.\nNow, let’s go back to our business.\nAnyone who has gone to one of those events where people gather to play bingo 3 knows that bingo cards don’t usually come separated in individual pieces of paper. Sellers usually have strips of six bingo cards in their hands. In some events, you can buy bingo cards directly. In others, you have to buy the entire strip.\nSince this is a 90-ball bingo game and each card contains fifteen numbers, six bingo cards with no repeated numbers is all we need to have all the numbers of the game in a single strip. You see where it is going?. Yes, we won’t generate isolated cards, we’ll generate entire strips. This is how a bingo strip looks like (just imagine them vertically stacked on a single strip)\n\nExample bingo strip from bingo.es\nValid cards and valid strips\nBingo cards are not just a bunch of numbers thrown at a piece of paper. All valid strips are composed of six valid cards each made of three valid rows. But not any combinations of three valid rows make up a valid card nor any combinations of six valid cards make up a valid strip. What a shame!\nBut what is a valid row, a valid card, a va… whatever. Let’s just get to the point and list the rules that will govern how we generate bingo cards.\nValid row\nWe’re going to think that a row is a numeric vector of length nine where some elements are empty and some are filled with numbers.\nExactly five elements are numbers, and four are empty.\nThere can’t be more than two consecutive empty elements, which is equivalent to having at most three consecutive numbers.\nExample valid rows\n\n\n\nExample invalid rows\n\n\n\nValid card\nWe can think that a bingo card is a matrix of three rows and nine columns. Each row must be a valid row as specified in the previous point, plus\nNo column can be completely empty.\nNo column can be completely filled with numbers.\nNumbers are sorted in ascending order within columns.\nExample valid card\n\n\n\nValid strip\nA valid strip contains six valid cards that satisfy the following conditions\nThe first column must have nine numbers and nine empty slots.\nColumns 2 to 8 must have ten numbers and eight empty slots.\nColumn 9 must have eleven numbers and seven empty slots.\nIn total, we have \\(6\\times3\\times9 = 162\\) slots in a strip.\n90 of them are filled with numbers, 72 are not.\nSample this, sample that, I’ve got no need to compute them all4\nOne approach to generate bingo cards would be to get all possible combinations of row layouts, bingo layouts, number arrangements, etc. But the number of cards you could generate is huge and the task wouldn’t be easy at all.\nThe approach used here is one that mixes some simple combinatorics and random sampling. We use permutations to compute all the possible row layouts. Then, we sample rows to create cards and sample cards to create strips5.\nFirst of all, we are going to find valid layouts (i.e. the skeleton of our bingo strips). Once we have them, we are going to fill them with numbers.\nFinding valid rows\nIf we represent empty slots with a 0 and filled slots with a 1, getting all permutations between four 0s and five 1s is as simple as calling combinat::permn(c(rep(0, 4), rep(1, 5))). However, this is not what we want because not all the returned layouts are valid rows. We need to select only those row layouts that are valid in a bingo card.\nThe following function, find_window(), receives a numeric vector x and looks for find windows of length width where all the elements are equal to what. If such a window is found, the function returns TRUE, otherwise it returns FALSE.\n\n\nfind_window = function(x, width, what) {\n  for (i in 1:(length(x) - width)) {\n    if (all(x[i:(i + width)] == what)) return(TRUE)\n  }\n  return(FALSE)\n}\n\n\n\nThen we write a function called get_rows() that generates all the possible row layouts and uses find_window() to select the layouts that satisfy our conditions.\n\n\nget_rows = function() {\n  # Get all row layouts\n  rows = combinat::permn(c(rep(0, 4), rep(1, 5)))\n  # Keep rows with at most two consecutive empty slots\n  rows = rows[!vapply(rows, find_window, logical(1), 2, 0)]\n  # Keep rows with at most three consecutive filled slots\n  rows = rows[!vapply(rows, find_window, logical(1), 3, 1)]\n  return(rows)\n}\n\n\n\nSampling valid cards\nWe noted that a valid card is made of three valid rows, but not all combinations of three valid rows make up a valid card. What if we sample three row layouts and keep/discard the combination based on whether they make up a valid card or not? We can repeat this until we have some desired number of card layours. The process is as follows\nLet \\(N\\) be the number of cards we want to generate.\nWhile the number of cards generated is smaller than \\(N\\), do:\nSample three rows and make up the card.\nCount the number of filled slots per column.\nIf all the counts are between 1 and 3, keep the card, else discard it.\n\nOnce we’re done, we end up with \\(N\\) bingo card layouts that are valid in terms of our requirements above.\nThis idea is implemented in a function called get_cards(). It receives the rows we generate with get_rows() and the number of card layouts we want to generate. Finally it returns a list whose elements are vectors of length 3 with the row indexes6.\n\n\nget_cards = function(rows, cards_n = 2000) {\n  rows_n = length(rows)\n  cards = vector(\"list\", cards_n)\n  \n  attempts = 0\n  card_idx = 0\n  \n  while (card_idx < cards_n) {\n    attempts = attempts + 1\n    # Sample three rows\n    row_idxs = sample(rows_n, 3)\n    mm = matrix(unlist(rows[row_idxs]), ncol = 9, byrow = TRUE)\n    col_sums = colSums(mm)\n    \n    # Select valid cards.\n    # These have between 1 and 3 numbers per column.\n    if (all(col_sums != 0) && all(col_sums != 3)) {\n      card_idx = card_idx + 1\n      cards[[card_idx]] = list(row_idxs, col_sums)\n    }\n\n    # Print message every 1000 attempts\n    if (attempts %% 1000 == 0) {\n      message(\"Attempt \", attempts, \" | Cards built:\", card_idx, \"\\n\")\n    }\n  }\n  # Check duplicates\n  dups = duplicated(lapply(cards, `[[`, 1))\n  message(\"There are \", sum(dups), \" duplicated cards.\")\n  return(cards)\n}\n\n\n\nSampling valid strips\nThis is the much like what we did above, with two differences.\nInstead of sampling three row layouts, we sample six card layouts. Instead of checking if the number of filled slots per column are between 1 and 3, we check if they match a number between 9 and 11 specific to each of them.\nThen, we have get_strips(). It receives a list called cards where each element contains the three row indexes corresponding to each card layout. rows is a list of row layouts and strips_n controls how many strip layouts we want to generate.\n\n\nget_strips = function(cards, rows, strips_n = 100) {\n  valid_counts = c(9, rep(10, 7), 11)\n\n  cards_n = length(cards)\n  strips = vector(\"list\", strips_n)\n  \n  attempts = 0\n  strip_idx = 0\n  \n  while (strip_idx < strips_n) {\n    attempts = attempts + 1\n    \n    # Sample 6 cards\n    cards_idxs = sample(cards_n, 6)\n    strip = cards[cards_idxs]\n    \n    # Contains column counts by card\n    card_counts = matrix(\n      unlist(lapply(strip, `[[`, 2)), \n      ncol = 9, byrow = TRUE\n    )\n    \n    # Check if strip column counts are valid\n    if (all(colSums(card_counts) == valid_counts)) {\n      strip_idx = strip_idx + 1\n      # Get row indexes contained in the selected card indexes\n      rows_idxs = unlist(lapply(cards[cards_idxs], `[[`, 1))\n      strips[[strip_idx]] = matrix(\n        unlist(rows[rows_idxs]), \n        ncol = 9, byrow = TRUE\n      )\n    }\n    # Print message every 1000 attempts\n    if (attempts %% 1000 == 0) {\n      message(\"Attempt \", attempts, \" | Strips built:\", strip_idx, \"\\n\")\n    }\n  }\n  dups = duplicated(strips) \n  message(\"There are \", sum(dups), \" duplicatd layouts.\\n\")\n  return(strips)\n}\n\n\n\nA last but not least step\nI’ve never seen a bingo game where you are given empty layouts and are asked to put numbers yourself. So let’s wrap this up and fill our empty cards!\nfill_strips() receives the strip layouts we generated, randomly selects n of them, and, also randomly, fills the slots the cards with numbers. Of course, the first column contains numbers from 1 to 9, the second column contains numbers from 10 to 19… and so on until the last column, that has numbers from 80 to 90.\n\n\nfill_strips = function(strips, n = 100) {\n  # Numbers that go in each column\n  numbers = list(1:9, 10:19, 20:29, 30:39, 40:49, 50:59, 60:69, 70:79, 80:90)\n  # Row indexes corresponding to each card in the strip\n  card_rows = list(1:3, 4:6, 7:9, 10:12, 13:15, 16:18)\n  \n  fill_strip = function(strip) {\n    # Put numbers in the slots with a 1 (meaning they must contain a number)\n    strip[strip == 1] = unlist(\n      # This `sample()` reorders the numbers in each column randomly\n      mapply(sample, numbers, sapply(numbers, length))\n    )\n    \n    for (i in seq_along(card_rows)) {\n      strip_ = strip[card_rows[[i]], ]\n      # Numbers in a given column are sorted in ascending order within cards\n      x = sort(strip_)\n      strip_[strip_ != 0] = x[x != 0]\n      strip[card_rows[[i]], ] = strip_\n    }\n    return(strip)\n  }\n  # Strip layouts can be repeated\n  strips = lapply(sample(strips, n, replace = TRUE), fill_strip)\n  message(\"There are \", sum(duplicated(strips)), \" duplicated strips.\\n\")\n  return(strips)\n}\n\n\n\nAnd we finally get our bingo strips :)\n\n\nset.seed(0303456)\nrows = get_rows()\ncards = get_cards(rows, 1000)\nstrips = get_strips(cards, rows, 20)\nstrips = fill_strips(strips, 50)\n# Output messages have been suppressed\n\n\n\nLet’s check some of them\n\n\nstrips[[1]]\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n [1,]    0   11   20    0   48    0    0   74   80\n [2,]    8    0    0   31    0   51   60   78    0\n [3,]    0   19   27   39    0   54   62    0    0\n [4,]    1    0   26    0   42   55    0    0   84\n [5,]    2   14    0   34    0    0   65   77    0\n [6,]    0   17   29    0   43   59    0    0   89\n [7,]    0    0   22   33    0    0   64   75   88\n [8,]    0   15    0   35   45    0    0   79   90\n [9,]    9    0   25    0   49   50   66    0    0\n[10,]    3    0   28   30    0    0   61   71    0\n[11,]    7    0    0   36   40   58    0    0   81\n[12,]    0   10    0    0   44    0   63   76   87\n[13,]    0    0   21   37    0   52   68   70    0\n[14,]    5   16    0    0   41    0    0   72   82\n[15,]    0   18    0   38   47   57    0    0   86\n[16,]    0    0   23    0   46   53    0   73   83\n[17,]    4   12    0   32    0    0   67    0   85\n[18,]    6   13   24    0    0   56   69    0    0\n\n\n\nstrips[[30]]\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n [1,]    0    0   25    0   43   50    0   74   80\n [2,]    0   16   26   34    0    0   65   79    0\n [3,]    6   17    0   38    0   58    0    0   86\n [4,]    3    0   27    0   40   51   61    0    0\n [5,]    4    0    0   32   49   59    0    0   81\n [6,]    0   19   29   35    0    0   68   71    0\n [7,]    1   14    0    0   47    0   60   75    0\n [8,]    2    0   20   31    0    0   66    0   83\n [9,]    0    0   24    0   48   55    0   77   89\n[10,]    0    0   28   33   42    0   64   76    0\n[11,]    5   12    0   39    0    0   67    0   84\n[12,]    9   15    0    0   45   54    0    0   87\n[13,]    0   13   21    0    0   52    0   73   85\n[14,]    0   18   22    0   44    0   63   78    0\n[15,]    8    0    0   37   46   56    0    0   90\n[16,]    0    0   23   30    0   53   62    0   82\n[17,]    7   10    0   36    0    0   69   70    0\n[18,]    0   11    0    0   41   57    0   72   88\n\nAre we going to play on R consoles?\nAll we got so far are matrices that look like a bingo strip. But honestly, without any given context, they just look like a bunch of matrices of the same dimension filled with 0s and other integer numbers. Our last task is to generate a .pdf output where these matrices really look like bingo cards.\nIn this last part of the post we make use of the grid package. For those who haven’t heard of it, it is the low level plotting library behind ggplot2, for example.\nHere we have a little function, make_grid(), that given a number of rows and columns returns the natural parent coordinates of the borders the grid that defines the rectangles within each card.\n\n\nmake_grid = function(rows, cols) {\n  lines_rows = grid::unit((0:rows) / rows, \"npc\")\n  lines_cols = grid::unit((0:cols) / cols, \"npc\")\n  return(list(\"row\" = lines_rows, \"col\" = lines_cols))\n}\n\n\n\nAnd now we have the main function used to plot the bingo strips. Since the function is quite large, I prefer to explain how it works with comments in the body.\n\n\nplot_strips = function(strips, col = \"#8e44ad\", width_row = 0.925, \n                       width_col = 0.975) {\n  \n  # `rows` and `cols` are the dimensions of each card\n  rows = 3\n  cols = 9\n  g = make_grid(rows, cols)\n  # Compute the center of each square in the card grid\n  centers_rows = g$row[-1] - grid::unit(1 / (rows * 2), \"npc\")\n  centers_cols = g$col[-1] - grid::unit(1 / (cols * 2), \"npc\")\n  # Sort the centers appropiately\n  # This is required because of how we loop over the values in each card\n  x_coords = rep(centers_cols, each = rows)\n  y_coords = rep(rev(centers_rows), cols)\n  \n  # Create unique identifiers for the cards\n  cards_n = paste(paste0(\"CARD N\", intToUtf8(176)), \n                  seq_len(length(strips) * 6))\n  # Compute the number of sheets we're going to need. \n  # Each sheet contains two strips\n  sheets_n = ceiling(length(strips) / 2)\n  \n  # Initial numbers\n  card_idx = 0\n  strip_idx = 0\n  \n  # Loop over sheets\n  for (sheet_idx in seq_len(sheets_n)) {\n    # Each sheet is a grid of 6 rows and 3 columns. \n    # Columns 1 and 3 are where we place the strips. \n    # Column 2 just gives vertical separation.\n    l = grid::grid.layout(nrow = 6, ncol = 3, \n                          widths = c(48.75, 2.5 + 3.75, 48.75))\n    # Start a new page filled with white\n    grid::grid.newpage()\n    grid::grid.rect(gp = grid::gpar(col = NULL, fill = \"white\"))\n    \n    vp_mid = grid::viewport(0.5, 0.5, width_row, width_col, layout = l)\n    grid::pushViewport(vp_mid)\n    \n    # Loop over columns 1 and 3\n    for (j in c(1, 3)) {\n      # Select strip\n      strip_idx = strip_idx + 1\n      if (strip_idx > length(strips)) break\n      strip = strips[[strip_idx]]\n      \n      # Loop over rows (these rows represent the 6 rows assigned to cards)\n      for (i in 1L:l$nrow) {\n        card_idx = card_idx + 1\n        vp_inner = grid::viewport(layout.pos.row = i, layout.pos.col = j)\n        grid::pushViewport(vp_inner)\n        \n        # Add card identification number on top-left\n        grid::grid.text(\n          label = cards_n[card_idx],\n          x = 0,\n          y = 0.96,\n          just = \"left\",\n          gp = grid::gpar(fontsize = 9)\n        )\n        \n        # Draw a grill that separates the slots in the card\n        vp_mid_inner = grid::viewport(0.5, 0.5, 1, 0.80)\n        grid::pushViewport(vp_mid_inner)\n        grid::grid.grill(h = g$row,  v = g$col, gp = grid::gpar(col = col))\n        \n        # Select the numbers that correspond to this card\n        numbers = as.vector(strip[(3 * i - 2):(3 * i), ])\n        # Logical vector that indicates which rectangles are filled\n        # with nunumbers and which rectangles are empty\n        lgl = ifelse(numbers == 0, FALSE, TRUE)\n        \n        # Draw the numbers in positions given by the rectangle centers\n        grid::grid.text(\n          label = numbers[lgl],\n          x = x_coords[lgl],\n          y = y_coords[lgl],\n          gp = grid::gpar(fontsize = 18)\n        )\n        \n        # Fill empty slots with color\n        grid::grid.rect(\n          x = x_coords[!lgl],\n          y = y_coords[!lgl],\n          height = grid::unit(1 / rows, \"npc\"),\n          width = grid::unit(1 / cols, \"npc\"),\n          gp = grid::gpar(\n            col = NA,\n            fill = farver::encode_colour(farver::decode_colour(col), 0.7)\n          )\n        )\n        # End\n        grid::popViewport()\n        grid::popViewport()\n      }\n    }\n    grid::popViewport()\n  }\n}\n\n\n\nNow, all we need is to pass the strips generated above to plot_strips() and wrap that call within grDevices::pdf() and grDevices::dev.off().\n\n\n# Height and width are in inches and here they correspond to legal paper size\ngrDevices::pdf(\"strips.pdf\", height = 14, width = 8.5)\nplot_strips(strips)\ngrDevices::dev.off()\n\n\n\nIf it works, you’ll have a 25 pages pdf with bingo cards that look like this one\n\nFirst card in the output\nIf you can’t (or just don’t want to) run the code, here you have the generated pdf.\n\nHow I dare to call this a project?↩︎\nBut you should try this bingo, you gonna like it!↩︎\nSome are also known as sobremesa↩︎\nHaven’t you heard Estallando desde el océano by Sumo?↩︎\nIf you’ve heard of Sampford’s pps sampling, this is going to be familiar↩︎\nI know that returning row indexes is less intuitive than returning card layouts, but this approach requires less memory because it only stores 3 values per card, instead of 18.↩︎\n",
    "preview": {},
    "last_modified": "2022-01-15T20:16:36-03:00",
    "input_file": {}
  }
]
