---
title: "New families in Bambi"
date: 2021-07-14
output:
  distill::distill_article:
    self_contained: false
description: | 
  In this third post about my work during this Google Summer of Code
  I describe two families of models recently added. The first one, is the 
  Student T family, used to make linear regressions more robust. The second, 
  is the Beta family which can be used to model ratings and proportions. 
editor_options:
  chunk_output_type: console
---

```{r plots-setup, echo=FALSE}
knitr::opts_chunk$set(fig.align = "center", out.width = "95%")
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

## Introduction

I'm very happy I could contribute with many exciting changes to
[Bambi](https://bambinos.github.io/bambi). Some changes, such as the
reorganization of the default priors and built-in families, are not visible to
the user but make the codebase more modular and easier to read. Other changes,
such as the ones I'm going to describe here, have a direct impact on what you
can do with Bambi.

Today I'll describe two new built-in families that have been added to Bambi.
The first one, already described in my previous post, is the `"t"` family. This
can be used to make linear regressions more robust to outliers. The second one the
`"beta"` family which can be used to model ratings and proportions.

## Setup


```{r r-setup, include=FALSE}
library(betareg)
library(ggplot2)
library(patchwork)
library(reticulate)

use_condaenv("bmb", required = TRUE)

BLUE = "#003f5c"
PURPLE = "#7a5195"
PINK = "#ef5675"

data("GasolineYield", package = "betareg")
```

```{python}
import arviz as az
import numpy as np
import pandas as pd

from bambi import Model, Prior
```

## Robust linear regression with the `t` family.


A Bayesian robust linear regression looks as follows

$$
y_i \sim \text{StudentT}(\mu_i, \lambda, \nu)
$$

where $\mu_i = \beta_0 + \beta_1 x_{1, i} + \cdots + \beta_p x_{p, i}$, $\lambda$ is
the precision parameter and $\nu$ is the degrees of freedom.

This wouldn't be a Bayesian model without priors. Bambi uses the following
priors by default:

$$
\begin{array}{c}
\beta_0 \sim \text{Normal}(\mu_{\beta_0}, \sigma_{\beta_0}) \\
\beta_j \sim \text{Normal}(\mu_{\beta_j}, \sigma_{\beta_j})  \\
\lambda \sim \text{HalfCauchy(1)}
\end{array}
$$

where the  $\mu_{\beta_j}$ and $\sigma_{\beta_j}$ are estimated from the data.
By default, $\nu=2$, but it is also possible to assign it a probability 
distribution (as we're going to see below).

Before seeing how this new family works, let's simulate some data. On this
opportunity, we're using the same dataset than in the [previous post](https://tcapretto.netlify.app/post/2021-07-05-robust-linear-regression-with-bambi/).
This is a toy dataset with one predictor `x`, one response `y`,
and some outliers contaminating the beautiful linear relationship between the
variables.

```{python}
size = 100
true_intercept = 1
true_slope = 2

x = np.linspace(0, 1, size)
y = true_intercept + true_slope * x + np.random.normal(scale=0.5, size=size)

x_out = np.append(x, [0.1, 0.15, 0.2])
y_out = np.append(y, [8, 6, 9])

data = pd.DataFrame(dict(x = x_out, y = y_out))
```

```{r, echo=FALSE}
ggplot(py$data) +
  geom_point(aes(x, y), shape = 21, fill = PURPLE, color = "black", size = 2.2)
```

### Model specification and fit


Using this new family is extremely easy. It is almost as simple as running a 
default normal linear regression. The only difference is that we need to add 
the `family="t"` argument to the `Model()` instantiation.

```{python}
model = Model("y ~ x", data, family="t")
model
```

The output above shows information about the family being used and the parameters
for the default priors. Next, we just do `model.fit()` to run the sampler.

```{python eval=FALSE}
idata = model.fit()
```

### Use custom priors


Let's say we are not happy with having a fixed value for the degrees of freedom
and we want to assign it a prior distribution. Is that a problem? Of course not!

```{python}
# Use a Gamma prior for the degrees of freedom
model = Model("y ~ x", data, family="t")
model.set_priors({"nu": Prior("Gamma", alpha=3, beta=1)})
model
```

And hit the inference button

```{python fit-t, cache=TRUE}
idata = model.fit()
```

### Explore results


First of all we can see the marginal posteriors for the parameters in the model
and their respective traces

```{python post-t, echo=FALSE, cache=TRUE}
a = idata.posterior["Intercept"].values.flatten()
b = idata.posterior["x"].values.flatten()
sigma = idata.posterior["y_sigma"].values.flatten()
nu = idata.posterior["y_nu"].values.flatten()

posterior_df = pd.DataFrame({
  "β_0": a, "β_1": b, "σ": sigma, "ν": nu, 
  "chain": ["1"] * 1000 + ["2"] * 1000, "draw": [n for n in range(1000)] * 2
})
```

```{r post-t-plot, echo=FALSE, cache=TRUE}
posterior_df = tidyr::pivot_longer(
  py$posterior_df, 
  cols = c("β_0", "β_1", "σ", "ν")
)
plt1 = ggplot(posterior_df) +
  geom_histogram(
    aes(value, fill = chain), 
    position = "identity", 
    alpha = 0.6, 
    bins = 40
  ) +
  scale_fill_manual(values = c(PURPLE, BLUE)) + 
  facet_wrap(
    vars(name), 
    nrow = 4,
    scales = "free",
    strip.position = "top"
  )  + 
  theme(
    legend.position = "none",
    axis.title = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) 

plt2 = ggplot(posterior_df) +
  geom_line(
    aes(draw, value, color = chain), alpha = 0.6
  ) +
  scale_color_manual(values = c(PURPLE, BLUE)) + 
  facet_wrap(
    vars(name), 
    nrow = 4,
    scales = "free",
    strip.position = "top"
  )  + 
  theme(
    legend.position = "none",
    axis.title = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) 
  
plt1 + plt2
```

And it is also good to explore the posterior distribution of regression lines

```{r post-t-plot-2, echo=FALSE, cache=TRUE}
ggplot(py$data) +
  geom_abline(
    aes(intercept = a, slope = b),
    data = data.frame(a = py$a, b = py$b),
    color = "gray30",
    alpha = 0.1
  ) +
  geom_abline(
    intercept = py$true_intercept,
    slope = py$true_slope,
    size = 1.2,
    color = "black"
  ) +
  geom_point(aes(x, y), shape = 21, fill = PURPLE, color = "black", size = 2.2)
```

where the line in black is the true regression line.

## Beta regression with the `beta` family.


Beta regression is useful to model response variables that have values within
the $(0, 1)$ interval. This type of regression is based on the assumption that 
the conditional distribution of the response variable follows a Beta distribution
with its mean related to a set of regressors through a linear predictor with 
unknown coefficients and a link function.

The beta regression model is based on an alternative parameterization of the 
beta density in terms of the mean $\mu$ and a precision parameter $\kappa$.

$$
\begin{array}{lr}
\displaystyle f(y | \mu, \kappa) = 
  \frac{\Gamma(\kappa)}{\Gamma(\mu\kappa)\Gamma((1-\mu)\kappa)} 
  y^{\mu\kappa -1}
  y^{(1 - \mu)\kappa -1}, & 0 < y < 1
\end{array}
$$

with $0 < \mu < 1$ and $\kappa > 0$.

If we use the same notation than for the robust linear regression, 
the beta regression model is defined as 

$$
y_i \sim \text{Beta}(g^{-1}(\mu_i), \kappa)
$$

where $\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}$, $\kappa$ is
the precision parameter and $g$ is a twice differentiable, strictly increasing, 
link function.

Bambi uses again the following priors by default:

$$
\begin{array}{c}
\beta_0 \sim \text{Normal}(\mu_{\beta_0}, \sigma_{\beta_0}) \\
\beta_j \sim \text{Normal}(\mu_{\beta_j}, \sigma_{\beta_j})  \\
\kappa \sim \text{HalfCauchy(1)}
\end{array}
$$

where the  $\mu_{\beta_j}$ and $\sigma_{\beta_j}$ are estimated from the data.
By default, $g$ is the logit function. Other options available are the identity,
the probit, and the cloglog link functions.

It's possible to resume all of this in a very simplistic way by seeing that 
the beta regression as a very close relative of the GLM family. 
This model presents all the characteristics of GLMs, with the exception that the
beta distribution doesn't belong to the exponential family.

### Model specification and fit


Here we are going to use the `GasolineYield` dataset from the `betareg` R package.
This dataset is about the proportion of crude oil converted to gasoline. 
The response variable is the proportion of crude oil after distillation and 
fractionation. In this example, we use the temperature at which gasoline has 
vaporized in Fahrenheit degrees (`"temp"`) and a factor that indicates ten unique 
combinations of gravity, pressure and temperature (`"batch"`).

The following is just a re-ordering of the categories in the `"batch"` variable
so it matches the original contrasts used in the `betareg` package.

```{python}
data = r.GasolineYield
data["batch"] = pd.Categorical(
  data["batch"], 
  ["10", "1", "2", "3", "4", "5", "6", "7", "8", "9"], 
  ordered=True
)
```

Next, we define the model. The only difference is that we indicate 
`family="beta"`. Bambi handles all the rest for us.

```{python}
# Note this model does not include an intercept
model = Model("yield ~ 0 + temp + batch", data, family="beta")
model
```

And `model.fit()` is all we need to ask the sampler to start running.

```{python model-fit-beta, cache=TRUE}
idata = model.fit(target_accept=0.9)
```

### Explore results

Once we got the posterior, we explore it. This time we're going to plot 
highest density intervals for the marginal posteriors corresponding to the 
parameters in the model.

```{python beta-summary, echo=FALSE, cache=TRUE}
summary = az.summary(idata)
```

```{r, beta-summary-2, echo=FALSE, cache=TRUE}
model_summary = py$summary[1:4]
labels = c(
  "β_temp", "β_batch[10]", "β_batch[1]", "β_batch[2]", "β_batch[3]",
  "β_batch[4]", "β_batch[5]", "β_batch[6]", "β_batch[7]", "β_batch[8]", 
  "β_batch[9]", "κ"
)
model_summary$row = seq(1, 12, by = 1)
model_summary$panel = c("1-Temperature", rep("2-Batch", 10), rep("3-Precision"))
```

```{r posterior-model-beta-plot, echo=FALSE, cache=TRUE, out.width="100%"}
ggplot(model_summary) +
  geom_point(
    aes(mean, row), size = 2, color = BLUE
  ) +
  geom_segment(
    aes(x = `hdi_3%`, xend=`hdi_97%`, y = row, yend = row), size= 1, color = BLUE
  ) + 
  labs(
    x = "Marginal posterior",
    y = "Paremeter"
  ) + 
  scale_y_continuous(
    breaks = seq(1, 12, by = 1),
    labels = labels
  ) + 
  facet_grid(cols = vars(panel), scales = "free") +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()
  )
```

