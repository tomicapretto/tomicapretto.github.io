{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697af76b",
   "metadata": {},
   "source": [
    "## $R^2$ for Bayesian regression models\n",
    "\n",
    "In [R-squared for Bayesian Regression Models](https://www.tandfonline.com/doi/abs/10.1080/00031305.2018.1549100) by Gelman et al. (2018),\n",
    "the authors propose a generalization of the classical coefficient of determination $R^2$ to Bayesian regression models.\n",
    "\n",
    "The coefficient of determination they propose is defined as:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{Explained variance}}{\\text{Explained variance} + \\text{Residual variance}} = \\frac{\\text{var}_\\text{fit}}{\\text{var}_\\text{fit} + \\text{var}_\\text{res}}\n",
    "$$\n",
    "\n",
    "where $\\text{var}_\\text{fit}$ and $\\text{var}_\\text{res}$ are computed as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{var}_\\text{fit} &= \\text{V}\\left(\\mathbb{E}(\\tilde{y}_i \\mid \\boldsymbol{x}_i, \\boldsymbol{\\theta})\\right) =  \\text{V} \\left(y_i^\\text{pred}\\right) \\\\\n",
    "\\text{var}_\\text{res} &= \\text{M}\\left(\\mathbb{V}\\left(\\tilde{y}_i - y_i^\\text{pred} \\mid \\boldsymbol{x}_i, \\boldsymbol{\\theta}\\right) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $M$ and $V$ denote the sample mean and variance operators, respectively.\n",
    "\n",
    "The first term is the variance of the expectation of future data and the second one is the expected variance of future residuals.\n",
    "\n",
    "This Bayesian $R^2$ is conditional on the explanatory variables and the model parameters $\\boldsymbol{\\theta}$.\n",
    "For this reason, this $R^2$ is proposed as an _a posteriori_ mesasure of model fit.\n",
    "\n",
    "If we have draws from the posterior distribution, we can compute the Bayesian $R^2$ for each draw."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ab8e0",
   "metadata": {},
   "source": [
    "### Common cases\n",
    "\n",
    "In the case of normal regression models, the components of the Bayesian $R^2$ simplify to:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{var}_\\text{fit} &= \\text{V}(\\mu_i) \\\\\n",
    "\\text{var}_\\text{res} &= \\sigma^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mu_i = \\boldsymbol{x}_i^T \\boldsymbol{\\beta}$.\n",
    "\n",
    "And for logistic regression models, they become:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{var}_\\text{fit} &= \\text{V}(\\pi_i) \\\\\n",
    "\\text{var}_\\text{res} &= \\text{M}(\\pi_i (1 - \\pi_i))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\pi_i = \\text{expit}(\\boldsymbol{x}_i^T \\boldsymbol{\\beta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5787620",
   "metadata": {},
   "source": [
    "## The R2D2 prior\n",
    "\n",
    "The R2D2 prior for normal regression models has been introduced in the paper \n",
    "[Bayesian Regression Using a Prior on the Model Fit: The R2-D2 Shrinkage Prior](https://arxiv.org/abs/1609.00046) by Zhang et al. (2016).\n",
    "\n",
    "Consider the normal regression model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y_i \\mid \\mu_i, \\sigma^2 &\\underset{iid}{\\sim} \\text{Normal}(\\mu_i, \\sigma^2) \\\\\n",
    "\\mu_i &= \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{pi} \\\\\n",
    "&= \\alpha + \\boldsymbol{X}\\boldsymbol{\\beta}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{X}$ is the $n \\times p$ design matrix (without the intercept), $\\alpha$ is the intercept term, and $\\boldsymbol{\\beta}$, of length $p$, is the vector of coefficients.\n",
    "\n",
    "The central idea in the R2-D2 prior is to place a prior directly on the coefficient of determination $R^2$.\n",
    "For the purpose of defining prior distributions, however, Zhang et al. work with the **marginal** coefficient of determination: a version of $R^2$ that averages over both the design matrix $\\boldsymbol{X}$ and the regression coefficients $\\boldsymbol{\\beta}$, rather than the conditional coefficient of determination proposed by Gelman et al, that we mentioned above.\n",
    "\n",
    "For the linear regression model, the marginal $R^2$ is defined as:\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\mathbb{V}(\\boldsymbol{x}^T \\boldsymbol{\\beta})}{\\mathbb{V}(\\boldsymbol{x}^T \\boldsymbol{\\beta}) + \\sigma^2}\n",
    "= \\frac{\\sigma^2 W}{\\sigma^2 W + \\sigma^2}\n",
    "= \\frac{W}{W + 1}\n",
    "$$\n",
    "\n",
    "which is the ratio of the marginal variance of the linear predictor to the marginal variance of the outcome.\n",
    "\n",
    "Then, the R2-D2 prior is specified as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta_j &\\sim \\text{Normal}(0, \\phi_j W \\sigma^2) \\\\\n",
    "\\boldsymbol{\\phi} &\\sim \\text{Dirichlet}(\\xi_1, \\dots, \\xi_p) \\\\\n",
    "W & = \\frac{R^2}{1 - R^2}\\\\\n",
    "R^2 &\\sim \\text{Beta}(a, b) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Through the transformation $W = R^2/(1-R^2)$, the prior on $R^2$ induces a prior on $W$, which governs the total prior variance of the linear predictor $\\boldsymbol{x}^T \\boldsymbol{\\beta}$.\n",
    "Combined with the Dirichlet prior on the variance proportions $\\boldsymbol{\\phi}$, this results in the $R^2$-induced Dirichlet Decomposition (R2-D2) prior.\n",
    "\n",
    "It can be shown that the induced prior on $W$ is a Beta Prime distribution with parameters $a$ and $b$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
